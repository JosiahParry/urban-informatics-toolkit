[{"path":"index.html","id":"welcome","chapter":"1 Welcome","heading":"1 Welcome","text":"Welcome Urban Informatics Toolkit. online book intended serve effective introduction understanding analysis urban commons. find material theoretical underpinnings Urban Informatics introduction statistical programming language R allow work confidently data encounter research, coursework, elsewhere.book many things . Within two years study Urban Informatics Program Northeastern University, five years self-directed education R, two years teaching R, innumerable hours frustration spent attempting understand otherwise digestible language.best avoid overly technical verbose descriptions theory concepts. attempt explain everything friend.end pages hope become self-sufficient analyses. intention teach everything need know impossible. Data analytics always changing current dated tomorrow. wrote , fact, one commonly used tools altered degree much already written became dated. purpose book, , brief latest developments, equip proper tools navigate rapidly evolving field.book three goals. First, ensure reader understanding fundamentals scientific inquiry Big Data era. Second, familiarize reader effective approaches data analysis. third, equip reader enough knowledge R might understand ’s happening hood, speak.","code":""},{"path":"index.html","id":"structure-of-the-book","chapter":"1 Welcome","heading":"1.1 Structure of the book","text":"book four sections, ’s theme. first section, Foundations Urban Informatics, explore nature big data role field. section review different approaches scientific inquiry, seeking understand newer methods enabled big data, closing review Broken Windows Theory development ecometrics.second section, Toolkit Foundations, introduce R. section highly technical. first ensure download software data needed follow along exercises book. section cover everything reading data manipulating , also discussing fundamental R theory.third section, Visualization Strategies, largest, dedicated entirely information visualization. section learn craft visualizations understand make kind graphic. also discuss various traditional graphic types, update expand . section expansive due importance visualization.final section, Expanding Toolkit, deal multiple datasets, statistics, spatial data. course, section spatial analysis , necessity, lacking, subject expansive even cursory sketch contents fill multiple books.","code":""},{"path":"index.html","id":"considerations","chapter":"1 Welcome","heading":"1.2 Considerations","text":"fields digital humanities continue grow number tools available R increase work continually add improve writing. topics like see included expanded upon, please submit issue GitHub https://github.com/JosiahParry/urban-informatics-toolkit/issues reach directly.","code":""},{"path":"the-utility-and-danger-of-big-data.html","id":"the-utility-and-danger-of-big-data","chapter":"2 The Utility and Danger of Big Data","heading":"2 The Utility and Danger of Big Data","text":"Urban Informatics , way, byproduct “deluge” “proliferation” data. society progressed technologically, able capture store data unprecedented scale. led massive stores data used primarily record keeping, updated near-real time. Every tweet Facebook post subsequently recorded remote database ensure can accessed later time. Dan O’Brien notes characteristic big data utmost consequence “advancement science policy digital age.”1 Since data capturing every day behavior consider “naturally occurring.” Naturally occurring data useful essentially track record individuals’ behavior time. close can get measuring behavior real time.urban context, importance big data becomes ever apparent. city government Boston, instance, keeping detailed records property assessments tax debts since late 18th century, including demographic characteristics debtors, even locations ward level. administrative records kept ink paper just decades ago. digitization efforts, data now accessible historians, urban scholars, local government, general public. data accessible provides way quantitatively inspect development city geography, policies, demography, much .number benefits naturally occurring data provide. first , theory, comprehensive contains information residents. administrative data , example, able determine number employed tax paying citizens, well underemployed receive government benefits. Additionally, since data already collected, associated costs minimal. contrast empirically collected data, administrative data just representation single moment time, rather continually changing updating. due fact administrative data collected municipal level inherently dealing geospatial data—data associated location.many benefits administrative big data, dangers, . first even though big data comprehensive theory, always take objective observations natural world. must cognizant fact biases humans also represented data. separate theory data. take Dan O’Brien“. . . point science explain things work way . . .limit inquiries correlation eschew explanation, longer conducting science.”2\n— Daniel T. O’BrienWhile using big data present dangers, discard entirely. cope dangers must cognizant.","code":""},{"path":"data-in-the-municipal-context.html","id":"data-in-the-municipal-context","chapter":"3 Data in the municipal context","heading":"3 Data in the municipal context","text":"new Gold Rush. Rapidly improving technology collection storage data created vast troves unexplored information.focus gaze towards municipal government, can see proliferation data, slower. Local governments collecting data centuries, recently always accessible, even considered “data.” Since 19th century, Boston issuing recording building permits. massive digitization effort permits now accessible online database.3 governments turning modern methods data storage, also creating applications encourage citizens engage local governments. Mobile web applications hopefully facilitate greater interaction citizen government.4 every one citizen--government interactions recorded stored database—though open accessible citizen scientist.Boston built mobile applications residents. Notable among apps BOS:3115, ParkBoston6, less popular Boston PayTix7, new Blue Bikes8. BOS:311 residents can communicate directly Department Public Works recording issue, location, even image issue. Blue Bikes trips, 311 requests, much provided public via Analyze Boston, Boston’s data portal.9This new availability data unintentionally altered way scientists interact data. purposes scientific inquiry, scientists analysts historically close data generation process.residents citizens might interact governmental agencies, don’t scientists. Likewise, governmental agencies engage citizens governments, scientists. , much——open public data use within urban informatics—greater digital humanities—fields generated express purpose analyzed. inherently changes way one approaches analysis.approaching data nature, researchers begun embracing exploratory data analysis (EDA). EDA extremely useful developing insights data priori hypotheses. influential book, R Data Science10 Garret Grolemund Hadley Wickham describe inductive approach exploratory data analysis.“Data exploration art looking data, rapidly generating hypotheses, quickly testing , repeating .”11When researchers set test hypothesis often become closely involved data generation process. scenario, researchers likely preconceived hypotheses expectations may find hidden data.condition often case working open data. always know outset looking . open data—data really—never know may find begin dig.","code":""},{"path":"approaches-to-and-schools-of-urban-informatics.html","id":"approaches-to-and-schools-of-urban-informatics","chapter":"4 Approaches to and Schools of Urban Informatics","heading":"4 Approaches to and Schools of Urban Informatics","text":"","code":""},{"path":"approaches-to-and-schools-of-urban-informatics.html","id":"scientific-approaches","chapter":"4 Approaches to and Schools of Urban Informatics","heading":"4.1 Scientific approaches","text":"sciences generally, two approaches scientific inquiry: inductive deductive. two approaches can best characterized “bottom ” “top ,” respectively. origins, strengths, weaknesses. argument raging since 17th century approach best.Deductive reasoning establishment principle based set accepted premises. Take classic example: people mortal; Socrates person therefore mortal. call “top-” reasoning.Inductive reasoning works reverse order. construction inferences general principles based observed phenomena. instance things fly wings, wings must necessary flight. might call “bottom-” reasoning.adapt logic scientific approaches can think following order operations. deduction start theory workings observed phenomenon. theory, create hypothesis, observe phenomenon, use observations test th theory. induction observe patterns. observations create hypotheses.","code":""},{"path":"approaches-to-and-schools-of-urban-informatics.html","id":"the-chicago-school","chapter":"4 Approaches to and Schools of Urban Informatics","heading":"4.2 The Chicago School","text":"Much known urban sciences today can traced back late 19th early 20th centuries University Chicago. university , time, epicenter new field American Sociology, came known Chicago School. era, social scientists creating grand theories world. Take, example, new field anthropology crafted theories origins human race. Chicago School “fostered different view sociology: first-hand data collection emphasized; research particular case setting stressed; induction deduction promoted.”12Scholars Robert Park, Ernest Burgess, Louis Wirth developed number micro-theories understand city. notably culminating work City Park Burgess, collection essays encapsulates decades careful observation led number theories, many still influence today. body work, important many ways, early paragon inductive approach social research.","code":""},{"path":"approaches-to-and-schools-of-urban-informatics.html","id":"complexity-and-santa-fe-institute","chapter":"4 Approaches to and Schools of Urban Informatics","heading":"4.3 Complexity and Santa Fe Institute","text":"end spectrum rests Santa Fe Institute (SFI) complexity science. SFI’s mission “endeavor understand unify underlying, shared patterns complex physical, biological, social, cultural, technological, even possible astrobiological worlds.”13 Crudely, goal unify theory one general master theory. Central theoretical focus view “valuable ideas often confined echo chambers academia.”14 wrong.work important bridging gaps disciplines. apply biological theories scaling human development. findings fruitful. focus interconnectivity theory natural social worlds messy work must done. spirit Chicago School, illustrated ’s view city organism.","code":""},{"path":"approaches-to-and-schools-of-urban-informatics.html","id":"a-hybrid-approach","chapter":"4 Approaches to and Schools of Urban Informatics","heading":"4.4 A Hybrid Approach","text":"Today, social scientists enjoy access better data ever. able take much hybrid approach. data provide fine detail scientists can discern never seen patterns. allows us take much inductive approach, can craft theories patterns observe, test theories creative ways. hybrid methodology incorporates inductive deductive approaches. Boston Area Research Initiative (BARI) one group utilizes hybrid approach.utilize methology, scientists start data. use publicly available (open) administrative data explore. get metaphorical hands dirty data. munge , transform , rearrange , walk away tidbit information. discover taken together develop theory—framework understanding observed. Next, develop hypothesis using new theory apply set data new circumstance test refine theories developed. create theory observation, test theories new unexplored observations.Dan O’Brien, Director BARI, claims track inductive approach Chicago School. , BARI also actively seeks evaluate existing theory. better examples BARI approach development use ecometrics understand city test existing theory.next chapter learn ecometrics, origins, use evaluating prominent criminological Broken Windows theory.","code":""},{"path":"ecometrics.html","id":"ecometrics","chapter":"5 Ecometrics","heading":"5 Ecometrics","text":"Central work done BARI development utilization ecometrics. Ecometrics quantitative representation physical social environment. Urban Informatics context, ecometrics created extract latent constructs—variables can inferred dataset—illustrate physical social phenomenon.understand , need contextualize datasets. created intention beng analyzed measure blight neighborhood social unrest city. lack 311 reports may tell story underserved neighborhood. Census measures income combined 911 reports neighborly complaints domestic disturbance may tell story gilded community beautiful brick façades next collective efficacy. datasets contain gems—beautifully inelegant snapshots societal quotidian. measuring ? ’s tough part create ecometrics. provide us way adapt existing data address new problems.work BARI conducts, production city-wide ecometrics social physical disorder emblematic hybrid approach. understand work need venture back 1982 article Atlantic called Broken Windows.15","code":""},{"path":"ecometrics.html","id":"broken-windows-theory","chapter":"5 Ecometrics","heading":"5.1 Broken Windows Theory","text":"beginning crack cocaine epidemic, criminologists George Kelling James Wilson wrote famous article titled “Broken Windows” outlined new theory explain occurrence crimes. premise article presence disorder concerning neighborhood’s residents actual crime occurs. , “visual cues disorder … begets predatory crime neighborhood decline.”16“Broken Windows” captured eyes pundits policy makers. simplicity made attractive led policies stop frisk. public support went counter previous scientific findings. Due contrast public policy scientific findings researchers began develop ways quantifying disorder city. seminal article Sampson Raudenbush (1999), practice systematic social observation created.17 process photographs public spaces taken coded identify disorder—e.g. presence empty beer cans—can quantitatively analyzed. early example ecometric.","code":""},{"path":"ecometrics.html","id":"quantifying-disorder","chapter":"5 Ecometrics","heading":"5.2 Quantifying Disorder","text":"2015, O’Brien Sampson published article “Public Private Spheres Neighborhood Disorder: Assessing Pathways Violence Using Large-scale Digital Records.”18 article epitomizes hybrid approach urban studies. , O’Brien Sampson utilize 911 dispatches 311 call data create measures social physical disorder. measures used put Broken Windows theory test. process using existing administrative datasets method estimating social physical phenomena illustrates inductive approach. explicit measure public disorder measures must inferred. Whereas testing testing efficacy Broken Windows indicative traditional deductive process explicit testing theory.Quantifying disorder small task. 2015 paper authors writeTaking challenge, O’Brien, Sampson, Winship (2015) proposed methodology ecometrics age digital data, identifying three main issues data articulating steps addressing . (1) identifying relevant content, (2) assessing validity, (3) establishing criteria reliability.19 20This astute summation problems arise big data can overcome. biggest concerns, mentioned opening section, validity data using.","code":""},{"path":"ecometrics.html","id":"defining-the-phenomenon","chapter":"5 Ecometrics","heading":"5.2.1 Defining the phenomenon","text":"method propose requires us three main tasks. first clearly define phenomenon hoping measure. Following, must idenify relevant data. example, O’Brien Sampson (2015), define five ecometrics asPublic social disorder, panhandlers, drunks, loud disturbances;Public violence involve gun (e.g., fight);Private conflict arising personal relationships (e.g., domestic\nviolence);Prevalence guns [sic] violence, indicated shootings incidents involving guns; andAlcohol, including public drunkenness public consumption \nalcohol.21\"definitions provide clear pictures measured. next step surf data best match variables observations measures. , process—usually factor analysis—ensure measures truly relevant.","code":""},{"path":"ecometrics.html","id":"validating-the-measure","chapter":"5 Ecometrics","heading":"5.2.2 Validating the measure","text":"ecometric defined properly measured, next step validate . process similar ground truthing geospatial sciences. Often geographic coordinates recorded individual go physical location ensure whatever recorded actually . ecometrics. developed measures, need compare objective truth.Sampson & Raudenbush (1999), develop measures physical disorder systematic social observation.22 order validate measures, compared results neighborhood audit.23 audit served ground truth used make adjustments needed.","code":""},{"path":"ecometrics.html","id":"addressing-reliability","chapter":"5 Ecometrics","heading":"5.2.3 Addressing reliability","text":"ecometric, like others, naturally time bound snapshot social physical world. measures change time. useful know reliable measure different periods time24. authors bit statistical finesse—e.g. factor analysis—best left explain. take away ecometrics often time variant important us know flexible can time scales.","code":""},{"path":"the-basics.html","id":"the-basics","chapter":"6 The basics","heading":"6 The basics","text":"","code":""},{"path":"the-basics.html","id":"what-is-r-and-why-do-i-care","chapter":"6 The basics","heading":"6.1 What is R and why do I care?","text":"R? R 18th letter alphabet, fourth letter QWERTY—like keyboard—, importantly, R software package statistical computing.First, brief history lesson. R descendant S statistical programming language whose naissance can traced back 1976 Bell Laboratories.25 S developed, people sought commercialize language. 1993, license well development selling rights given private company. S became available commercialized S-PLUS.26Later, seeing need improved statistical software environment, two researchers University Auckland created new statistical programming language. became known R. R developed image S. S code able run R code. However, early decision make R (also referred R-project) free open source changed fate dramatically.Today, R-project developed maintained group known R Core “represent multiple statistical disciplines based academic, --profit industry-affiliated institutions multiple continents”27. define R “integrated suite software facilities data manipulation, calculation graphical display.”28R essence fancy calculator. designed math—specifically statistics—capabilities extend well beyond basic mathematics. R intents purposes programming language, one , theory, feel like data analysis programming.29R unique amonth commercial statistical software Stata SPSS. fundamentally, R free project. monetarily free, free refers “liberty, price.”30. think important familiarize four freedoms free software. freedoms listed one four, though referred freedoms 0 3.:freedom run program wish, purpose (freedom 0).freedom study program works, change computing wish (freedom 1). Access source code precondition .freedom redistribute copies can help others (freedom 2).freedom distribute copies modified versions others (freedom 3). can give whole community chance benefit changes. Access source code precondition .31These freedoms large part success R language. free nature R, academics industry experts around globe contributing language. means many new statistical techniques first implemented R.contributions people make R changing ways people perform data analysis. , need start understand tooling use part scientific process—apart . engage analyses contribute scientific literature, remember without tools using, much possible. engage science, think adhering four essential freedoms. enabling others findings wish? research accessible greater community? “give whole community chance benefit [work]?”32","code":""},{"path":"the-basics.html","id":"the-rstudio-ide","chapter":"6 The basics","heading":"6.2 The RStudio IDE","text":"R downloaded, interacting requires fair amount computer science know . people love , can feel like programming matrix.reason, use RStudio program R. RStudio integrated development environment (IDE). means features need develop R one place. RStudio gives place write R code, execute , view graphics produce, much .like think R English language using RStudio like using Mircrosoft word. Chester Ismay Albert Kim’s book Modern Dive, provide another excellent analogy R RStudio. describe R engine car, RStudio dashboard.33In RStudio four quadrants work called panes.graphic borrowed RStudio, PBC’s Thomas Mock’s Introduction Tidyverse.34","code":""},{"path":"the-basics.html","id":"the-editor","chapter":"6 The basics","heading":"6.2.1 The Editor","text":"top left pane, actually write code. see image tab name R file edited, mpg-plot.R. simplest way R code written, documents .R extension. Files .R extension called “scripts.” , like movie script, list order lines—case, lines code—occur. Think R script word document. put writing want keep.also second type R file called R Markdown document, .Rmds. special type file lets us intersperse regular prose code chunks. Rmd extremely flexible enables user render content many different formats PDF, PowerPoint, HTML, others. example, book written R Markdown. , keep things simple, use R scripts vast majority book.","code":""},{"path":"the-basics.html","id":"the-console","chapter":"6 The basics","heading":"6.2.2 The console","text":"bottom left pane console. console R code actually executed. run line chunk code editor, see processed console. often treat console scratch paper. place can explore R objects code without affecting primary R file. become comfortable typing R code editor seeing executed console.","code":""},{"path":"the-basics.html","id":"output","chapter":"6 The basics","heading":"6.2.3 Output","text":"versatile quadrant RStudio. primarily use quadrant look graphs. pane navigating files, looking help documentation, viewing charts produce, interactive applications may develop.","code":""},{"path":"the-basics.html","id":"installing-r-rstudio","chapter":"6 The basics","heading":"6.3 Installing R & RStudio","text":"Now somewhat familiar R RStudio, time install . recommend installing R first.R can downloaded Central R Archival Network (CRAN).35 CRAN official location things R. CRAN provides access R software, license, copyright, software extensions (called R packages).RStudio provided RStudio, Public Benefit Corporation (RStudio, PBC). install RStudio navigate download page.36\ninstalled can open RStudio get started. Look circular R logo. get lost navigating RStudio IDE, sure refer cheat sheet.","code":""},{"path":"the-basics.html","id":"preventative-care","chapter":"6 The basics","heading":"6.4 Preventative Care","text":"","code":""},{"path":"the-basics.html","id":"r-projects","chapter":"6 The basics","heading":"6.4.1 R Projects","text":"open RStudio able get rolling. following section save anyone collaborate indescribable amount headaches. Many users tempted open RStudio begin analysis. need contextualize every analysis ’s project. common overarching theme, intent, purpose, analysis delineated project identifiable others.probably already projects life. Consider school work. Suppose organized folder structure semester folder, course folder within . example folder organization looks like .case course project. important thing keep mind project self-contained. working inside self-contained folders can ensure problems accessing files. R uses concept working directory. working directory place R starts looking files.Imagine R using working directory spring working info-design work. impression working info-design folder, R believe working spring. try load data data folder inside info-design (looks like info-design/data), tell R get . instinctual path data R actually expecting info-design/data.prevent can create R project. R project imposes standalone structure need prevent issue.way create new project navigating File > New Project.Click New Directory. create new folder . Next, RStudio prompt specify kind project create. Today, generic New Project. future, suspect end creating Shiny applications much .final step specify project called put . image name new directory uitk, short Urban Informatics Toolkit, place directory R. sure select folder want project live .tips:Think able find project .put spaces periods directory name. Use _ - feel need.open new RStudio session. notice Files pane now uitk.Rproj file . file tells RStudio project, don’t delete ! like open RStudio project can either open .Rproj file file navigator following File > Open Project.","code":"fall/\n  big-data-for-cities/\n    projects/\n  urban-theory/\n    literature/\n  \nspring/\n  info-design/\n    data/\n  intro-data-mining/"},{"path":"the-basics.html","id":"the-data","chapter":"6 The basics","heading":"6.4.2 The data","text":"order complete exercises throughout book need data accessible . can download .37 link download file called data.zip. downloaded open file. create folder called data. Move entire folder new project. created project called uitk R directory, move folder ~/R/uitk. create folder path ~/R/uitk/data.","code":""},{"path":"the-basics.html","id":"your-workspace","chapter":"6 The basics","heading":"6.4.3 Your Workspace","text":"another effort impose good habits reproducibility standards suggest change one setting RStudio. Navigate Tools > Global Options change setting.setting makes analysis dependent upon code write, data objects create interactively programming. general rule thumb R script able run top bottom successfully.","code":""},{"path":"the-basics.html","id":"before-we-embark","chapter":"6 The basics","heading":"6.5 Before we embark","text":"Lastly, want emphasize R RStudio can used much statistical analysis. can used make art.can used make beautiful graphics BBC.R can found infrastructure modern world. R utilized global financial institutions, civil rights groups ACLU, investigative journalism, national defense, much . feel thing get learning R simple statistics.","code":""},{"path":"the-basics.html","id":"getting-help","chapter":"6 The basics","heading":"6.6 Getting Help","text":"may run problems may want help. don’t know go, getting help may feel impossible. two main places recommend: Stack Overflow RStudio community.Stack Overflow one piece large Stack Exchange network dedicated strictly programming issues.38 Stack Exchange network strictly dedicated question answer format. topic subdomain. unfamiliar, think Stack Exchange like Reddit, Quora, Yahoo! Answers.RStudio Community page community forum created RStudio.39 location members R community—including —ask questions, engage thoughtful dialogue, much . Stack Overflow committed programming languages RStudio Community maintained entirely R users.’s first time asking technical question online, recommend RStudio Community, group generally accessible beginners. , sure create reproducible example, called reprex short, community can best help .40","code":""},{"path":"the-basics.html","id":"reminders","chapter":"6 The basics","heading":"6.7 Reminders","text":"Learning program can difficult frustrating. understand everything immediately. Don’t get . Take breaks don’t push hard self-critical. Eat healthy snack, go exercise, sleep, social, whatever makes happy come back. much happier.ever find bout programming-induced frustration, try one :Drink water.Get sleep. Without sleep wil running 60% less.Eat greens.Shower. Feeling clean can change perspective approach.Get blood flowing! Go walk. squats pushups.","code":""},{"path":"r-as-a-calculator.html","id":"r-as-a-calculator","chapter":"7 R as a calculator","heading":"7 R as a calculator","text":"get going, let’s find footing. R statistical programming language. means R math pretty well . chapter ’ll learn basics using R including:arithmetic operatorscreating assigning variablesusing functions","code":""},{"path":"r-as-a-calculator.html","id":"arithmetic-operators","chapter":"7 R as a calculator","heading":"7.1 Arithmetic Operators","text":"remember PEMDAS? , quick refresher PEMDAS specifies order operations math expression. math inside parentheses, exponents, multiplication division addition subtraction. can’t write math , need type . basic arithmetic operators^ : exponentiation (exponents) [E]* : multiplication [M]/ : division [D]+ : addition []- : subtraction [S]can used together parentheses [P] ( ) determine order operations (PEMDAS)three different ways execute code inside RStudio41. easiest way cursor line code like execute. execute hold command + enter (Mac) control + enter (PC). Alternatively can press Run button top source page.Now, try mathematic expressions console.","code":""},{"path":"r-as-a-calculator.html","id":"variable-assignment","chapter":"7 R as a calculator","heading":"7.2 Variable assignment","text":"’m sure recall basic algebra questions like \\(y = 3x - 24\\). equation, x y variables represents value. often need create variables represent value set values. R, refer variables objects. Objects can single number, set words, matrixes, many things.create object need assign value. Object assignment done assignment operator looks like <-. can automagically insert assignment operator opt + -.Let’s work example. solve y x equal 5.First, need assign 5 variable x.want see contents object, can print . print object can type name .can reference value x stores mathematic expressions. Now y equal? Now solve y equation!","code":"\nx <- 5\nx## [1] 5\ny <- 3 * x - 24\n\ny## [1] -9"},{"path":"r-as-a-calculator.html","id":"functions","chapter":"7 R as a calculator","heading":"7.3 Functions","text":"Functions special kind R object. simply, function object performs action (usually) produces output. Functions exist simplify task. can identify function parentheses appended function name. function looks like function_name().R many functions come built . collection functions come box R called *Base R**.example simple base R function sum(). sum() takes number inputs calculates sum inputs.can run sum() without providing inputs.can provide inputs (formally called function arguments) sum(). example find sum 10 writeThe sum single number number . can provide arguments sum(). Additional arguments specified separating commas—e.g. function(argument_1, argument_2).find sum 10, 3, 2 write sum(10, 3, 2).Much analysis done functions. become much comfortable rather quickly.ever need know function works, can look help page typing ?function_name() console. bring documentation page bottom right pane.","code":"\nsum()## [1] 0\nsum(10)## [1] 10\nsum(10, 3, 2)## [1] 15"},{"path":"r-as-a-calculator.html","id":"data-representations","chapter":"7 R as a calculator","heading":"7.4 Data Representations","text":"Often need store collections data together—perhaps daily reported cases COVID, number voters per state, etc. two values together usually store called vector. Vectors representation one-dimensional data. Think vector like column table—table two dimensional x y axes. think vectors way, restriction vector elements must type hard grasp.create vector use function c(), short combine, argument value vector. example c(1, 3, 5) creates vector three elements. value referred element vector. number elements vector referred length. can math vectors similarly used R like basic calculator.Notice add value 1 element vector. vector calculations vectors elements generally either length scalar (vector length 1).Look example add vector length odds object. elements added position. first position (1 4 respectively) added together. second positions added together, forth.Now, important also note types data work R besides numeric data. broad category called character string data. like think words, things look like words , alternatively, things just don’t look like numbers.represent character data using quotation marks. example object comes built R called letters. print letters can see contains 26 elements—one letters alphabet. can see many elements vector function length(). length(letters) returns 26.Headache prevention: many people confuse quote things . good rule thumb objects (things ’ve assigned created <-) quoted values . time ’ll get better hang , promise.R different type vectors encounter (can even make !) don’t feel like know right away.","code":"\nodds <- c(1, 3, 5)\n\n# make odds even by adding 1\nodds + 1 ## [1] 2 4 6\nodds + c(4, 2, 0)## [1] 5 5 5"},{"path":"r-as-a-calculator.html","id":"extensions-to-r-packages","chapter":"7 R as a calculator","heading":"7.5 Extensions to R (packages)","text":"R created statistical programming language, designed intention extended include even functionality. Extensions R called packages. R packages often provide set functions accomplish specific kind task.analyze, manipulate, visualize data, use number different packages . Throughout book become familiar set packages together known Tidyverse.R packages come installed box. need install selves. Base R includes function called install.packages(). install.packages() download specified package CRAN install us.download packages, must tell install.packages() package download. provide name package argument install.packages(). name package needs put quotations install.packages(\"package-name\").Note: putting text quotations creating called character string.Reminder: create objects assignment operator <-.don’t use quotes (create character string), R thinks referring object created.","code":""},{"path":"r-as-a-calculator.html","id":"exercise","chapter":"7 R as a calculator","heading":"7.5.1 Exercise","text":"Use new knowledge functions installing packages install tidyverse.","code":"\ninstall.packages(\"tidyverse\")"},{"path":"r-as-a-calculator.html","id":"loading-packages","chapter":"7 R as a calculator","heading":"7.6 Loading Packages","text":"Now installed tidyverse, going need know make available use. load package, use function library(). Oddly, though, specifying package load, put name quotations.Note: best practice load packages top R script.Notice message . load tidyverse, actually loading eight packages ! packages listed “Attaching packages” well associated versions.’ll also see messages says Conflicts. conflicts indicate package loaded function name one already loaded. format looks like packagename::function(). case, can see functions filter() lag() dplyr conflict functions name base stats installation. use filter() lag() using version dplyr.now successfully installed loaded tidyverse. Next, begin learn visually analyze data!","code":"\nlibrary(tidyverse)## ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──## ✓ ggplot2 3.3.0     ✓ purrr   0.3.4\n## ✓ tibble  3.0.5     ✓ dplyr   1.0.2\n## ✓ tidyr   1.1.2     ✓ stringr 1.4.0\n## ✓ readr   1.4.0     ✓ forcats 0.5.0## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n## x dplyr::filter() masks stats::filter()\n## x dplyr::lag()    masks stats::lag()"},{"path":"r-as-a-calculator.html","id":"whats-next","chapter":"7 R as a calculator","heading":"7.7 What’s next?","text":"Now rudimentary foundation underneath embark upon long journey—journey learn analyze data R RStudio. following chapters learn program R creating compelling graphics, manipulating data, performing statistical tests.next chapter dive right working data. begin learning read data R. , work first visual analysis focus skills used general data manipulation.","code":""},{"path":"reading-data.html","id":"reading-data","chapter":"8 Reading data","heading":"8 Reading data","text":"Requisite data analysis data. Making data available analyse always easiest tasks. chapter review data imported formats may take. complete chapter get going first analaysis!","code":""},{"path":"reading-data.html","id":"background","chapter":"8 Reading data","heading":"8.1 Background","text":"three general sources social scientists receive access data: 1) text files, 2) databases, 3) application programming interfaces (APIs). Frankly, though age “big data,” always able interface directly sources. partnerships public private often receive shared data. example, BARI’s work Boston Police Department provides annual access crime data. BARI’s access limited. credentials log database perform queries. presented called flat text file—hink Word document frills whatsoever. focus flat text files chapter.Flat text files sufficient 85% data needs. Now, mean flat text file? flat text file file stores data plain text. words, can open text file actually read data eyes screen reader. long tech pundits believed—still —text data thing past. Perhaps may true future, plain text still persists good reasons . Since plain text extremely simple lightweight usually take much memory. Also, fancy embellishing data plain text file, can easily shared machine machine without concern becoming dependent specific tool software.","code":""},{"path":"reading-data.html","id":"actually-reading-data","chapter":"8 Reading data","heading":"8.2 Actually Reading Data","text":"Within tidyverse package called readr (pronounced read-r) use reading rectangular data text files. just threw phrase rectangular data . fair actually describe means. Rectangular resembles table consist rows columns. technical terms rectangular data two-dimensional data structure rows columns.Note rows consist observations columns consist variables.42To get started, let us load tidyverse load readr us.likely seen encountered flat text files wild form csv. important know csv stands help understand actually . stands comma separated values. _csv_s flat text data file data rectangular! new line file indicates new row. Within row, comma indicates new column. opened one text editor like text edit notepad csv look something like .read csv use readr::read_csv() function. read_csv() read csv file create tibble. tibble type data structure interacting throughout book. tibble rectangular data structure rows columns. Since csv contains rectangular data, natural stored tibble.Note: syntax used referencing function namespace (package name). syntax pkgname::function(). means read_csv() function package readr. something see frequently websites like StackOverflow.look arguments read_csv() entering ?read_csv() console. notice many arguments can set. give lot control R read data. now, time, need concerned extra arguments. need tell R data file lives. haven’t deduced help page yet, supply first argument file. argument either path file, connection, literal data (either single string raw vector).Note: see word string, means values inside quotations—.e. “string”.read dataset use next chapter. data stored file named acs_edu.csv. can try reading file path.Oops. ’ve got red text never fun. Except, important error message , frankly, get lot.says:Error: ‘acs_edu.csv’ exist current working directoryI’ve bolded two portions error message. Take moment think error telling .weren’t able figure just impatient (like ): error telling us R looked file provided acs_edu.csv find . usually means ’ve either misspelled file name, told R look appropriate folder (.k.. directory).acs_edu.csv actually lives directory called data. tell R—computer system, really—file write data/acs_edu.csv. tells R first enter data directory look acs_edu.csv file.Now, read acs_edu.csv file!really good! Except, happened function ran. data imported saved anywhere means able interact . saw output data. order interact data need assign object.Reminder: assign object assignment operator <-—.e. new_obj <- read_csv(\"file-path.csv\"). Objects things interact tibble. Functions read_csv() usually, always, modify create objects.order interact data, let us store output tibble object called acs.Notice now data printed console. good sign! means R read data stored properly acs object. don’t store function results, results (usually) printed . print object, can just type ’s name console.sometimes little overwhelming view. previewing data, function dplyr::glimpse() (namespace notation ) great option. Try using function glimpse() first argument acs object.","code":"\nlibrary(tidyverse) column_a, column_b, column_c\n10, \"these are words\", .432\n1, \"and more words\", 1.11\nread_csv(\"acs_edu.csv\")\n\n## Error: 'acs_edu.csv' does not exist in current working directory \n##   ('/Users/Josiah/GitHub/urban-commons-toolkit').\nread_csv(file = \"data/acs_edu.csv\") \n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   med_house_income = col_double(),\n#>   less_than_hs = col_double(),\n#>   hs_grad = col_double(),\n#>   some_coll = col_double(),\n#>   bach = col_double(),\n#>   white = col_double(),\n#>   black = col_double()\n#> )\n#> # A tibble: 1,456 x 7\n#>    med_house_income less_than_hs hs_grad some_coll  bach white   black\n#>               <dbl>        <dbl>   <dbl>     <dbl> <dbl> <dbl>   <dbl>\n#>  1           105735       0.0252   0.196     0.221 0.325 0.897 0.0122 \n#>  2            69625       0.0577   0.253     0.316 0.262 0.885 0.0171 \n#>  3            70679       0.0936   0.173     0.273 0.267 0.733 0.0795 \n#>  4            74528       0.0843   0.253     0.353 0.231 0.824 0.0306 \n#>  5            52885       0.145    0.310     0.283 0.168 0.737 0.0605 \n#>  6            64100       0.0946   0.294     0.317 0.192 0.966 0.00256\n#>  7            37093       0.253    0.394     0.235 0.101 0.711 0.0770 \n#>  8            87750       0.0768   0.187     0.185 0.272 0.759 0.0310 \n#>  9            97417       0.0625   0.254     0.227 0.284 0.969 0.00710\n#> 10            43384       0.207    0.362     0.262 0.124 0.460 0.105  \n#> # … with 1,446 more rows\nacs <- read_csv(file = \"data/acs_edu.csv\") \nacs\n#> # A tibble: 1,456 x 7\n#>    med_house_income less_than_hs hs_grad some_coll  bach white   black\n#>               <dbl>        <dbl>   <dbl>     <dbl> <dbl> <dbl>   <dbl>\n#>  1           105735       0.0252   0.196     0.221 0.325 0.897 0.0122 \n#>  2            69625       0.0577   0.253     0.316 0.262 0.885 0.0171 \n#>  3            70679       0.0936   0.173     0.273 0.267 0.733 0.0795 \n#>  4            74528       0.0843   0.253     0.353 0.231 0.824 0.0306 \n#>  5            52885       0.145    0.310     0.283 0.168 0.737 0.0605 \n#>  6            64100       0.0946   0.294     0.317 0.192 0.966 0.00256\n#>  7            37093       0.253    0.394     0.235 0.101 0.711 0.0770 \n#>  8            87750       0.0768   0.187     0.185 0.272 0.759 0.0310 \n#>  9            97417       0.0625   0.254     0.227 0.284 0.969 0.00710\n#> 10            43384       0.207    0.362     0.262 0.124 0.460 0.105  \n#> # … with 1,446 more rows\nglimpse(acs)\n#> Rows: 1,456\n#> Columns: 7\n#> $ med_house_income <dbl> 105735, 69625, 70679, 74528, 52885, 64100, 37093, 87…\n#> $ less_than_hs     <dbl> 0.02515518, 0.05773956, 0.09364548, 0.08426318, 0.14…\n#> $ hs_grad          <dbl> 0.19568768, 0.25307125, 0.17332284, 0.25298192, 0.31…\n#> $ some_coll        <dbl> 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.283073…\n#> $ bach             <dbl> 0.32473048, 0.26167076, 0.26677159, 0.23124279, 0.16…\n#> $ white            <dbl> 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.737102…\n#> $ black            <dbl> 0.012213740, 0.017090069, 0.079514240, 0.030640286, …"},{"path":"reading-data.html","id":"other-common-data-formats","chapter":"8 Reading data","heading":"8.3 Other common data formats","text":"csv files going ubiquitous, invariably run data formats. workflow almost always . want read excel files, can use function readxl::read_excel() readxl package.Another common format tsv stands tab separated format. readr::read_tsv() able assist .reason special delimiters like |, readr::read_delim() function work best. example readr::read_delim(\"file-path\", delim = \"|\") trick!Additionally, another extremely common data type json short javascript object notation. json data type usually read directly text file interact API. happen encounter json flat text file, use jsonlite package. jsonlite::read_json().new skill ready first analysis. next chapter perform first graphical analysis using package ggplot2 tidyverse.","code":"\nacs_xl <- readxl::read_excel(\"data/acs_edu.xlsx\")\n\nglimpse(acs_xl)\n#> Rows: 1,456\n#> Columns: 7\n#> $ med_house_income <dbl> 105735, 69625, 70679, 74528, 52885, 64100, 37093, 87…\n#> $ less_than_hs     <dbl> 0.02515518, 0.05773956, 0.09364548, 0.08426318, 0.14…\n#> $ hs_grad          <dbl> 0.19568768, 0.25307125, 0.17332284, 0.25298192, 0.31…\n#> $ some_coll        <dbl> 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.283073…\n#> $ bach             <dbl> 0.32473048, 0.26167076, 0.26677159, 0.23124279, 0.16…\n#> $ white            <dbl> 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.737102…\n#> $ black            <dbl> 0.012213740, 0.017090069, 0.079514240, 0.030640286, …"},{"path":"exploratory-visual-analysis.html","id":"exploratory-visual-analysis","chapter":"9 Exploratory Visual Analysis","heading":"9 Exploratory Visual Analysis","text":"next part book introduce visual data exploration use R package ggplot2. ask questions data, visualize relationships, draw inferences graphics develop.image R Data Science renowned representation data analysis workflow. concept map encompasses need get data (import), clean (tidy), explore , finally communicate insights. box blue representation exploratory data analysis (EDA). performing EDA find transforming data—creating new variables, aggregating, etc.—visualizing , creating statistical models.chapter focus visualization step EDA. heard trope “image worth thousand words.” ’d take leap say good visualization worth ten thousand words. older statistical manual National Institute Standards Technology (NIST) beautifully lays role visualization plays EDA.reason heavy reliance graphics nature main role EDA open-mindedly explore, graphics gives analysts unparalleled power , enticing data reveal structural secrets, always ready gain new, often unsuspected, insight data. combination natural pattern-recognition capabilities possess, graphics provides, course, unparalleled power carry .43In following section, become acquainted package ggplot2 use perform visual analysis American Community Survey. walk process building chart ground drawing inferences along way.","code":""},{"path":"exploratory-visual-analysis.html","id":"the-american-community-survey","chapter":"9 Exploratory Visual Analysis","heading":"9.1 The American Community Survey","text":"first data exploration work data American Community Survey (ACS). ACS central Urban Informatics (UI), exhibit primary characteristic rely upon UI—namely naturally occurring. topic explore depth later. order use ACS data, must understand data actually working . ACS one fundamental data sets American social sciences. ACS administered US Census Bureau done much different purposes. Article Section 2 US Constitution legislates decennial census.. . . [] enumeration shall made within three Years first Meeting Congress United States, within every subsequent Term ten Years, Manner shall Law direct.clause requires US government conduct complete counting every single individual United States purposes determining many congressional representatives state . censuses provided detailed image many people US, lacked much information beyond . first census asked household “number free white males 16 years” “16 years upward,” “number free White females,” “number free persons, ”number slaves\"44 Since breadth questions asked census increased well supplementary sources information.ACS developed response two shortcomings decennial census. first census occurrs every ten years. , still , need consistent current data. censuses infrequent, also provide colorful picture lives within US. Local, state, federal governments desired context constituents .ACS developed first officially released 200545. ACS uses “series monthly samples” “produce annual estimates small areas (census tracts block groups) formerly surveyed via decennial census long-form sample”46. Catherine Rampell wrote New York times“tells Americans poor , rich , suffering, thriving, people work, kind training people need get jobs, languages people speak, uses food stamps, access health care, .”47The impact ACS wide stretching funding social research.","code":""},{"path":"exploratory-visual-analysis.html","id":"understanding-acs-estimates","chapter":"9 Exploratory Visual Analysis","heading":"9.1.1 Understanding ACS Estimates","text":"Continuous sampling done US Census Bureau occurs monthly basis used produce annual estimates48. two different types estimates one can retrieve ACS. 1-year 5-year estimates. kind estimate serves different purpose.choosing 1-year 5-year estimates making tradeoff. 1-year estimates provide us current data possible expense smaller sample size. means estimates reliable 5-year estimates collected period 60 months. hand, consider 5-year estimates, benefit large sample size increased reliability, lose ability make statements single year.cases 5-year estimates used researchers analyzing populations rates derived five years data collection. requires , researcher, qualify statement effect “rate unemployment 2014-2018 4%”49. , unable use consecutive 5-year estimates analyze annual trends. case need analyse annual trends 1-year estimates recommended.also another important tradeoff one must consider using ACS data unit analysis. census ACS conducted household level. However, estimates provided geographic areas. geographic areas hierarchy going block groups smallest level geography, census tracts, counties. Beyond counties geographies state level even larger. Urban informatics inherently focuses micro, arguably meso, unit analysis.following analysis done using Massachusetts census indicators published BARI. dataset based 2017 5-year estimates ACS tract level.","code":""},{"path":"exploratory-visual-analysis.html","id":"a-first-visualization","chapter":"9 Exploratory Visual Analysis","heading":"9.2 A first visualization","text":"first introduction R, explore relationship education income Massachusetts.","code":""},{"path":"exploratory-visual-analysis.html","id":"familiarize-yourself","chapter":"9 Exploratory Visual Analysis","heading":"9.2.1 Familiarize yourself","text":"one best way begin exploratory analysis guarantee interesting outcomes. one begins EDA, must know data actually contain.make life little bit easier, ’re going use accompanying R package called uitk provide data. First ’re going need install GitHub lives. ’ll first need install another package called remotes help us . Run lines.Now package installed, load tidyverse uitk.uitk package exports (makes available) object called acs_edu. contains data demographic information every census tract Massachusetts. Print acs_edu console. see?executing object name printing object.## # tibble: 1,456 x 7 printed top followed column names, types—e.g. <dbl>—respective values , far left see numbers 1 10 row values indicating row number.Let us dissect # tibble: 1,456 x 7 little bit . alone quite informative. tells us type object working tibble 1,456 rows 7 columns.tibble method representing rectangular data similar table one may create within Excel rows columns. working tibbles try adhere called principles tidy data50. three key principles keep mind working rectangular data.variable forms column.observation forms row.value represents combination observation variable.can often confusion variable observation. Tidy Data Hadley Wickham write “variable contains values measure underlying attribute (like height, temperature, duration) across units. observation contains values measured unit (like person, day, race) across attributes.”51Say tibble survey respondents. case row respondent column variable associated respondent. something age, birth date, respondents response survey question.case acs_edu tibble, unit observation, aka row, census tract. variable measures different characteristic census tract. example, column med_house_income estimate median household income given census tract. columns indicate proportion population meets criteria.one know criteria columns represent? brings us importance column names. Column names descriptors corresponding variables. surprsingly difficult task! acs_edu can infer—though always documentation supplement data—variables measure income, educational attainment rates, race.","code":"\ninstall.packages(\"remotes\")\nremotes::install_github(\"josiahparry/uitk\")\nlibrary(tidyverse)\nlibrary(uitk)\nacs_edu\n#> # A tibble: 1,456 x 7\n#>    med_house_income less_than_hs hs_grad some_coll  bach white   black\n#>               <dbl>        <dbl>   <dbl>     <dbl> <dbl> <dbl>   <dbl>\n#>  1           105735       0.0252   0.196     0.221 0.325 0.897 0.0122 \n#>  2            69625       0.0577   0.253     0.316 0.262 0.885 0.0171 \n#>  3            70679       0.0936   0.173     0.273 0.267 0.733 0.0795 \n#>  4            74528       0.0843   0.253     0.353 0.231 0.824 0.0306 \n#>  5            52885       0.145    0.310     0.283 0.168 0.737 0.0605 \n#>  6            64100       0.0946   0.294     0.317 0.192 0.966 0.00256\n#>  7            37093       0.253    0.394     0.235 0.101 0.711 0.0770 \n#>  8            87750       0.0768   0.187     0.185 0.272 0.759 0.0310 \n#>  9            97417       0.0625   0.254     0.227 0.284 0.969 0.00710\n#> 10            43384       0.207    0.362     0.262 0.124 0.460 0.105  \n#> # … with 1,446 more rows"},{"path":"exploratory-visual-analysis.html","id":"form-a-question","chapter":"9 Exploratory Visual Analysis","heading":"9.2.2 Form a question","text":"familiarized data working , can begin form question can feasibly explored answered present data. importance domain expertise EDA understated. Without understanding underlying phenomena data measuring extremely difficult come meaningful insights.background sociology. Within sociology, specifically social stratification, believed education leads social prestiege, economic stability, readily accessible white population. Given background data available acs_edu, explore relationship education income. try answer question relationship education income? first look variable isolation try identify relationship may exist two variables.","code":""},{"path":"exploratory-visual-analysis.html","id":"building-a-graph","chapter":"9 Exploratory Visual Analysis","heading":"9.2.3 Building a graph","text":"create visualizations use package ggplot2 tidyverse. ggplot2 previously loaded us loaded tidyverse.Reminder: library(pkg_name) loads package workspace makes functions objects exports available .begin building ggplot, use function ggplot(). two function arguments need aware . data aesthetics mapping first second arguments respectively (e.g. function(first_arg, second_arg)). data tibble wish visualize. case want visualize data acs_edu.begin constructing first visualization ggplot() function using acs_edu object.Reminder: Functions characterised parentheses end name. Functions typically things whereas objects store information.Notice plot entirely empty. defined want visualize. ggplot uses called grammar graphics (expanded upon depth Visualizing Trends Relationships chapter) requires us sequentially build graphs first defining data variables visualized adding layers plot.next step need take define columns want visualize. called aesthetics defined using aes() function supplied mapping argument. purpose aes() tell ggplot columns mapped . important fundamental x y arguments. refer x y axes chart begin make.begin analyze relationship med_house_income bach (bachelor’s degree attainment rate), due diligence looking distribution first. Let us start med_house_income column. exploring single variable, want supply x argument aes().Note data mapping arguments set positionally. Since data first argument put tibble first position need specify argument name directly. wanted explicit function arguments write ggplot(data = acs_edu, mapping = aes(x = med_house_income)).Alright, making progress. can see x axis now filled bit . axis breaks labeled axis . order see data graphical representation, need determine want see data. , sort geometry used visualize variable.add geometry ggplot, use plus sign + signifies adding layer top basic graph. many ways can visualize univariate data histogram stood test time. create histogram add geom_histogram() layer existing ggplot code.Note: ensure code legible add new layer line. RStudio manage indentation . Code readibility important thank later instilling good practices start.histogram illustrates distribution median household income state Massachusetts. median value seems sit somewhere around $75k outliers near $250k well demonstrating right skew.Reminder: skew [observations].Usually look distributions wealth extremely right skewed meaning people make outrageous amount money. interesting histogram rather normally distributed almost challenging intuition. ACS something called top-coding. Top-coding practice creating ceiling value. example, tract median household income $1m, reduced top-coded value—appears $250k. creates called censored data.Censored data: data “value measurement observation partially known.”5253Let us now create histogram second variable interest, bach.histogram illustrates distribution bachelor degree attainment rate (proportion people bachelor’s degree) across census tracts Massachusetts. homework ahead time, know national attainment rate 2018 people 25 ~35%54. histogram shows within MA lot variation attainment rate low 0% high 60%. steep peak distribution tells us fair amount variation distribution.Now intuition distribution characteristics med_house_income bach, can begin try answer question effect education median household income? phrasing question determine visualize data.stating research questions often phrase effect x y? formulation determining bach, independent variable, plotted x axis med_house_income plotted y axis. visualize bivariate relationship create scatter plot.structure phrasing useful continuity verbal communication, graphical representation, hypothesis testing.can visualize relationship adding additional mapped aesthetics. case, map x y arguments aes() function. Rather adding histogram layer, need create scatter plot. Scatter plots created plotting points (x, y) pair. get effect use geom_point() layer.scatter plot provides lot information. see positive linear trend—bach value increases med_house_income variable. looking scatter plot looking see consistent pattern can sussed .scatterplot can see linear pattern. points scatter plot closer eachother demonstrate less spread, means stronger relationship two variables. Imagine drew line middle points, want point close line possible. point away line, variation . cases often create linear regression models estimate relationship.Using ggplot, can plot estimated linear regression line top scatterplot. done geom_smooth() layer. default, geom_smooth() plot linear relationship. , need specify kind smoothing like. plot estimated linear model, set method = \"lm\".Wonderful! finish graphic, add informative labels. Labels live layer created labs(). argument maps aesthetic—e.g. x y. default ggplot uses column names axis labels, labels usually legible.Let’s give plot title better labels axes. set following arguments labs()x = \"% population Bachelor's Degree\"y = \"Median Household Income\"title = \"Relationship Education Income\"Note argument placed new line. , improve readability.can determine graph? Take minutes write see can infer .Consider asking questions:relationship variables?relationship linear?direction trend go?spread value pairs?outliers?chart ilicits lines inquiry. example, sociological literature well documented achievement gap. achievement gap can traced along racial lines—though inherently caused race rather intertwined . Can seen tracts Massachusetts?can visualize third variable chart mapping another aesthetic. add third variable visualization generally trying illustrate group membership size / magnitude third variable. Due large number points chart already, may benefit mapping color rather size—imagine 1,000 points overlapping even already .can map proportion population white color points. setting color aesthetic white. ’re , let us include subtitle informative viewer.can conclude now? addition third variable increase decrease utility scatter plot? trend seem mediated race? ’ll leave questions answer.’ve now completed first visual analysis. ’ve learned create publication ready histograms scatter plots using ggplot2. small feat!chapter provided data used visualization exercises. ’re going want able visualize analyze data. next chapter introduces reading data common file formats may encounter.","code":"\nggplot(acs_edu)\nggplot(acs_edu, aes(x = med_house_income))\nggplot(acs_edu, aes(x = med_house_income)) + \n  geom_histogram()\nggplot(acs_edu, aes(x = bach)) + \n  geom_histogram()\nggplot(acs_edu, aes(x = bach, y = med_house_income)) +\n  geom_point()\nggplot(acs_edu, aes(x = bach, y = med_house_income)) +\n  geom_point() +\n  geom_smooth(method = \"lm\")\n#> `geom_smooth()` using formula 'y ~ x'\nggplot(acs_edu, aes(x = bach, y = med_house_income)) +\n  geom_point() +\n  geom_smooth(method = \"lm\") +\n  labs(x = \"% of population with a Bachelor's Degree\",\n       y = \"Median Household Income\",\n       title = \"Relationship between Education and Income\")\n#> `geom_smooth()` using formula 'y ~ x'\nggplot(acs_edu, aes(x = bach, y = med_house_income, color = white)) +\n  geom_point(alpha = .4) +\n  geom_smooth(method = \"lm\") + \n  labs(x = \"% of population with a Bachelor's Degree\",\n       y = \"Median Household Income\",\n       title = \"Relationship between Education and Income\",\n       subtitle = \"Colored by whiteness\") \n#> `geom_smooth()` using formula 'y ~ x'"},{"path":"general-data-manipulation.html","id":"general-data-manipulation","chapter":"10 General data manipulation","heading":"10 General data manipulation","text":"spent last chapter performing first exploratory visual analysis. visualizations able inductively conclude median household income proportion population bachelors degree increases, share population white.able make wonderful visualizations, skip number steps exploratory analysis process! Arguably important skipped curation dataset. ACS dataset already cleaned curated . almost always case. , ’re going spend chapter learning ways selecting subsets data.Now ability read data, important get comfortable handling . people call process rearranging, cleaning, reshaping data massaging, plumbing, engineering, myriad names. , refer data manipulation. preferable catch-term illicit images toilets Phoebe Buffay (masseuse!).may heard 80/20 rule, least one many 80/20 rules. 80/20 rule ’m referring idea data scientists spend 80% time cleaning manipulating data. 20% analysis part—creating statistics models. mention working data mostly data manipulation statistics. prepared get hands dirty data.say find messy-unsanitized-gross--fun--look-data time. , really important skills clean data. Right now ’re going go foundational skills learn select columns rows, filter data, create new columns, arrange data. , using dplyr package tidyverse.data used last chapter select variables annual census data release team Boston Area Research Initiative (BARI) provides. census indicators used provide picture changing landscape Boston Massachusetts generally. chapter work rather real life scenario may well encounter using BARI data.","code":""},{"path":"general-data-manipulation.html","id":"scenario","chapter":"10 General data manipulation","heading":"10.1 Scenario","text":"local non-profit interested commuting behavior Greater Boston residents. adviser suggested assist non-profit work. ’ve just coffee project manager learn specific research question . seems extension Green Line great interest . spoke length history Big Dig impact commuters working city. poured conversation spatial social stratification city. looks watch realizes ’s miss commuter train home. shake hand, thank time (free coffee ’re grad student), affirm email week data work .’re back apartment, french press next laptop—though close—notes open, ready begin. pore notes realize now rather good understanding Green Line Extensions impact Big Dig , really idea commuting behavior Greater Boston interested . realize even confirm constitutes Greater Boston area. push coffee grinds pour first cup coffee. take least two cups coffee.scenario sounds like something stress dream. scenario found many times sure find one point well. comfortable get data analysis asking good questions, guided directed can make seemingly vague objectives.","code":""},{"path":"general-data-manipulation.html","id":"getting-physical-with-the-data","chapter":"10 General data manipulation","heading":"10.2 Getting physical with the data","text":"data used chapters one two curated annual census indicator release BARI[^indicators]. dataset acs_edu created. use data provide relevant data relating commuting Greater Boston Area. first thing ’ll notice data large somewhat unforgiving work . better way get started big data?using tidyverse read manipulate data (last chapter). Recall load tidyverse using library(tidyverse).Refresher: tidyverse collection packages used data analysis. load tidyverse loads readr, ggplot2, dplyr us, among packages. now, though, relevant packages.Load tidyverse uitk.Wonderful! data accessible R, important get familiar data . means need know variables available us get feel values variables represent.Try printing acs_raw object console.Oof, yikes. ’s bit messy! mention R even print columns. ’s ran room. ’re working wide data (many columns), ’s generally best view preview data. function dplyr::glimpse() can help us just . glimpse() provide summary overview data frame. output first tell us many rows columns . , order appears object, prints column name followed type—e.g. <dbl> , <chr> double character—first values column.Provide acs_raw argument glimpse().Much better, right? frankly still lot text, way presented rather useful. variable written followed data type, .e. <dbl>, preview values column. <dbl> make sense yet, worry. go data types depth later. Data types fun think important fun!acs_raw dataset acs_edu created. can see, many, many, many different variables ACS data provide us . tip iceberg.Now think.Looking preview data, columns think useful non-profit understanding commuter behavior? “” always best answer. providing much data one may moved inaction now must determine variables useful use .spotted columns commute_less10, commute1030, commute3060, commute6090, commute_over90, eyes intuition served well! variables tell us proportion sampled population given census tract commute times fall within indicated duration range, .e. 30-60.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\nacs_raw\nglimpse(acs_raw)"},{"path":"general-data-manipulation.html","id":"selecting","chapter":"10 General data manipulation","heading":"10.2.1 select()ing","text":"now intuition important variables, next problem soon arises: isolate just variables? Whenever find needing select deselect columns tibble dplyr::select() main function go . select() selects variables tibble returns another tibble.work use select(), refer help documentation see can get somewhat intuition running ?select() console. press enter, documentation page pop RStudio.reasons directing towards function documentation.get comfortable navigating RStudio IDE.Expose R vocabulary.Soon ’ll advanced book figure way functions work !Perhaps help documentation little overwhelming absolutely confusing. ’s okay. ’s just exposure! exposure things make sense. Let’s tackle arguments one one..data: tbl. main verbs S3 generics provide methods tbl_df(), dtplyr::tbl_dt() dbplyr::tbl_dbi().want take away argument definition “tbl.” Whenever read tbl think self “oh, just tibble.” recall, read rectangular data readr::read_csv() readr::read_*() function end tibble. verify case, can double check objects using function .tbl(). function takes object returns logical value (TRUE FALSE) statement true. Let’s double check acs_raw fact tbl.Aside: object type usually function let’s test objects type follow structure .obj_type() occasional is_obj_type(). go object types later.can read asking R question “object tbl?” resultant output .tbl(acs_raw) TRUE. Now can doubly confident object can used select().second argument select() little bit difficult grasp, don’t feel discouraged isn’t clicking right away. lot written argument definition feel necessary understand get go....: One unquoted expressions separated commas. can treat variable names like positions, can use expressions like x:y select ranges variables...., referred “dots” means can pass number arguments function. Translating “one unquoted expressions separated commas” regular person speak reiterates can multiple arguments passed select(). “Unquoted expressions” means want select column put column name quotes.“can treat variable names like positions” translates “want first column can write number 1 etc.” , want first tenth variable can pass 1:10 argument ....important thing ... assign ... argument, example . ... = column_a correct notation. provide column_a alone.always, makes sense see practice. now go many ways can select columns using select(). gotten hang selecting columns return back assisting non-profit.go :selecting nameselecting positionselect helpers","code":"\nis.tbl(acs_raw)\n#> [1] TRUE"},{"path":"general-data-manipulation.html","id":"selecting-exercises","chapter":"10 General data manipulation","heading":"10.2.2 select()ing exercises","text":"select() enables us choose columns tibble based names. remember unquoted column names.Try :select column name acs_rawThe column name passed .... Recall dots allows us pass “one ore unquoted expressions separated commas.” test statement , select town addition name acs_rawTry :select name town acs_rawGreat, ’re getting hang .Now, addition selecting columns solely based names, can also select range columns using format col_from:col_to. writing select() register want every column including col_from including col_to.Let’s refresh data look like:Try :select columns age_u18 age_o65.Now really throw ! can even reverse order ranges.Try :select columns age_o65 age_u18.","code":"\nselect(acs_raw, name)\n#> # A tibble: 1,478 x 1\n#>    name                                                 \n#>    <chr>                                                \n#>  1 Census Tract 7281, Worcester County, Massachusetts   \n#>  2 Census Tract 7292, Worcester County, Massachusetts   \n#>  3 Census Tract 7307, Worcester County, Massachusetts   \n#>  4 Census Tract 7442, Worcester County, Massachusetts   \n#>  5 Census Tract 7097.01, Worcester County, Massachusetts\n#>  6 Census Tract 7351, Worcester County, Massachusetts   \n#>  7 Census Tract 7543, Worcester County, Massachusetts   \n#>  8 Census Tract 7308.02, Worcester County, Massachusetts\n#>  9 Census Tract 7171, Worcester County, Massachusetts   \n#> 10 Census Tract 7326, Worcester County, Massachusetts   \n#> # … with 1,468 more rows\nselect(acs_raw, name, town)\n#> # A tibble: 1,478 x 2\n#>    name                                                  town         \n#>    <chr>                                                 <chr>        \n#>  1 Census Tract 7281, Worcester County, Massachusetts    HOLDEN       \n#>  2 Census Tract 7292, Worcester County, Massachusetts    WEST BOYLSTON\n#>  3 Census Tract 7307, Worcester County, Massachusetts    WORCESTER    \n#>  4 Census Tract 7442, Worcester County, Massachusetts    MILFORD      \n#>  5 Census Tract 7097.01, Worcester County, Massachusetts LEOMINSTER   \n#>  6 Census Tract 7351, Worcester County, Massachusetts    LEICESTER    \n#>  7 Census Tract 7543, Worcester County, Massachusetts    WEBSTER      \n#>  8 Census Tract 7308.02, Worcester County, Massachusetts WORCESTER    \n#>  9 Census Tract 7171, Worcester County, Massachusetts    BERLIN       \n#> 10 Census Tract 7326, Worcester County, Massachusetts    WORCESTER    \n#> # … with 1,468 more rows\nglimpse(acs_raw)\nselect(acs_raw, age_u18:age_o65)\n#> # A tibble: 1,478 x 4\n#>    age_u18 age1834 age3564 age_o65\n#>      <dbl>   <dbl>   <dbl>   <dbl>\n#>  1   0.234   0.202   0.398   0.166\n#>  2   0.181   0.151   0.461   0.207\n#>  3   0.171   0.214   0.437   0.178\n#>  4   0.203   0.227   0.436   0.133\n#>  5   0.177   0.203   0.430   0.190\n#>  6   0.163   0.237   0.439   0.162\n#>  7   0.191   0.326   0.380   0.102\n#>  8   0.202   0.183   0.466   0.148\n#>  9   0.188   0.150   0.462   0.200\n#> 10   0.244   0.286   0.342   0.128\n#> # … with 1,468 more rows\nselect(acs_raw, age_o65:age_u18)\n#> # A tibble: 1,478 x 4\n#>    age_o65 age3564 age1834 age_u18\n#>      <dbl>   <dbl>   <dbl>   <dbl>\n#>  1   0.166   0.398   0.202   0.234\n#>  2   0.207   0.461   0.151   0.181\n#>  3   0.178   0.437   0.214   0.171\n#>  4   0.133   0.436   0.227   0.203\n#>  5   0.190   0.430   0.203   0.177\n#>  6   0.162   0.439   0.237   0.163\n#>  7   0.102   0.380   0.326   0.191\n#>  8   0.148   0.466   0.183   0.202\n#>  9   0.200   0.462   0.150   0.188\n#> 10   0.128   0.342   0.286   0.244\n#> # … with 1,468 more rows"},{"path":"general-data-manipulation.html","id":"selecting-by-position","chapter":"10 General data manipulation","heading":"10.2.3 Selecting by position","text":"“…can treat variable names like positions…”taken argument definition dots .... Like providing name column, can also provide positions (also referred index). previous example, selected name column. can select column ’s position . name second column tibble. select position like :Try :select age_u18 age_o65 positionYou may see going . Just like column names, can select range columns using method index_from:index_to.Try :select columns age_u18 age_o65 using : column positionselect columns reverse order indexesBase R Side Bar: help build intuition, want point base R functionality. Using colon : integers (whole numbers) actually select() specific functionality. something rather handy built directly R. Using colon operator, can create ranges numbers exact way . want create range numbers 1 10, write 1:10. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.scenario, want select last two columns. may know names position. Luckily, ’s function .last_col() handy function enables us select last column. also option get offset last column. offset allow us grab second last column setting offset 1. setting offset, last_col() offset + 1 last column. offset set 1, grabbing second last column.Let’s give shot:last_col() comes another packages called tidyselect imported dplyr. package contains number helper functions. 9 total helpers ’ve already learned one . briefly review four . ’m sure able deduce functions work solely based names. functions :starts_with(): string search columns start withends_with(): string search columns end withcontains(): string search column names positioneverything(): selects remaining columnsEach function take character string searches column headers .Try :find columns start \"med\"select columns end \"per\"find column contains string \"yr\"select columns start med select everything else","code":"\nselect(acs_raw, 2)\n#> # A tibble: 1,478 x 1\n#>    name                                                 \n#>    <chr>                                                \n#>  1 Census Tract 7281, Worcester County, Massachusetts   \n#>  2 Census Tract 7292, Worcester County, Massachusetts   \n#>  3 Census Tract 7307, Worcester County, Massachusetts   \n#>  4 Census Tract 7442, Worcester County, Massachusetts   \n#>  5 Census Tract 7097.01, Worcester County, Massachusetts\n#>  6 Census Tract 7351, Worcester County, Massachusetts   \n#>  7 Census Tract 7543, Worcester County, Massachusetts   \n#>  8 Census Tract 7308.02, Worcester County, Massachusetts\n#>  9 Census Tract 7171, Worcester County, Massachusetts   \n#> 10 Census Tract 7326, Worcester County, Massachusetts   \n#> # … with 1,468 more rows\nselect(acs_raw, 6, 9)\n#> # A tibble: 1,478 x 2\n#>    age_u18 age_o65\n#>      <dbl>   <dbl>\n#>  1   0.234   0.166\n#>  2   0.181   0.207\n#>  3   0.171   0.178\n#>  4   0.203   0.133\n#>  5   0.177   0.190\n#>  6   0.163   0.162\n#>  7   0.191   0.102\n#>  8   0.202   0.148\n#>  9   0.188   0.200\n#> 10   0.244   0.128\n#> # … with 1,468 more rows\nselect(acs_raw, 6:9)\n#> # A tibble: 1,478 x 4\n#>    age_u18 age1834 age3564 age_o65\n#>      <dbl>   <dbl>   <dbl>   <dbl>\n#>  1   0.234   0.202   0.398   0.166\n#>  2   0.181   0.151   0.461   0.207\n#>  3   0.171   0.214   0.437   0.178\n#>  4   0.203   0.227   0.436   0.133\n#>  5   0.177   0.203   0.430   0.190\n#>  6   0.163   0.237   0.439   0.162\n#>  7   0.191   0.326   0.380   0.102\n#>  8   0.202   0.183   0.466   0.148\n#>  9   0.188   0.150   0.462   0.200\n#> 10   0.244   0.286   0.342   0.128\n#> # … with 1,468 more rows\nselect(acs_raw, 9:6)\n#> # A tibble: 1,478 x 4\n#>    age_o65 age3564 age1834 age_u18\n#>      <dbl>   <dbl>   <dbl>   <dbl>\n#>  1   0.166   0.398   0.202   0.234\n#>  2   0.207   0.461   0.151   0.181\n#>  3   0.178   0.437   0.214   0.171\n#>  4   0.133   0.436   0.227   0.203\n#>  5   0.190   0.430   0.203   0.177\n#>  6   0.162   0.439   0.237   0.163\n#>  7   0.102   0.380   0.326   0.191\n#>  8   0.148   0.466   0.183   0.202\n#>  9   0.200   0.462   0.150   0.188\n#> 10   0.128   0.342   0.286   0.244\n#> # … with 1,468 more rows\nselect(acs_raw, last_col())\n#> # A tibble: 1,478 x 1\n#>    m_atown      \n#>    <chr>        \n#>  1 HOLDEN       \n#>  2 WEST BOYLSTON\n#>  3 WORCESTER    \n#>  4 MILFORD      \n#>  5 LEOMINSTER   \n#>  6 LEICESTER    \n#>  7 WEBSTER      \n#>  8 WORCESTER    \n#>  9 BERLIN       \n#> 10 WORCESTER    \n#> # … with 1,468 more rows\nselect(acs_raw, last_col(offset = 1))\n#> # A tibble: 1,478 x 1\n#>    area_acr_1\n#>         <dbl>\n#>  1     23242.\n#>  2      8868.\n#>  3     24610.\n#>  4      9616.\n#>  5     18993.\n#>  6     15763.\n#>  7      9347.\n#>  8     24610.\n#>  9      8431.\n#> 10     24610.\n#> # … with 1,468 more rows\nselect(acs_raw, last_col(offset = 1):last_col())\n#> # A tibble: 1,478 x 2\n#>    area_acr_1 m_atown      \n#>         <dbl> <chr>        \n#>  1     23242. HOLDEN       \n#>  2      8868. WEST BOYLSTON\n#>  3     24610. WORCESTER    \n#>  4      9616. MILFORD      \n#>  5     18993. LEOMINSTER   \n#>  6     15763. LEICESTER    \n#>  7      9347. WEBSTER      \n#>  8     24610. WORCESTER    \n#>  9      8431. BERLIN       \n#> 10     24610. WORCESTER    \n#> # … with 1,468 more rows\nselect(acs_raw, starts_with(\"med\"))\n#> # A tibble: 1,478 x 7\n#>    med_house_income med_gross_rent med_home_val med_yr_built_raw med_yr_built\n#>               <dbl>          <dbl>        <dbl>            <dbl> <chr>       \n#>  1           105735           1640       349000             1988 1980 to 1989\n#>  2            69625            894       230200             1955 1950 to 1959\n#>  3            70679           1454       207200             1959 1950 to 1959\n#>  4            74528            954       268400             1973 1970 to 1979\n#>  5            52885           1018       223200             1964 1960 to 1969\n#>  6            64100            867       232700             1966 1960 to 1969\n#>  7            37093            910       170900             1939 Prior to 19…\n#>  8            87750           1088       270100             1939 Prior to 19…\n#>  9            97417           1037       379600             1981 1980 to 1989\n#> 10            43384           1017       156500             1939 Prior to 19…\n#> # … with 1,468 more rows, and 2 more variables: med_yr_moved_inraw <dbl>,\n#> #   med_yr_rent_moved_in <dbl>\nselect(acs_raw, ends_with(\"per\"))\n#> # A tibble: 1,478 x 8\n#>    fam_pov_per fam_house_per fem_head_per same_sex_coup_p… grand_head_per\n#>          <dbl>         <dbl>        <dbl>            <dbl>          <dbl>\n#>  1      0.0475         0.797       0.0899            0            0      \n#>  2      0.0652         0.698       0.120             0            0.00583\n#>  3      0.0584         0.659       0.114             0            0      \n#>  4      0.0249         0.657       0.121             0            0      \n#>  5      0.198          0.531       0.158             0            0.00946\n#>  6      0.0428         0.665       0.0603            0            0.0353 \n#>  7      0.0762         0.632       0.227             0            0.00643\n#>  8      0.101          0.636       0.0582            0.297        0.0260 \n#>  9      0.0149         0.758       0.0721            0            0.00434\n#> 10      0.0954         0.460       0.225             0            0.0279 \n#> # … with 1,468 more rows, and 3 more variables: vacant_unit_per <dbl>,\n#> #   renters_per <dbl>, home_own_per <dbl>\nselect(acs_raw, contains(\"yr\"))\n#> # A tibble: 1,478 x 4\n#>    med_yr_built_raw med_yr_built  med_yr_moved_inraw med_yr_rent_moved_in\n#>               <dbl> <chr>                      <dbl>                <dbl>\n#>  1             1988 1980 to 1989                2004                 2012\n#>  2             1955 1950 to 1959                2003                 2010\n#>  3             1959 1950 to 1959                2007                 2012\n#>  4             1973 1970 to 1979                2006                 2011\n#>  5             1964 1960 to 1969                2006                 2011\n#>  6             1966 1960 to 1969                2000                 2009\n#>  7             1939 Prior to 1940               2011                 2012\n#>  8             1939 Prior to 1940               2006                 2012\n#>  9             1981 1980 to 1989                2004                 2012\n#> 10             1939 Prior to 1940               2011                   NA\n#> # … with 1,468 more rows\nselect(acs_raw, contains(\"yr\"), everything())\n#> # A tibble: 1,478 x 59\n#>    med_yr_built_raw med_yr_built med_yr_moved_in… med_yr_rent_mov… ct_id_10\n#>               <dbl> <chr>                   <dbl>            <dbl>    <dbl>\n#>  1             1988 1980 to 1989             2004             2012  2.50e10\n#>  2             1955 1950 to 1959             2003             2010  2.50e10\n#>  3             1959 1950 to 1959             2007             2012  2.50e10\n#>  4             1973 1970 to 1979             2006             2011  2.50e10\n#>  5             1964 1960 to 1969             2006             2011  2.50e10\n#>  6             1966 1960 to 1969             2000             2009  2.50e10\n#>  7             1939 Prior to 19…             2011             2012  2.50e10\n#>  8             1939 Prior to 19…             2006             2012  2.50e10\n#>  9             1981 1980 to 1989             2004             2012  2.50e10\n#> 10             1939 Prior to 19…             2011               NA  2.50e10\n#> # … with 1,468 more rows, and 54 more variables: name <chr>, total_pop <dbl>,\n#> #   pop_den <dbl>, sex_ratio <dbl>, age_u18 <dbl>, age1834 <dbl>,\n#> #   age3564 <dbl>, age_o65 <dbl>, for_born <dbl>, white <dbl>, black <dbl>,\n#> #   asian <dbl>, hispanic <dbl>, two_or_more <dbl>, eth_het <dbl>,\n#> #   med_house_income <dbl>, pub_assist <dbl>, gini <dbl>, fam_pov_per <dbl>,\n#> #   unemp_rate <dbl>, total_house_h <dbl>, fam_house_per <dbl>,\n#> #   fem_head_per <dbl>, same_sex_coup_per <dbl>, grand_head_per <dbl>,\n#> #   less_than_hs <dbl>, hs_grad <dbl>, some_coll <dbl>, bach <dbl>,\n#> #   master <dbl>, prof <dbl>, doc <dbl>, commute_less10 <dbl>,\n#> #   commute1030 <dbl>, commute3060 <dbl>, commute6090 <dbl>,\n#> #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n#> #   by_walk <dbl>, total_house_units <dbl>, vacant_unit_per <dbl>,\n#> #   renters_per <dbl>, home_own_per <dbl>, med_gross_rent <dbl>,\n#> #   med_home_val <dbl>, area_acres <dbl>, town_id <dbl>, town <chr>,\n#> #   fips_stco <dbl>, county <chr>, area_acr_1 <dbl>, m_atown <chr>"},{"path":"general-data-manipulation.html","id":"selecting-rows","chapter":"10 General data manipulation","heading":"10.3 Selecting Rows","text":"Though somewhat infrequent event, handy know select rows. two ways can select rows. first specifying exactly rows position. way filter data based condition—.e. median household income within range. functions slice() filter() respectively. remainder chapter introduce slice(). learn filter next chapter.Like select() can also select rows. rows names, must select rows based position. Given familiarity selecting column position cake walk .Similar last_col() function n(). n() rather handy little function tells us many observations tibble. allows specify last row tibble.Unlike last_col(), n() provides us number. Instead specifying offset can instead subtract directly output n(). grab last three rows can write (n() - 3):n(). put n()-3 inside parantheses R knows process n() - 3 first.Try :select first row, rows 100-105, last row","code":"\nslice(acs_raw, n())\n#> # A tibble: 1 x 59\n#>   ct_id_10 name  total_pop pop_den sex_ratio age_u18 age1834 age3564 age_o65\n#>      <dbl> <chr>     <dbl>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1  2.50e10 Cens…      5821   2760.     0.885   0.181   0.204   0.435   0.180\n#> # … with 50 more variables: for_born <dbl>, white <dbl>, black <dbl>,\n#> #   asian <dbl>, hispanic <dbl>, two_or_more <dbl>, eth_het <dbl>,\n#> #   med_house_income <dbl>, pub_assist <dbl>, gini <dbl>, fam_pov_per <dbl>,\n#> #   unemp_rate <dbl>, total_house_h <dbl>, fam_house_per <dbl>,\n#> #   fem_head_per <dbl>, same_sex_coup_per <dbl>, grand_head_per <dbl>,\n#> #   less_than_hs <dbl>, hs_grad <dbl>, some_coll <dbl>, bach <dbl>,\n#> #   master <dbl>, prof <dbl>, doc <dbl>, commute_less10 <dbl>,\n#> #   commute1030 <dbl>, commute3060 <dbl>, commute6090 <dbl>,\n#> #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n#> #   by_walk <dbl>, total_house_units <dbl>, vacant_unit_per <dbl>,\n#> #   renters_per <dbl>, home_own_per <dbl>, med_gross_rent <dbl>,\n#> #   med_home_val <dbl>, med_yr_built_raw <dbl>, med_yr_built <chr>,\n#> #   med_yr_moved_inraw <dbl>, med_yr_rent_moved_in <dbl>, area_acres <dbl>,\n#> #   town_id <dbl>, town <chr>, fips_stco <dbl>, county <chr>, area_acr_1 <dbl>,\n#> #   m_atown <chr>\nslice(acs_raw, (n() - 3):n())\n#> # A tibble: 4 x 59\n#>   ct_id_10 name  total_pop pop_den sex_ratio age_u18 age1834 age3564 age_o65\n#>      <dbl> <chr>     <dbl>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1  2.50e10 Cens…      2519   3083.     0.806   0.202   0.268   0.397   0.132\n#> 2  2.50e10 Cens…      3500   5392.     1.05    0.205   0.277   0.395   0.122\n#> 3  2.50e10 Cens…      5816   2677.     1.20    0.191   0.233   0.458   0.118\n#> 4  2.50e10 Cens…      5821   2760.     0.885   0.181   0.204   0.435   0.180\n#> # … with 50 more variables: for_born <dbl>, white <dbl>, black <dbl>,\n#> #   asian <dbl>, hispanic <dbl>, two_or_more <dbl>, eth_het <dbl>,\n#> #   med_house_income <dbl>, pub_assist <dbl>, gini <dbl>, fam_pov_per <dbl>,\n#> #   unemp_rate <dbl>, total_house_h <dbl>, fam_house_per <dbl>,\n#> #   fem_head_per <dbl>, same_sex_coup_per <dbl>, grand_head_per <dbl>,\n#> #   less_than_hs <dbl>, hs_grad <dbl>, some_coll <dbl>, bach <dbl>,\n#> #   master <dbl>, prof <dbl>, doc <dbl>, commute_less10 <dbl>,\n#> #   commute1030 <dbl>, commute3060 <dbl>, commute6090 <dbl>,\n#> #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n#> #   by_walk <dbl>, total_house_units <dbl>, vacant_unit_per <dbl>,\n#> #   renters_per <dbl>, home_own_per <dbl>, med_gross_rent <dbl>,\n#> #   med_home_val <dbl>, med_yr_built_raw <dbl>, med_yr_built <chr>,\n#> #   med_yr_moved_inraw <dbl>, med_yr_rent_moved_in <dbl>, area_acres <dbl>,\n#> #   town_id <dbl>, town <chr>, fips_stco <dbl>, county <chr>, area_acr_1 <dbl>,\n#> #   m_atown <chr>\nslice(acs_raw, 1, 100:105, n())\n#> # A tibble: 8 x 59\n#>   ct_id_10 name  total_pop pop_den sex_ratio age_u18 age1834 age3564 age_o65\n#>      <dbl> <chr>     <dbl>   <dbl>     <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n#> 1  2.50e10 Cens…      4585    333.     1.13    0.234  0.202    0.398  0.166 \n#> 2  2.50e10 Cens…      5223   2402.     1.21    0.183  0.171    0.450  0.197 \n#> 3  2.50e10 Cens…      5586    592.     1.09    0.278  0.116    0.413  0.193 \n#> 4  2.50e10 Cens…      4474   1119.     0.962   0.282  0.0847   0.427  0.206 \n#> 5  2.50e10 Cens…      6713    674.     0.928   0.223  0.216    0.423  0.139 \n#> 6  2.50e10 Cens…      6676   3541.     0.999   0.249  0.266    0.395  0.0902\n#> 7  2.50e10 Cens…      8141    820.     1.25    0.258  0.169    0.410  0.164 \n#> 8  2.50e10 Cens…      5821   2760.     0.885   0.181  0.204    0.435  0.180 \n#> # … with 50 more variables: for_born <dbl>, white <dbl>, black <dbl>,\n#> #   asian <dbl>, hispanic <dbl>, two_or_more <dbl>, eth_het <dbl>,\n#> #   med_house_income <dbl>, pub_assist <dbl>, gini <dbl>, fam_pov_per <dbl>,\n#> #   unemp_rate <dbl>, total_house_h <dbl>, fam_house_per <dbl>,\n#> #   fem_head_per <dbl>, same_sex_coup_per <dbl>, grand_head_per <dbl>,\n#> #   less_than_hs <dbl>, hs_grad <dbl>, some_coll <dbl>, bach <dbl>,\n#> #   master <dbl>, prof <dbl>, doc <dbl>, commute_less10 <dbl>,\n#> #   commute1030 <dbl>, commute3060 <dbl>, commute6090 <dbl>,\n#> #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n#> #   by_walk <dbl>, total_house_units <dbl>, vacant_unit_per <dbl>,\n#> #   renters_per <dbl>, home_own_per <dbl>, med_gross_rent <dbl>,\n#> #   med_home_val <dbl>, med_yr_built_raw <dbl>, med_yr_built <chr>,\n#> #   med_yr_moved_inraw <dbl>, med_yr_rent_moved_in <dbl>, area_acres <dbl>,\n#> #   town_id <dbl>, town <chr>, fips_stco <dbl>, county <chr>, area_acr_1 <dbl>,\n#> #   m_atown <chr>"},{"path":"general-data-manipulation.html","id":"revisiting-commmuting","chapter":"10 General data manipulation","heading":"10.4 Revisiting commmuting","text":"’ve just spent fair amount time learning work data. ’s now time return problem hand. still haven’t addressed data use partner non-profit. urban informatics largely technical, still mostly intellectual. think problems methodical data selection curation. think data tells us important.exercises, hope looking data thinking may helpful non-profit. , goal provide useful, need.","code":""},{"path":"general-data-manipulation.html","id":"exercise-1","chapter":"10 General data manipulation","heading":"10.4.1 Exercise","text":"now incumbent upon curate data BARI Census Indicator dataset non-profit. Refamiliarize data. Select subset columns believe provide best insight commuting behavior Greater Boston Area also providing demographic insight area.making decisions like , like think quote Master Disguise:“Answer questions : ? ? ? ?”Save resultant tibble object named commute something else informative.one approach question. , selected columns pertaining commute time (columns start commute), method people commute (begin ), medisan household income, name census tract. name census tract helpful identifying “.”[^indicators]","code":"\ncommute <- select(acs_raw,\n                  county,\n                  hs_grad, bach, master,\n                  starts_with(\"commute\"),\n                  starts_with(\"by\"),\n                  med_house_income)"},{"path":"thats-too-much-data.html","id":"thats-too-much-data","chapter":"11 That’s too much data","heading":"11 That’s too much data","text":"Let us continue scenario developed last chapter. non-profit seeking graduate student assistance provide curated dataset provides insight commuting behavior Greater Boston Area. Using BARI’s Massachusetts’ Census Indicators dataset, able reduce 52 initial columns 11. However data entire state just Greater Boston Area. leaves us two tasks: 1) define Greater Boston Area 2) create subset data fit criteria defined 1.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\ncommute## # A tibble: 648 x 14\n##    county hs_grad  bach master commute_less10 commute1030 commute3060\n##    <chr>    <dbl> <dbl>  <dbl>          <dbl>       <dbl>       <dbl>\n##  1 MIDDL…   0.389 0.188 0.100          0.0916       0.357       0.375\n##  2 MIDDL…   0.167 0.400 0.130          0.0948       0.445       0.344\n##  3 MIDDL…   0.184 0.317 0.139          0.0720       0.404       0.382\n##  4 MIDDL…   0.258 0.322 0.144          0.0983       0.390       0.379\n##  5 MIDDL…   0.301 0.177 0.0742         0.0670       0.379       0.365\n##  6 MIDDL…   0.159 0.310 0.207          0.0573       0.453       0.352\n##  7 MIDDL…   0.268 0.247 0.149          0.0791       0.475       0.368\n##  8 MIDDL…   0.261 0.300 0.126          0.137        0.450       0.337\n##  9 MIDDL…   0.315 0.198 0.140          0.0752       0.478       0.329\n## 10 MIDDL…   0.151 0.348 0.151          0.0830       0.474       0.322\n## # … with 638 more rows, and 7 more variables: commute6090 <dbl>,\n## #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n## #   by_walk <dbl>, med_house_income <dbl>"},{"path":"thats-too-much-data.html","id":"filtering","chapter":"11 That’s too much data","heading":"11.1 filter()ing","text":"Previously looked ways selecting columns. , focus creating subsets data. rely function dplyr::filter() . filter() differs slice() filter check see data fit specified criteria whereas slice concerned position row.Explore help documentation filter() running command ?filter() console.first argument filter() , course, tibble like subset. Secondly, see .... case filter(), provide called logical expressions .... logical expression one returns TRUE FALSE values typically comparison sort. filter() returns observations logical expression, condition, true.Almost every time shopping browsing online, whether Amazon Etsy, filtering search results using logic. Whether checking Prime tick box, specifying price range Etsy, restaurant rating Yelp. conditions providing search.can create types filters data , need understand craft logical expressions .example, using commute dataset, can check see Census Tracts 75% commuters traveling automobile.Filter commute values by_auto greater 0.75.\nAssign result object auto_commuters.\nAssign result object auto_commuters.Select print by_auto column auto_commuters.line checks every single value by_auto asks “value 0.75?” filter include row output. Another way say by_auto 0.75 condition met.R, programming languages, number logical operators used check conditions.","code":"\nauto_commuters <- filter(commute, by_auto > 0.75)\n\nselect(auto_commuters, by_auto)## # A tibble: 291 x 1\n##    by_auto\n##      <dbl>\n##  1   0.857\n##  2   0.862\n##  3   0.888\n##  4   0.884\n##  5   0.967\n##  6   0.905\n##  7   0.935\n##  8   0.915\n##  9   0.913\n## 10   0.906\n## # … with 281 more rows"},{"path":"thats-too-much-data.html","id":"logical-operators","chapter":"11 That’s too much data","heading":"11.2 Logical operators","text":"R myriad ways create logical expressions. focus six main relational logical operators. called relational checking see one value relates another. general tend ask questions like “?” “?” “one value larger smaller another?” getting hang , encourage try verbalize logical expressions.< : less > : greater <= : less equal >= : greater equal == : exactly equal (like think “thing?”)!= : equal ( “things ”)!: Negation. returns opposite value.Let’s bring back early algebra work variable x y. Let’s say x = 3 y = 5.x less y?Yes . R returns logical value represented TRUE FALSE.x greater y?sake illustration:power logical operators isn’t necessarily ability compare one value another, ability compare many values one value even many values multiple values. filter() helps us .Using filter() can compare one column another. , values columns row compared. example, can identify Census Tracts people walk drive.Note: saving results object called walk select columns.can also use filter see walking rates driving rates . shown use == test things .important distinction: <- different = = different ==. ever confused operator use ask goal . goal assign object use <-. goal assign argument value use =. trying compare two things use ==.far checking one condition cases actually suffice. may want check multiple conditions one time. use filter can add logical expression another argument. , filter check see conditions met , , row returned. called “” statement. Meaning condition one condition two need TRUE.Building upon walking example, can narrow observations adding second condition returns observations median household incomes $40,000.","code":"\n# set variables.\nx <- 3\ny <- 5\n\nx < y## [1] TRUE\n# greater than?\nx > y## [1] FALSE\n# less than or equal\nx <= y## [1] TRUE\n# greater or equal\nx >= y## [1] FALSE\n# exactly equal?\nx == y## [1] FALSE\n# not equal\nx != y## [1] TRUE\nwalking <- filter(commute, by_walk > by_auto)\n\nselect(walking, county, by_walk, by_auto)## # A tibble: 48 x 3\n##    county    by_walk by_auto\n##    <chr>       <dbl>   <dbl>\n##  1 MIDDLESEX   0.427   0.213\n##  2 MIDDLESEX   0.545   0.142\n##  3 MIDDLESEX   0.372   0.287\n##  4 MIDDLESEX   0.299   0.240\n##  5 SUFFOLK     0.548   0.105\n##  6 SUFFOLK     0.526   0.146\n##  7 SUFFOLK     0.569   0.148\n##  8 SUFFOLK     0.514   0.251\n##  9 SUFFOLK     0.393   0.290\n## 10 SUFFOLK     0.293   0.184\n## # … with 38 more rows\nwalk_auto <- filter(commute, by_walk == by_auto)\n\nselect(walk_auto, county, by_walk, by_auto)## # A tibble: 1 x 3\n##   county  by_walk by_auto\n##   <chr>     <dbl>   <dbl>\n## 1 SUFFOLK       0       0\nlow_inc_walk <- filter(commute, \n                       by_walk > by_auto,\n                       med_house_income < 40000)\n\nselect(low_inc_walk, by_walk, by_auto, med_house_income)## # A tibble: 10 x 3\n##    by_walk by_auto med_house_income\n##      <dbl>   <dbl>            <dbl>\n##  1   0.581   0.214            21773\n##  2   0.647   0.108            36250\n##  3   0.407   0.170            34677\n##  4   0.381   0.159            30500\n##  5   0.465   0.192            28618\n##  6   0.340   0.243            16094\n##  7   0.536   0.112            19267\n##  8   0.436   0.161            22930\n##  9   0.451   0.170            36500\n## 10   0.677   0.107            31218"},{"path":"thats-too-much-data.html","id":"statements","chapter":"11 That’s too much data","heading":"11.2.1 & statements:","text":"Now want introduce two logical operators, (&), implicitly used filter statement, (|). & compares two conditions return TRUE TRUE. | return TRUE one conditions TRUE.Now illustrative example & statements :","code":"\n# We have TRUE and TRUE, this should be false because they aren't both TRUE\nTRUE & FALSE ## [1] FALSE\n# both a TRUE, we expect TRUE\nTRUE & TRUE## [1] TRUE\n# The first statement is TRUE, but the second is not TRUE, expect FALSE\n(1 == 1) & (1 < 1)## [1] FALSE\n# The first statement is TRUE and the second is TRUE, expect TRUE\n(1 == 1) & (1 <= 1)## [1] TRUE"},{"path":"thats-too-much-data.html","id":"statements-1","chapter":"11 That’s too much data","heading":"11.2.2 | statements:","text":"statements used evaluate whether something meets least one two conditions. means time statement evaluates FALSE expressions result FALSE.can alter previous filter() statement show us places walk drive low median household income.","code":"\n# True is present, so we expect TRUE\nTRUE | TRUE## [1] TRUE\n# True is present, so we expect TRUE\nTRUE | FALSE## [1] TRUE\n# \nFALSE | FALSE## [1] FALSE\nlow_inc_or_walk <- filter(commute,\n                          by_walk > by_auto | med_house_income < 40000)\n\nselect(low_inc_or_walk, by_walk, by_auto, med_house_income)## # A tibble: 98 x 3\n##    by_walk by_auto med_house_income\n##      <dbl>   <dbl>            <dbl>\n##  1 0.0729    0.824            38514\n##  2 0.427     0.213            65000\n##  3 0.545     0.142            44202\n##  4 0.372     0.287            70972\n##  5 0.299     0.240           116140\n##  6 0.00933   0.688            39183\n##  7 0.548     0.105            78616\n##  8 0.526     0.146           116538\n##  9 0.115     0.423            26125\n## 10 0.0347    0.516            30283\n## # … with 88 more rows"},{"path":"thats-too-much-data.html","id":"negation","chapter":"11 That’s too much data","heading":"11.2.3 Negation","text":"Many times easier create logical statement say want opposite results. case use bang operator exclamation mark, !. negate logical value logical statement put bang front statement value.example can make FALSE true negating .can take previous exampleKeep pocket later.","code":"\n!FALSE## [1] TRUE\n# The first statement is TRUE and the second is TRUE, expect TRUE\n(1 == 1) & (1 <= 1)## [1] TRUE\n# negate it\n!(1 == 1) & (1 <= 1)## [1] FALSE"},{"path":"thats-too-much-data.html","id":"defining-the-greater-boston-area","chapter":"11 That’s too much data","heading":"11.3 Defining the Greater Boston Area","text":"now developed requisite skills subset commuting data just Greater Boston Area. still haven’t completely decided constitutes . take naïve approach say Suffolk, Norfolk, Middlesex counties Greater Boston Area. can now filter data just counties!code actually rather redundant written county == three different times. using equality comparison can actually use sepcial %% operator. lets us look value vector values (’ll learn vectors shortly).example:looks see value left hand side three values vector—thing looks like c(val1, val2, ...). Using can rewrite gba_commute :","code":"\ngba_commute <- filter(commute, county == \"SUFFOLK\" | county == \"NORFOLK\" | county == \"MIDDLESEX\")\n1 %in% c(1, 2, 3)## [1] TRUE\ngba_commute <- filter(commute, county %in% c(\"SUFFOLK\", \"NORFOLK\", \"MIDDLESEX\"))"},{"path":"thats-too-much-data.html","id":"writing-data","chapter":"11 That’s too much data","heading":"11.3.1 Writing Data","text":"created proper subset data needed. However, one hurdle jump—sending data. need get tibble R data format can used—probably csv. readr provides funcitonality well.used read_csv() earlier, write csv use write_csv(). functionality beautifully simple. first argument tibble ’re going write, gba_commute case. second path write data.general recommend project two folders. One titled data-raw keep scripts raw data used process data. suggest data folder well. keep tidy, finalized, data files.Now csv file can shared!","code":"\nwrite_csv(gba_commute, \"data/gba_commute.csv\")"},{"path":"the-pipe.html","id":"the-pipe","chapter":"12 The pipe %>%","heading":"12 The pipe %>%","text":"now using one function time. can feel like rather limiting times. approach taking perform action, save resultant object, perform another action. leads either overwriting existing object multiple times assignment <- creating multiple objects. former solution great story reproducibility. point within script may refr many different objects name. second solution can clutter working environment lead excess usage memory.Well , “instead?” may asking. answer “use forward pipe operator, course.” forward pipe operator looks like %>%. special function allows user “pipe object forward function . . . expression” (Milton Wickham, 2019). true power tidyverse comes .: pipe operator takes object output expression ’s left hand side lhs provides first argument function right hand side. Additionally, exposes lhs temporary variable .. documented lhs %>% rhs.creator #TidyTuesday RStudio employee, Thomas Mock created illustrative example pipe can simplify complex R function calls55.first example illustrates creation intermediate variables.second demonstrates nesting function calls inside function calls.Nested function calls often difficult debug user may get caught mintuae properly places parentheses.Note: Debugging process taking misbehaving code fixing .Using pipe chain functions can rewritten order happens.using pipe able align thinking code writing. Additionally, function call separated line makes debugging less daunting task.","code":"did_something <- do_something(data)\n\ndid_another_thing <- do_another_thing(did_something)\n\nfinal_thing <- do_last_thing(did_another_thing)final_thing <- do_last_thing(\n  do_another_thing(\n    do_something(\n      data\n    )\n  )\n)final_thing <- data %>% \n  do_something() %>% \n  do_another_thing() %>% \n  do_last_thing()"},{"path":"the-pipe.html","id":"applying-the-pipe","chapter":"12 The pipe %>%","heading":"12.1 Applying the pipe","text":"Remember pointed first argument almost every function data? comes handy. allows us use pipe chain together functions “makes intuitive read write” (magrittr vignette).tidyverse designed mind. select(), filter(), mutate() among many others data first functions. Moreover, output function always data frame allows user provide output input next function.always, helpful way wrap head around see action. Let’s take one lines code used adapt use pipe. select name column data . Previously may writtenBut now able write complex function chains asIn chapter filtering data began reading data, selecting columns, filtered data. recreate low_inc_or_walk object identified Census Tracts higher rate commuters walk drive median household income $40,000.reason pipe works output function call yet another tibble pipe operator passing resultant tibble first argument next function.pipe aid manipulation data, also lot utility crafting ggplots. piping tibble ggplot call, allows quickly iterate input data either filtering data creating new variables visualization purposes.following sections use pipe operator favor listed alternatives.","code":"my_tbl <- select(data_frame, ...)\nsmaller_tbl <- filter(my_tbl, ...)\nnew_col_tbl <- mutate(smaller_tbl, ...)data %>% \n  filter() %>% \n  mutate() %>% \n  select()\nlibrary(tidyverse)\nlibrary(uitk)\n\nlow_inc_or_walk <- acs_raw %>% \n  select(\n    county,\n    starts_with(\"commute\"),\n    starts_with(\"by\"),\n    med_house_income\n  ) %>% \n  filter(\n    by_walk > by_auto,\n    med_house_income < 40000\n  )\n\nglimpse(low_inc_or_walk)\n#> Rows: 11\n#> Columns: 11\n#> $ county           <chr> \"HAMPSHIRE\", \"SUFFOLK\", \"SUFFOLK\", \"SUFFOLK\", \"SUFFO…\n#> $ commute_less10   <dbl> 0.40234261, 0.34490741, 0.41918528, 0.09421755, 0.06…\n#> $ commute1030      <dbl> 0.5077599, 0.3726852, 0.4042926, 0.5200162, 0.552884…\n#> $ commute3060      <dbl> 0.07027818, 0.22777778, 0.14673675, 0.35624747, 0.27…\n#> $ commute6090      <dbl> 0.01288433, 0.04953704, 0.02058695, 0.02951880, 0.07…\n#> $ commute_over90   <dbl> 0.006734993, 0.005092593, 0.009198423, 0.000000000, …\n#> $ by_auto          <dbl> 0.1889132, 0.2140026, 0.1082621, 0.1704500, 0.159292…\n#> $ by_pub_trans     <dbl> 0.04570873, 0.11933069, 0.15384615, 0.29191557, 0.30…\n#> $ by_bike          <dbl> 0.014344761, 0.019815059, 0.008547009, 0.071286340, …\n#> $ by_walk          <dbl> 0.5801118, 0.5808014, 0.6471306, 0.4066109, 0.380531…\n#> $ med_house_income <dbl> 2499, 21773, 36250, 34677, 30500, 28618, 16094, 1926…"},{"path":"creating-new-measures.html","id":"creating-new-measures","chapter":"13 Creating new measures","heading":"13 Creating new measures","text":"’s week now non-profit finally emailed back. ecstatic provided begat even questions . indicated median household income data intruiging, difficult report . , like report income quintiles well. Moreover, also like see rate Bachelor’s Master’s degrees combined one general educational attainment variable.poses challenges . know asked, just necessarily achieve R. accomplish ’re going learn use dplyr::mutate() function. sake example, let’s select columns ’re going need make tibble called df just work .mutate() function let’s us create modify variables. arguments mutate() select()—.data .... case mutate() dots works little bit differently. indicating data, create columns specifying name-value pair. simply names arguments name columns creating. value expression. example use mutate(df, one = 1) create column called one value 1. using mutate, however, result expression needs either one value, many values rows.take df, can add columns bach master together create new column called edu_attain.even think ways can check observations specified income threshold.function immensly useful can combined almost expression create new data us. Furthermore number handy functions built dplyr help us create new columns. case_when(), min_rank(), ntile() among others. can always explore ?function_name(). purposes, look use ntile().ntile() function calculate percentiles us. Given column data, x, number buckets, n, can create new column ranks. case, interested calculating quintile med_house_income. means can provide med_house_income n = 5 arguments ntile() group observations quintile.Now can put everything together one mutate call create new variables requested!made changes can now write data csv share . process becomes iterative, ’s good put structure data idea history. One general practice good get dating files. case label file yyyy-mm-dd-commute.csv.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\ndf <- select(commute, med_house_income, bach, master)\n\ndf\n#> # A tibble: 648 x 3\n#>    med_house_income  bach master\n#>               <dbl> <dbl>  <dbl>\n#>  1            75085 0.188 0.100 \n#>  2           132727 0.400 0.130 \n#>  3           110694 0.317 0.139 \n#>  4           109125 0.322 0.144 \n#>  5            76746 0.177 0.0742\n#>  6           138700 0.310 0.207 \n#>  7           104673 0.247 0.149 \n#>  8            73191 0.300 0.126 \n#>  9           121488 0.198 0.140 \n#> 10            99358 0.348 0.151 \n#> # … with 638 more rows\nmutate(df, edu_attain = bach + master)\n#> # A tibble: 648 x 4\n#>    med_house_income  bach master edu_attain\n#>               <dbl> <dbl>  <dbl>      <dbl>\n#>  1            75085 0.188 0.100       0.288\n#>  2           132727 0.400 0.130       0.531\n#>  3           110694 0.317 0.139       0.456\n#>  4           109125 0.322 0.144       0.466\n#>  5            76746 0.177 0.0742      0.251\n#>  6           138700 0.310 0.207       0.516\n#>  7           104673 0.247 0.149       0.396\n#>  8            73191 0.300 0.126       0.426\n#>  9           121488 0.198 0.140       0.338\n#> 10            99358 0.348 0.151       0.499\n#> # … with 638 more rows\nmutate(df, above_70k_inc = med_house_income > 80000) \n#> # A tibble: 648 x 4\n#>    med_house_income  bach master above_70k_inc\n#>               <dbl> <dbl>  <dbl> <lgl>        \n#>  1            75085 0.188 0.100  FALSE        \n#>  2           132727 0.400 0.130  TRUE         \n#>  3           110694 0.317 0.139  TRUE         \n#>  4           109125 0.322 0.144  TRUE         \n#>  5            76746 0.177 0.0742 FALSE        \n#>  6           138700 0.310 0.207  TRUE         \n#>  7           104673 0.247 0.149  TRUE         \n#>  8            73191 0.300 0.126  FALSE        \n#>  9           121488 0.198 0.140  TRUE         \n#> 10            99358 0.348 0.151  TRUE         \n#> # … with 638 more rows\nmutate(df, inc_quintile = ntile(med_house_income, 5))\n#> # A tibble: 648 x 4\n#>    med_house_income  bach master inc_quintile\n#>               <dbl> <dbl>  <dbl>        <int>\n#>  1            75085 0.188 0.100             2\n#>  2           132727 0.400 0.130             5\n#>  3           110694 0.317 0.139             4\n#>  4           109125 0.322 0.144             4\n#>  5            76746 0.177 0.0742            2\n#>  6           138700 0.310 0.207             5\n#>  7           104673 0.247 0.149             4\n#>  8            73191 0.300 0.126             2\n#>  9           121488 0.198 0.140             5\n#> 10            99358 0.348 0.151             4\n#> # … with 638 more rows\nupdated_commute <- commute %>% \n  mutate(edu_attain = bach + master, \n         inc_quintile = ntile(med_house_income, 5))"},{"path":"data-structures.html","id":"data-structures","chapter":"14 Data Structures","heading":"14 Data Structures","text":"topic skirting around time now think time rather important conversation. ’s one almost never fun quite necessary without , may many painful lessons learned future. ’re going spend next chapter talking data structures—! ’ll cover three common , end , hope much stronger idea working behaves way .cover vectors, data frames rather briefly, lists. ’ll talk defining characteristics can interact . Often theory behind object types omitted, mind learning early pay dividends. Take deep breath dive remind ain’t nothin’ thing.section undoubtedly theoretically dense software perspective entire book. concepts may little bit difficult grasp first go around particularly programming background. discouraged! tough way around , might well go . can grasp chapter programming R become much easier. develop intuition certain things happen data interact data structures.","code":""},{"path":"data-structures.html","id":"atomic-vectors","chapter":"14 Data Structures","heading":"14.1 Atomic Vectors","text":"like think atomic vector much like atom—building block R object. ’ve actually working atomic vectors entire time. haven’t explicit yet. point working mainly tibbles. secret: column tibble actually atomic vector.makes vector atomic can single data type one-dimensional—opposed tibbles two-dimensional56. may noticed every value column data type. means rather strict work good reason. Imagine wanted multiple column 10, happen values column actually written text? Let’s try exploring idea.common way create vector R use c() function. stands combine. can combine many elements want single vector using c(). element vector ’s argument (separated comma).example wanted create vector Boston’s unemployment rate rate month 2019 data (October writing Dec. 18th, 2019) write . save vector called unemp57.really great vectors can perform number operations —.e. find sum values, average, add value element, etc.wanted find average unemployment rate Boston Jan - Oct. 2019, can supply vector function mean().However, may thinking “12 months year 10 represented” , totally agree . Since data November December missing, denote update unemp accordingly. R uses NA represent missing data. represent can append two NAs vector . two ways can . can either combine unemp two NAs, rewrite vector.works, since saving unemp best practices use variable changing objects assignment.rather unclear might confuse someone read code later time—person may even . reason redefine .know 12 elements vector, sometimes quite nice sanity check oneself. can always find long (many elements ) vector supplying vector length() function.total six types vectors. Fortunately, four really matter us. integer, double, character, logical.Integers represent whole numbers. specify integer append L number 20L. Doubles number requires precision aka decimal places. can specify doubles number formats scientific notation. Generally easiest way , though, using decimal. Together integers doubles lumped category numeric. , well, numbers.learned previously, character vectors created use quotation marks; either \" '.’ve already created vector type double, unemp. can check type vector unemp typeof()Note: typeof() used internal R object lists vectors. cases want use class() return class object.Say create another vector called month numbers 1 12.Notice since didn’t specify L numbers R defaulted treating month double. possible good make distinction integer numeric.R number vectors built letters alphabet (letters LETTERS respectively), well month.abb, month.name, pi. month.name already available us let’s recreate .Notice quotes around vector element. identify character vectors.Logical vectors last kind vector need go . Logical vectors represented values TRUE FALSE. Simple enough. Onward!Recall vectors atomic meaning can one type per vector mix match. character presence another element different type, value coerced character. Coersion process implicitly contextually changing object one type another. example:Something similar happens logical value presence numeric valueIn presence numeric value TRUE becomes equal 1L FALSE equal 0L.behavior exists whenever logical value presented numeric expected function call .coersion occurs processes like combining values vector, casting process intentionally changing object’s class. number casting functions whice generaly take shape .class() as_class(). vector types covered casting functions.progress R journey find scenarios need cast objects one class another functions trick.now strong understanding underbellies R vectors. One thing missing understanding can select subsets vectors. extract value vectors append square brackets end vector vec[]. supply index value square brackets receive value positionTo select month January unemp vector, first element, provide value 1 brackets.extract one value, provide vector row indexes desire.yet another way extract values vectors. can provide logical vector square brackets. example, can identify every value unemp average rate.Notice NAs stayed NA? can pesky. Hadley writes Advanced R “missing values tend infectious: computations involving missing value return another missing value.”58How annoying NAs can ! prevent NAs showing upwe can add another condition index line remove NAs. Like .*() functions casting, also .*() functions testing. .*() returns logical vector length provided vector.Note: * called wildcard. wildcard character comes SQL present means string can follow. .*() intended indicate possible testing function .numeric(), is_tibble(), etc.learned, can negate logical vectors !. can negate test results include & condition identify unemployment values average aren’t missing.one last thing keep mind subsetting vectors using logical vector different length. use logical vector subset differing length, logical vector recycled remaining values vector subset. always, example best.Say object called x values 0 10 index subset . subset index index logical vector length two values TRUE FALSE, every observation returned. come third value x, R ran values index use goes back beginningAnd happens value single logical value?latter case see output says integer(0). informing vector contains 0 elements.","code":"\nunemp <- c(3.2, 2.8, 2.8, 2.4, 2.8, 2.9, 2.7, 2.6, 2.7, 2.3)\nunemp\n#>  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3\nmean(unemp)\n#> [1] 2.72\n# combining existing with 2 NAs\nc(unemp, NA, NA)\n#>  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA\n# for example\nunemp <- c(unemp, NA, NA)\nunemp <- c(3.2, 2.8, 2.8, 2.4, 2.8, 2.9, 2.7, 2.6, 2.7, 2.3, NA, NA)\nunemp\n#>  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA\n# how many observations are in `unemp`?\nlength(unemp)\n#> [1] 12\ntypeof(unemp)\n#> [1] \"double\"\nmonth <- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12)\n\ntypeof(month)\n#> [1] \"double\"\nmonth <- c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L)\n\ntypeof(month)\n#> [1] \"integer\"\nmonth.name\n#>  [1] \"January\"   \"February\"  \"March\"     \"April\"     \"May\"       \"June\"     \n#>  [7] \"July\"      \"August\"    \"September\" \"October\"   \"November\"  \"December\"\n\ntypeof(month.name)\n#> [1] \"character\"\nx <- c(\"a\", 1)\n\ntypeof(x)\n#> [1] \"character\"\nc(TRUE, 1, FALSE)\n#> [1] 1 1 0\nsum(TRUE, FALSE, FALSE)\n#> [1] 1\nas.integer(TRUE)\n#> [1] 1\nas.character(123)\n#> [1] \"123\"\nas.double(\"2.331\")\n#> [1] 2.331\nas.logical(0)\n#> [1] FALSE\nunemp[1]\n#> [1] 3.2\nunemp[c(1, 3)]\n#> [1] 3.2 2.8\n# find average removing missing values\navg_unemp <- mean(unemp, na.rm = TRUE)\n\n# identify which values are above average\nindex <- unemp > avg_unemp\n\nindex\n#>  [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE    NA    NA\nunemp[index]\n#> [1] 3.2 2.8 2.8 2.8 2.9  NA  NA\nis.na(unemp)\n#>  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE\nindex <- unemp > avg_unemp & !is.na(unemp)\n\nunemp[index]\n#> [1] 3.2 2.8 2.8 2.8 2.9\nx <- 0:10\nx\n#>  [1]  0  1  2  3  4  5  6  7  8  9 10\nindex <- c(TRUE, FALSE)\n\nx[index]\n#> [1]  0  2  4  6  8 10\nx[TRUE]\n#>  [1]  0  1  2  3  4  5  6  7  8  9 10\nx[FALSE]\n#> integer(0)"},{"path":"data-structures.html","id":"data-frames","chapter":"14 Data Structures","heading":"14.2 Data Frames","text":"entirety work book far tibbles. Tibbles actually special type data frame. Data frames R’s native way storing rectangular data. Rectangles two-dimensional, data frames.Data frames secretly just bunch vectors squished together. important thing vectors length. ensures observation (row) one value vector. nature data frame, column must adhere rules vectors.Let’s create tibble using unemp vector tibble() function. tibble() works somewhat similar manner mutate() arguments provide name value pairs. case tibble, argument take form col_name = vector.create tibble unemployment rate .can add month name create new column indicate month higher average unemployment rate.interact underlying vector data frame can use dollar sign $ operator. takes form tbl$col_name.example, extracting unemp_rate column looks like:Note difference select(tbl, col) tbl$col.difference $ returns underlying vector whereas select() always return another data frame. now ability filter data grab subset vector. yet visit grab single value data frame.try something likeTo grab 10th value first column. , still tibble able use directly like standalone number.can use brackets subset R object. data frames two dimensional, need specify indexes two dimensions. made hand drawn graph used cartesian plane, assume , idea. cartesian plane can identify point combination two values: x y. x refers horizontal axis y vertical axis. put cartesian plane frame reference rectangular data frame envision rows x columns y.specifying index, able select rows columns leaving x y spot empty respectively.replicate tidyverse example provide indexes 10 1 respectively.great, ’ve rewritten tidyverse code base R. , just like tidyverse code, maintain tibble data structure. use single bracket, maintains data structure object selecting . wrap brackets another set bracket, returned object class underlying object.code narrowing tibble single column single row index extracting underlying vector (second bracket). extract underlying vector using tidyverse, can use function dplyr::pull().Now brings us second-fundamental structure R: list. Yes, second-fundamental. ’ve keeping secret . Data frames actually just lists disguise. prove , remove class unemp_tbl return class unclassed object.right, data frames actually just lists disguised rectangles.","code":"\nlibrary(dplyr)\n\ntibble(\n  unemp_rate = unemp\n)\n#> # A tibble: 12 x 1\n#>    unemp_rate\n#>         <dbl>\n#>  1        3.2\n#>  2        2.8\n#>  3        2.8\n#>  4        2.4\n#>  5        2.8\n#>  6        2.9\n#>  7        2.7\n#>  8        2.6\n#>  9        2.7\n#> 10        2.3\n#> 11       NA  \n#> 12       NA\nunemp_tbl <- tibble(\n  unemp_rate = unemp, \n  month = month.name\n) %>% \n  mutate(above_avg = unemp_rate > avg_unemp)\n\nunemp_tbl\n#> # A tibble: 12 x 3\n#>    unemp_rate month     above_avg\n#>         <dbl> <chr>     <lgl>    \n#>  1        3.2 January   TRUE     \n#>  2        2.8 February  TRUE     \n#>  3        2.8 March     TRUE     \n#>  4        2.4 April     FALSE    \n#>  5        2.8 May       TRUE     \n#>  6        2.9 June      TRUE     \n#>  7        2.7 July      FALSE    \n#>  8        2.6 August    FALSE    \n#>  9        2.7 September FALSE    \n#> 10        2.3 October   FALSE    \n#> 11       NA   November  NA       \n#> 12       NA   December  NA\nunemp_tbl$unemp_rate\n#>  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA\nselect(unemp_tbl, unemp_rate)\n#> # A tibble: 12 x 1\n#>    unemp_rate\n#>         <dbl>\n#>  1        3.2\n#>  2        2.8\n#>  3        2.8\n#>  4        2.4\n#>  5        2.8\n#>  6        2.9\n#>  7        2.7\n#>  8        2.6\n#>  9        2.7\n#> 10        2.3\n#> 11       NA  \n#> 12       NA\nunemp_tbl %>% \n  select(1) %>% \n  slice(10)\n#> # A tibble: 1 x 1\n#>   unemp_rate\n#>        <dbl>\n#> 1        2.3\nunemp_tbl[,1]\n#> # A tibble: 12 x 1\n#>    unemp_rate\n#>         <dbl>\n#>  1        3.2\n#>  2        2.8\n#>  3        2.8\n#>  4        2.4\n#>  5        2.8\n#>  6        2.9\n#>  7        2.7\n#>  8        2.6\n#>  9        2.7\n#> 10        2.3\n#> 11       NA  \n#> 12       NA\nunemp_tbl[10,]\n#> # A tibble: 1 x 3\n#>   unemp_rate month   above_avg\n#>        <dbl> <chr>   <lgl>    \n#> 1        2.3 October FALSE\nunemp_tbl[10,1]\n#> # A tibble: 1 x 1\n#>   unemp_rate\n#>        <dbl>\n#> 1        2.3\nunemp_tbl[[10,1]]\n#> [1] 2.3\nunemp_tbl %>% \n  select(1) %>% \n  slice(10) %>% \n  pull()\n#> [1] 2.3\nunclass(unemp_tbl) %>% \n  class()\n#> [1] \"list\""},{"path":"data-structures.html","id":"lists","chapter":"14 Data Structures","heading":"14.3 Lists","text":"good chance interact often doesn’t mean shouldn’t know time comes.Lists generally flexible object type R. Unlike vectors data frames lists impose structure storage data.simple lists may resemble something like vector.Notice prints differently thanEach element list self-contained. think lists somewhat like shipping containers element container components element together. can include type R object list. example, can include unemp_tbl associated vectors.can view structure list get idea actually contained list.structure l shows us first element tibble (class tbl_df), elements numeric character vectors respectively.flexibility predetermined dimensions can specify brackets. Like extracting underlying vector value data frame use [[ indexing. like think [ walking storage container [[ actually opening going inside. get sense difference lets look unemp vector.using single bracket just selecting first element list returned another list.use double bracket going inside container actually plucking element list. plucked element, can use another set brackets subset item. grab tenth row first column unemp_tbl inside l can write.Now know data frames lists can actually extract underlying vectors using [[ well $. can get tenth row first column number ways.Frankly brackets can get little messy. tidyverse package purrr super handy function called pluck() handles brackets us. purrr::pluck() meant flexible indexing data structures (documentation).pluck() works first providing object ’d like index—, notice data first emphasis—providing position element like pluck object. Generally, use pluck() possible. code becomes readable adheres single style thoroughly.Congratulations! made end exceptionally dense chapter. may feel little overwhlemed expected. Nonetheless proud! asks move .","code":"\nlist(\"Jan\", \"Feb\", \"Mar\")\n#> [[1]]\n#> [1] \"Jan\"\n#> \n#> [[2]]\n#> [1] \"Feb\"\n#> \n#> [[3]]\n#> [1] \"Mar\"\nc(\"Jan\", \"Feb\", \"Mar\")\n#> [1] \"Jan\" \"Feb\" \"Mar\"\nl <- list(unemp_tbl, unemp, month.name)\nstr(l)\n#> List of 3\n#>  $ : tibble [12 × 3] (S3: tbl_df/tbl/data.frame)\n#>   ..$ unemp_rate: num [1:12] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 ...\n#>   ..$ month     : chr [1:12] \"January\" \"February\" \"March\" \"April\" ...\n#>   ..$ above_avg : logi [1:12] TRUE TRUE TRUE FALSE TRUE TRUE ...\n#>  $ : num [1:12] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 ...\n#>  $ : chr [1:12] \"January\" \"February\" \"March\" \"April\" ...\nl[2]\n#> [[1]]\n#>  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA\nclass(l[2])\n#> [1] \"list\"\nl[[2]]\n#>  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA\nclass(l[[2]])\n#> [1] \"numeric\"\nl[[1]][[10,1]]\n#> [1] 2.3\n# subsetting the data frame\nl[[1]][[10,1]]\n#> [1] 2.3\n\n# grabbing the first vector then position\nl[[1]][[1]][10]\n#> [1] 2.3\n\n# grabbing the vector by name then position\nl[[1]]$unemp_rate[10]\n#> [1] 2.3\npurrr::pluck(l, 1, 1, 10)\n#> [1] 2.3"},{"path":"data-structures.html","id":"exercises","chapter":"14 Data Structures","heading":"14.3.1 Exercises","text":"Drink waterMove around bit shake outCreate list vectors unemp, month.name, avg_unemp.Recreate unemp_tbl referencing list elements","code":"\nlibrary(purrr)\n\nunemp_l <- list(unemp, month.name, avg_unemp)\n\ntibble(\n  unemp_rate = pluck(unemp_l, 1),\n  month = pluck(unemp_l, 2)\n) %>% \n  mutate(above_avg = unemp_rate > pluck(unemp_l, 3))\n#> # A tibble: 12 x 3\n#>    unemp_rate month     above_avg\n#>         <dbl> <chr>     <lgl>    \n#>  1        3.2 January   TRUE     \n#>  2        2.8 February  TRUE     \n#>  3        2.8 March     TRUE     \n#>  4        2.4 April     FALSE    \n#>  5        2.8 May       TRUE     \n#>  6        2.9 June      TRUE     \n#>  7        2.7 July      FALSE    \n#>  8        2.6 August    FALSE    \n#>  9        2.7 September FALSE    \n#> 10        2.3 October   FALSE    \n#> 11       NA   November  NA       \n#> 12       NA   December  NA"},{"path":"summary-statistics.html","id":"summary-statistics","chapter":"15 Summary statistics","heading":"15 Summary statistics","text":"last chapter focused underlying data structures interact R. importantly covered atomic vector data structure learned columns tibble vectors. using mutate() create new columns, actually creating vectors. filtered data, checked values underlying vectors see matched specified conditions. Moving forward, begin think ways summarizing data. working vectors often. able understand role vector plays data frame operations make learning process even easier.Let us start asking question “statistic?” simply statistic single number used characterize sample data. often see statistics used describe central tendancy spread—measures like mean standard deviation. However, ways can characterize sample data restricted traditional frequentists statistics. want creative ways look data. addition evaluating central tendancy spread may find looking average word counts tweets, distances Boston Common, much .begin summarize data, taking observations maybe subset observations calculating one value represent sample. important framing . Whenever want create aggregate measure data must one observation per-subset. Meaning, data frame 150 rows 3 groups within , three resultant observations—though may many variables.Let us revisit commute, specifically rate commuters travel 30 60 minutes.course interest identify average rate 30-60 minute commuting, well standard deviation, median. look like done? Prior measuring central tendency spread, begin visualizing data. purpose visualizing data hand give intuition may behave shape.histogram can intuit number things. distribution looks fairly normally distributed meaning mean median likely close eachother equally sound measures center, likely somewhere around 0.3. Secondly, due distribution’s rather round shape (“fat tails”), can expected rather large standard deviation. intuition developed, one calculate relevant statistics quantify characteristics.Let’s calculate mean, median, standard deviation, range single variable. calculating statistics like mean standard deviation, calculating univariate statistics , working one column (variable) time—tends case almost always.first extract commute3060 column vector using dplyr::pull().Exercise: Read help documentation functions mean(), median(), sd() get sense functions work. Calculate mean, median, standard deviation commute_rate.results functions bring good bad news. good news output single value. bad news output NA. reiterate previous point, NA infect analyses. way get around perform calculations without .Note functions used argument called na.rm. na.rm tells R remove NAs prior calculation. can recalculate statistics na.rm argument set TRUE.Let us look one last example: identifying range. range() function returns minimum maximum values.Note: wrapping assignment parentheses, resultant object printed console.range() returned two values. can verified length() function.length two vector adhere providing just value. see problem illustrated next chapter. recreate , use min() max() functions.statistics—mean, median, standard deviation, etc—way characterize larger sample data. lesson take away always need single value summarising data. Often taking column (vector) calculating single metric .following chapter learn calculate summary statistics using tidyverse.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\ncommute\n#> # A tibble: 648 x 14\n#>    county hs_grad  bach master commute_less10 commute1030 commute3060\n#>    <chr>    <dbl> <dbl>  <dbl>          <dbl>       <dbl>       <dbl>\n#>  1 MIDDL…   0.389 0.188 0.100          0.0916       0.357       0.375\n#>  2 MIDDL…   0.167 0.400 0.130          0.0948       0.445       0.344\n#>  3 MIDDL…   0.184 0.317 0.139          0.0720       0.404       0.382\n#>  4 MIDDL…   0.258 0.322 0.144          0.0983       0.390       0.379\n#>  5 MIDDL…   0.301 0.177 0.0742         0.0670       0.379       0.365\n#>  6 MIDDL…   0.159 0.310 0.207          0.0573       0.453       0.352\n#>  7 MIDDL…   0.268 0.247 0.149          0.0791       0.475       0.368\n#>  8 MIDDL…   0.261 0.300 0.126          0.137        0.450       0.337\n#>  9 MIDDL…   0.315 0.198 0.140          0.0752       0.478       0.329\n#> 10 MIDDL…   0.151 0.348 0.151          0.0830       0.474       0.322\n#> # … with 638 more rows, and 7 more variables: commute6090 <dbl>,\n#> #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n#> #   by_walk <dbl>, med_house_income <dbl>\nggplot(commute, aes(commute3060)) +\n  geom_histogram(bins = 20)\n#> Warning: Removed 6 rows containing non-finite values (stat_bin).\n commute_rate <- pull(commute, commute3060)\nmean(commute_rate)\n#> [1] NA\n\nmedian(commute_rate)\n#> [1] NA\n\nsd(commute_rate)\n#> [1] NA\nmean(commute_rate, na.rm = TRUE)\n#> [1] 0.3896481\n\nmedian(commute_rate, na.rm = TRUE)\n#> [1] 0.3921988\n\nsd(commute_rate, na.rm = TRUE)\n#> [1] 0.08624757\n(commute_range <- range(commute_rate, na.rm = TRUE))\n#> [1] 0.0000000 0.6327961\nlength(commute_range)\n#> [1] 2\nmin(commute_rate, na.rm = TRUE)\n#> [1] 0\nmax(commute_rate, na.rm = TRUE)\n#> [1] 0.6327961"},{"path":"summarizing-the-tidy-way.html","id":"summarizing-the-tidy-way","chapter":"16 Summarizing the tidy way","heading":"16 Summarizing the tidy way","text":"Now basic understanding manipulate dataset, summarising dataset useful metrics important. massive datasets many subgroups, summary statistics important distilling information something consumable. Aggregation also important visualization purposes.already reviewed constitutes summary statistic create working vectors. done within context tidyverse. figured select, filter, mutate within chain functions. followed natural next step, group_by() summarise() functions.dplyr includes wonderful helper function called count(). just says . counts number observations tibble. Let’s explore !can also count groups data set. example, can count many observations per county.count() actually wrapper around function summarise() much flexible function. summarise() aggregate analog mutate(). difference mutate() summarise() result expression mutate() must number values rows—unless course specifying scalar value like TRUE—whereas summarise() requires result one element length one.Notes:\n- wrapper function executes another function.\n- scalar vector length one.can recreate first count() call summarise() handy n() function learned ago. follow pattern assigning column names expressions .Like mutate() restriction number new columns can create. Previously calculated min, max, mean, standard deviation commute3060 variable. done rather neatly summarise().Frankly alone somewhat unimpressive. power summarise() comes incorporating group_by() function chain. group_by() allows us explicitly identify groups within tibble defined given variable. resulting tibble group_by() call seemingly unchanged.However, look comments tibble, see something new: # Groups: county [3]. tells us couple things. First groups created using county column, fifteen groups, data frame now grouped implying future mutate() summarise() calls performed specified groups. look class grouped tibble see new class introduced grouped_df.Note: tibble classes tbl tbl_df top Base R class data.frame.tibble object class, dplyr knows operations grouped. example calculate mean, mean specified groups rather mean entire dataset.\nOne function extremely useful n() function identify many observations per group inside mutate call.including commute3060 column illustrate new n column group value.group one unique value n. discussed previously, want calculate aggregate measures, value per-group. ability perform grouped calculation within mutate()can extremely powerful, create proper aggregated dataset. , can use summarise()Let’s recreate grouped count .can also include summary statistic calculations .","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\ncount(commute)\n#> # A tibble: 1 x 1\n#>       n\n#>   <int>\n#> 1   648\ncount(commute, county)\n#> # A tibble: 3 x 2\n#>   county        n\n#>   <chr>     <int>\n#> 1 MIDDLESEX   318\n#> 2 NORFOLK     130\n#> 3 SUFFOLK     200\nsummarise(commute, n = n())\n#> # A tibble: 1 x 1\n#>       n\n#>   <int>\n#> 1   648\ncommute %>% \n  summarise(\n    min_commute = min(commute3060, na.rm = TRUE),\n    max_commute = max(commute3060, na.rm = TRUE),\n    avg_commute = mean(commute3060, na.rm = TRUE),\n    sd_commute  = sd(commute3060, na.rm = TRUE)\n    )\n#> # A tibble: 1 x 4\n#>   min_commute max_commute avg_commute sd_commute\n#>         <dbl>       <dbl>       <dbl>      <dbl>\n#> 1           0       0.633       0.390     0.0862\ncommute %>% \n  group_by(county)\n#> # A tibble: 648 x 14\n#> # Groups:   county [3]\n#>    county hs_grad  bach master commute_less10 commute1030 commute3060\n#>    <chr>    <dbl> <dbl>  <dbl>          <dbl>       <dbl>       <dbl>\n#>  1 MIDDL…   0.389 0.188 0.100          0.0916       0.357       0.375\n#>  2 MIDDL…   0.167 0.400 0.130          0.0948       0.445       0.344\n#>  3 MIDDL…   0.184 0.317 0.139          0.0720       0.404       0.382\n#>  4 MIDDL…   0.258 0.322 0.144          0.0983       0.390       0.379\n#>  5 MIDDL…   0.301 0.177 0.0742         0.0670       0.379       0.365\n#>  6 MIDDL…   0.159 0.310 0.207          0.0573       0.453       0.352\n#>  7 MIDDL…   0.268 0.247 0.149          0.0791       0.475       0.368\n#>  8 MIDDL…   0.261 0.300 0.126          0.137        0.450       0.337\n#>  9 MIDDL…   0.315 0.198 0.140          0.0752       0.478       0.329\n#> 10 MIDDL…   0.151 0.348 0.151          0.0830       0.474       0.322\n#> # … with 638 more rows, and 7 more variables: commute6090 <dbl>,\n#> #   commute_over90 <dbl>, by_auto <dbl>, by_pub_trans <dbl>, by_bike <dbl>,\n#> #   by_walk <dbl>, med_house_income <dbl>\ncommute %>% \n  group_by(county) %>% \n  class()\n#> [1] \"grouped_df\" \"tbl_df\"     \"tbl\"        \"data.frame\"\ncommute %>% \n  group_by(county) %>% \n  mutate(n = n()) %>% \n  select(county, commute3060, n) \n#> # A tibble: 648 x 3\n#> # Groups:   county [3]\n#>    county    commute3060     n\n#>    <chr>           <dbl> <int>\n#>  1 MIDDLESEX       0.375   318\n#>  2 MIDDLESEX       0.344   318\n#>  3 MIDDLESEX       0.382   318\n#>  4 MIDDLESEX       0.379   318\n#>  5 MIDDLESEX       0.365   318\n#>  6 MIDDLESEX       0.352   318\n#>  7 MIDDLESEX       0.368   318\n#>  8 MIDDLESEX       0.337   318\n#>  9 MIDDLESEX       0.329   318\n#> 10 MIDDLESEX       0.322   318\n#> # … with 638 more rows\ncommute %>% \n  group_by(county) %>% \n  summarise(n = n())\n#> `summarise()` ungrouping output (override with `.groups` argument)\n#> # A tibble: 3 x 2\n#>   county        n\n#>   <chr>     <int>\n#> 1 MIDDLESEX   318\n#> 2 NORFOLK     130\n#> 3 SUFFOLK     200\ncommute %>% \n  group_by(county) %>% \n  summarise(\n    n = n(),\n    min_commute = min(commute3060, na.rm = TRUE),\n    max_commute = max(commute3060, na.rm = TRUE),\n    avg_commute = mean(commute3060, na.rm = TRUE),\n    sd_commute  = sd(commute3060, na.rm = TRUE)\n    )\n#> `summarise()` ungrouping output (override with `.groups` argument)\n#> # A tibble: 3 x 6\n#>   county        n min_commute max_commute avg_commute sd_commute\n#>   <chr>     <int>       <dbl>       <dbl>       <dbl>      <dbl>\n#> 1 MIDDLESEX   318       0.104       0.612       0.383     0.0825\n#> 2 NORFOLK     130       0.186       0.613       0.392     0.0761\n#> 3 SUFFOLK     200       0           0.633       0.400     0.0972"},{"path":"toolkit-review.html","id":"toolkit-review","chapter":"17 Toolkit review","heading":"17 Toolkit review","text":"’ve now come end first technical section Urban Informatics Toolkit. officially covered lot ground. ’ve installed R RStudio. ’ve learned basics R operations data structures. ’ve read manipulated large dataset, selected columns, created new ones, even created visualization. ’ve learned chain multiple functions together even created sets summary statistics. important useful skills serve foundation everything else R.next section book focus entirely data visualization. begin learning Grammar Graphics. Next learn apply grammar R ggplot2. Following, create ton graphics build intuition create different types graphics.ready?hydrated?Take deep breath let’s get !","code":""},{"path":"layered-i.html","id":"layered-i","chapter":"18 Grammar of layered graphics I","heading":"18 Grammar of layered graphics I","text":"’ve made quite far book. Now, want bring us back beginning. first chapter created visualizations ggplot2. want unpack ggplot2 bit also address philosophical underpinnings visualization.chapter introduces idea grammar graphics, discusses visualizations appropriate, fundamental design principles follow.","code":""},{"path":"layered-i.html","id":"the-grammar-of-layered-graphics","chapter":"18 Grammar of layered graphics I","heading":"18.1 The Grammar of Layered Graphics","text":"gg ggplot2 refers grammar graphics (2 ’s second iteration). Grammar Graphics (Wilkinson, 1999) seminal book data visualization sciences , Wilkinson defines complete system (grammar) creating visualizations go beyond standard domain “named graphics”—e.g. histogram, barchart, etc.5960ggplot2 “open source implementation grammar graphics R.”61 can internalize grammar graphics, creating plots intuitive artistic process rather mechanical one.five high level components layered grammar62. cover first two chapter.Defaults:\nData\nMapping\nDataMappingLayers:\nData*\nMapping*\nGeom\nStat\nPosition\nData*Mapping*GeomStatPositionScalesCoordinatesFacets","code":""},{"path":"layered-i.html","id":"layers-and-defaults","chapter":"18 Grammar of layered graphics I","heading":"18.2 Layers and defaults","text":"Let’s first set package environment. use commute dataset .first chapter section explored principles put name . Recall can use ggplot() returns chart nothing.specified defaults. order us plot anything , need specify (data object) visualized, features (aesthetic mappings), (geoms). begin specify x y aesthetics scales interpreted.final step add geom layer inherit data, aesthetic mappings, scale, position geom_*() layer dictates geometry.common way might define ggplot, also aware fact layer can stand without defining defaults ggplot() call. geom inherits defaults ggplot(), geom_*() also arguments data, mapping, providing increased flexibility.Note: geom_*()s data second argument either put data name argument explicitly. choice . Choose wisely!happens provide information geom_point() entirely omit ggplot()?see plot, information required layer printed console. add empty ggplot call ahead layer, able create plot.able specify different data objects within layer provie extraordinarily helpful begin work spatial data, plotting two different data frames axes, creative problem wish solve.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\nggplot()\nggplot(commute, aes(med_house_income, by_auto))\nggplot(commute, aes(bach, med_house_income))+ \n  geom_point()\ngeom_point(aes(med_house_income, by_auto), commute)\n#> mapping: x = ~med_house_income, y = ~by_auto \n#> geom_point: na.rm = FALSE\n#> stat_identity: na.rm = FALSE\n#> position_identity\nggplot() + \n  geom_point(aes(med_house_income, by_auto), commute)"},{"path":"visualizing-trends-and-relationships.html","id":"visualizing-trends-and-relationships","chapter":"19 Visualizing Trends and Relationships","heading":"19 Visualizing Trends and Relationships","text":"now language creating graphics. Next must build intuition plots build . cover basic visualizations starting univariate followed bivariate plots. discuss ways extend visualizations beyond two variables simple principles design.cases data analysis start visualization. visualization dictated characteristics data available us. intro statistics probably learned four types data : nominal ordinal, together referred categorical; interval ratio, together referred numerical ’re going contextualize R terms categorical character numerical numeric.Categorical numeric different treated differently thus lead different kinds visualizations. refer categorical character, often thinking groups label. case don’t quantifiable numeric value, often count variables.Throughout chapter use another ACS dataset variables focused towards housing. file lives {uitk} package object acs_housing.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\nacs_housing\n#> # A tibble: 1,311 x 6\n#>    county         med_yr_moved_inr… med_house_income fam_house_per age_u18  bach\n#>    <chr>                      <dbl>            <dbl>         <dbl>   <dbl> <dbl>\n#>  1 Worcester Cou…              2004           105735         0.797   0.234 0.325\n#>  2 Worcester Cou…              2003            69625         0.698   0.181 0.262\n#>  3 Worcester Cou…              2007            70679         0.659   0.171 0.267\n#>  4 Worcester Cou…              2006            74528         0.657   0.203 0.231\n#>  5 Worcester Cou…              2006            52885         0.531   0.177 0.168\n#>  6 Worcester Cou…              2000            64100         0.665   0.163 0.192\n#>  7 Worcester Cou…              2011            37093         0.632   0.191 0.101\n#>  8 Worcester Cou…              2006            87750         0.636   0.202 0.272\n#>  9 Worcester Cou…              2004            97417         0.758   0.188 0.284\n#> 10 Worcester Cou…              2004            67121         0.652   0.179 0.237\n#> # … with 1,301 more rows"},{"path":"visualizing-trends-and-relationships.html","id":"univariate-visualizations","chapter":"19 Visualizing Trends and Relationships","heading":"19.1 Univariate visualizations","text":"always strong urge begin creating epic graphs facets, shapes, colors, hell, maybe even three dimensions. must resist urge! must understand distributions data start visualizing drawing conclusions. knows, may find anomalies errors data cleaning process even collection process. always begin studying individual variable characteristics univariate visualizations.Note univariate visualizations numeric variablesThere couple things looking numeric univariate visualization. broadest sense, ’re looking characterizations central tendency, spread. create visualizations ’re trying answer following questions:middle?one middle?close together points?points far middle?distribution flat? steep?exploring questions rely three types plots:HistogramDensityBox plotEach type plot serves different purpose.","code":""},{"path":"visualizing-trends-and-relationships.html","id":"histogram","chapter":"19 Visualizing Trends and Relationships","heading":"19.1.1 Histogram","text":"already created number histograms already always good revisit subject. Histograms puts data n buckets (bins, groups, whatever stats professor called ), counts number values fall bucket, use frequency count height bar.true benefit histogram easiest chart consume layperson. downside merely changing number bins, distribution can rather distorted , researcher analyst, ensure miscommunication data.wish create histogram, use geom_histogram(bins = n) geom layer. Since univariate visualization, specify one aesthetic mapping—case x.Let’s look distribution med_yr_moved_inraw column example.Create histogram med_yr_moved_inraw 10 bins.histogram rather informative! can see shortly 2000, steep increase people moving . Right 2005 can see number tapering —presumably due housing crises begat Great Recession.Now, specify number bins, get different histogram.histogram shows gaps buckets histogram. first glance, assume may missing data phenemonon data recording process led sort missingness. isn’t case! count number observations per year manually, story becomes apparent.Note: using base R function table() produce counts. produces class table object less friendly work . Using table() rather count serves two purposes: 1) get learn another function 2) printing method friendly bookdown document.glue function provides way create strings combining R expressions plain text. appendix.tells us something really important explains histogram wonky. histogram looks way specified bins unique values! moral story creating histogram, thoughtful considerate number bins using—changes whole story.","code":"\nggplot(acs_housing, aes(med_yr_moved_inraw)) +\n  geom_histogram(bins = 10)\nggplot(acs_housing, aes(med_yr_moved_inraw)) +\n  geom_histogram()\n(moved_counts <- table(acs_housing$med_yr_moved_inraw))\n#> \n#> 1991 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 \n#>    1    2    5    7   14   31   56   77  108  121  141  109  113   84   73   67 \n#> 2010 2011 2012 2013 \n#>  125  140   29    8\n\nglue::glue(\"There are {length(moved_counts)} unique values\")\n#> There are 20 unique values"},{"path":"visualizing-trends-and-relationships.html","id":"density-function-plot","chapter":"19 Visualizing Trends and Relationships","heading":"19.1.2 Density Function plot","text":"Histograms fairly straight-forward chart provides illustrates distribution sample space. histogram provide fine grain picture underlying distribution looks like. concerned understanding underlying shape distribution use kernel density plot (aka density plot).density plot represents variable continuous space creates much better visual representation underlying distribution shape curves.Like histogram, provide single variable aesthetic mappings. geom layer density distribution geom_density().Create density plot med_yr_moved_inrawNow compare histogram density plot.feel better job illustrating shape distribution?think interpretable?","code":"\nggplot(acs_housing, aes(med_yr_moved_inraw)) +\n  geom_density()"},{"path":"visualizing-trends-and-relationships.html","id":"boxplots","chapter":"19 Visualizing Trends and Relationships","heading":"19.1.3 Boxplots","text":"boxplot third univariate visualization cover. Unlike histograms density plot, box plot’s power comes able effectively illustrate outliers general spread variable.five elements make boxplot:MinimumFirst quartile (25th percentile)Median (50th percentile)Third quartile (75th percentile)MaximumWhen creating boxplot, definition minimum maximum change little bit. defining minimums maximums without outliers. context boxplot outliers determined IQR (inner quartile range). IQR different third first quartile. take IQR add third quartile find upper bound subtract IQR first quartile find lower bound.\\(IQR = Q3 - Q1\\)\\(Minimum = Q1 - IQR\\)\\(Maximum = Q3 + IQR\\)Note naive approach defining outlier. hard fast rule considered outlier. many considerations go defining outlier arbitrary statistical heuristics. sure deep think calling anything outlier.points fall outside minimum maximum plotted individually give idea potential outliers.create boxplot use geom_boxplot() function.Create boxplot med_house_incomeFrom boxplot, can tell median household income Massachusetts?","code":"\nggplot(acs_housing, aes(med_house_income)) +\n  geom_boxplot() "},{"path":"visualizing-trends-and-relationships.html","id":"empirical-cumulative-distribution-function-ecdf","chapter":"19 Visualizing Trends and Relationships","heading":"19.1.4 Empirical Cumulative Distribution Function (ECDF)","text":"","code":"\nggplot(acs_housing, aes(med_house_income)) +\n  geom_step(stat = \"ecdf\")"},{"path":"visualizing-trends-and-relationships.html","id":"barchart","chapter":"19 Visualizing Trends and Relationships","heading":"19.1.5 Barchart","text":"last univariate chart touch bar chart. faced single categorical variable much can summarize . approach identify frequency relative frequency level (unqiue value) occurs. essentially histogram values ranges order matter—though may interested ordering values.create bar chart categorical features simply provide feature aesthetic mapping add geom_bar() layerI’m sure ’re looking chart thinking something along lines “can’t read single label, awful.” yeah, ’re totally right. general creating bar chart better flip axes. main justification flipping axes can read labels better. addition making labels legible, flipping axes, comparisons bars perceivably easier.flip axes, can map county column y axis rather x (done positionally).find situation variables mapped x y columns can add coor_flip() layer plot handle flipping us.sure keep coord_flip() back pocket! rather handy function.","code":"\nggplot(acs_housing, aes(county)) +\n  geom_bar()\nggplot(acs_housing, aes(y = county)) +\n  geom_bar()\nggplot(acs_housing, aes(county)) +\n  geom_bar() +\n  coord_flip()"},{"path":"visualizing-trends-and-relationships.html","id":"bivariate-visualizations","chapter":"19 Visualizing Trends and Relationships","heading":"19.2 Bivariate visualizations","text":"ready introduce second variable analysis. bivariate relationships (two-variables) often looking answer, general, one variable changes another. way approach relationships dependent upon type variables work . can can either looking bivariate relationship of2 numeric variables,1 numeric variable 1 categorical,2 categorical variables.","code":""},{"path":"visualizing-trends-and-relationships.html","id":"two-numeric-variables","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.1 Two Numeric Variables","text":"","code":""},{"path":"visualizing-trends-and-relationships.html","id":"scatter-plot","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.1.1 Scatter plot","text":"confronted two numeric variables, first stop scatter plot. scatter plot positions takes two continuous variables plots point (x, y) coordinate. type plot illustrates two variables change —. exceptionally useful pinpointing linearity, clusters, points may disproportionately distorting relationship, etc.Scatter plots useful asking questions “x increases y change?” natural formulation statistical questions—.e. always interested x affects y—plot variable interest vertically along y axis independent variable along horizontal x axis.Take example question “proportion individuals age eighteen increase number family households?”Using scatter plot, can begin answer question! Notice formulation question asking y change x. formulation plot fam_house_per age_u18 column.Note: plotting something. plotting x y.Recall plot scatter plot use geom_point() layer x y aesthetic mapped.scatter plot useful, one downside aware number points plotting. Since 1,400 points—often case big data—likely stack top hiding points leading dark uninterpretable mass! want able decipher concentration points well shape.many points interpretable called overplottingTo improve visualization options. can make point transparent stack top eachother become darker. , can make points small cluster become bigger darker mass.implement stylistic enhancements need set aesthetic arguments inside geom layer. order change transparency layer change alpha argument. alpha takes value 0 1 0 entirely transparent 1 completely opaque. Try values see floats boat!Alternatively, can change size (even combination ) points. , change size argument inside geom_point(). finite range values can specify experimentation encouraged!Remember deciding alpha size parameters implementing stylistic changes correct solution. marginally better solutions.","code":"\nggplot(acs_housing, aes(fam_house_per, age_u18)) +\n  geom_point()\nggplot(acs_housing, aes(fam_house_per, age_u18)) +\n  geom_point(alpha = 1/4)\nggplot(acs_housing, aes(fam_house_per, age_u18)) +\n  geom_point(size = 1/3)"},{"path":"visualizing-trends-and-relationships.html","id":"hexagonal-heatmap","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.1.2 Hexagonal heatmap","text":"Scatter plots scale well hundreds thousand points. scatter plot becomes gross mass points, need find better way display data. One solution create heatmap points. can think heatmap two-dimension equivalent histogram.heatmap “divides plane rectangles [equal size], counts number cases rectangle,” count used color rectangle63. alternative rectangular heatmap hexagonal heatmap. hexagonal heatmap minor visual benefits rectangular heatmap. choosing one better suited task !geoms create heatmaps aregeom_bin2d() creating rectangular heatmap andgeom_bin2d() creating rectangular heatmap andgeom_hex() hexagonal heatmap.geom_hex() hexagonal heatmap.Convert scatter plot heat map using geoms.Convert scatter plot heat map using geoms.Just like histogram can determine number bins used aggragating data. adjusting bins argument geom_hex() geom_bin2d() can alter size hexagon rectangle. , decision many bins include trade-interpretability accurate representation underlying data.Set number bins 20","code":"\nggplot(acs_housing, aes(fam_house_per, age_u18)) +\n  geom_bin2d()\nggplot(acs_housing, aes(fam_house_per, age_u18)) +\n  geom_hex()\nggplot(acs_housing, aes(fam_house_per, age_u18)) +\n  geom_hex(bins = 20)"},{"path":"visualizing-trends-and-relationships.html","id":"one-numeric-and-one-categorical","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.2 One numeric and one categorical","text":"next type bivariate relationship encounter numeric variable categorical variable. general two lines inquiry might take. first similar approach single numeric variable interested measures centrality spread interested characteristics change category (group membership). second seeks compare groups based aggregate measure numeric variable.example, imagine interested evaluating educational attainment rate county Greater Boston Area. can take approach ranking educational attainment rate median average. , also try evaluate counties differ amount variation.explore different techniques addressing lines inqury.","code":"\ngba_acs <- acs_housing %>% \n  filter(toupper(county) %in% c(\"SUFFOLK COUNTY\", \"NORFOLK COUNTY\", \"MIDDLESEX COUNTY\"))"},{"path":"visualizing-trends-and-relationships.html","id":"ridgelines","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.2.1 Ridgelines","text":"Ridgelines wonderful method visualizing distribution numeric variable level categorical variable. Ridgeline plot density curve level categorical variable stacks vertically. , rather comfortable way assess shape level’s distribution.plot ridgeline, need install package ggridges use function ggridges::geom_density_ridges().Reminders: install packages install.packages(\"pkg-name\"). expression ggridges::geom_density_ridges() used referencing exported function namespace (package name). syntax pkgname::function().ridgeline plot clearly illustrates differences distributions within county variable. plot, can tell Suffolk County rather extreme variation Bachelor’s degree attainment rate. compared Norfolk Middlesex counties, becomes apparent median Suffolk County attainment rate falls almost 20% lower.plot may lead one investigate . Suffolk County large contains every single neighborhood Boston Back Bay, Mission Hill, Roxbury. can drill Suffolk County identifying income percentiles plotting well.Ridgelines perfect tool exploring changes variation among different groups. run ANOVA visualize variation variables ridgeline plot first!","code":"\nggplot(gba_acs, aes(bach, county)) +\n  ggridges::geom_density_ridges() "},{"path":"visualizing-trends-and-relationships.html","id":"boxplot","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.2.2 Boxplot","text":"’s time come back boxplot. boxplot indeed wonderful single variable. much way multiple density plots makes ridgeline fantastic, multiple box plots!, using boxplot concerned shape distribution rather data . boxplot extremely useful identifying skewness potential outliers.can look distribution educational attainment using boxplot just like . difference use geom_boxplot().","code":"\nggplot(gba_acs, aes(bach, county)) +\n  geom_boxplot()"},{"path":"visualizing-trends-and-relationships.html","id":"barchart-1","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.2.3 Barchart","text":"’ve already used barchart plot counts categorical variables. also useful visualizing summary values categorical variable.example, plot number observations per county can use knowledge summarise() recreate values.Now ’ve counted number points per value, can plot using either geom_bar() setting stat = \"identity\" can use geom_col(). prefer latter.’s thing, barcharts, horizontal barcharts particular phenomenal ranking. ggplot2 doesn’t order bars us. need . , use knowledge mutate() new function forcats::fct_reorder().fct_reorder() function used reordering categorical variables numeric variable. case, want reorder county n. , within mutate() function call alter county value output fct_reorder(county, n).confused fct_reorder(), remember check help documentation ?fct_reorder().modified county_counts tibble can piped ggplot().pattern follow rather frequently—particularly need rank variables. Now knowing number observations useful, really want use barchart visualizing value importance. Let’s continue example educational attainment entirety Massachusetts time.Using group_by() summarise(), calculate median Bachelor degree attainment rate call column med_bach.Reorder county avg_bachCreate ordered horizontal barchart avg_bach county.brings us naturally next type plot: lollipop plot.","code":"\ncounty_counts <- acs_housing %>% \n  group_by(county) %>% \n  summarise(n = n())\n#> `summarise()` ungrouping output (override with `.groups` argument)\n\ncounty_counts\n#> # A tibble: 14 x 2\n#>    county                n\n#>    <chr>             <int>\n#>  1 Barnstable County    49\n#>  2 Berkshire County     38\n#>  3 Bristol County      116\n#>  4 Dukes County          4\n#>  5 Essex County        151\n#>  6 Franklin County      17\n#>  7 Hampden County       89\n#>  8 Hampshire County     30\n#>  9 Middlesex County    283\n#> 10 Nantucket County      2\n#> 11 Norfolk County      115\n#> 12 Plymouth County      89\n#> 13 Suffolk County      168\n#> 14 Worcester County    160\nggplot(county_counts, aes(n, county)) +\n  geom_col()\ncounty_counts %>% \n  mutate(county = fct_reorder(county, n)) %>% \n  ggplot(aes(n, county)) +\n  geom_col()\nacs_housing %>% \n  group_by(county) %>% \n  summarise(med_bach = median(bach)) %>% \n  mutate(county = fct_reorder(county, med_bach)) %>% \n  ggplot(aes(med_bach, county)) + \n  geom_col()\n#> `summarise()` ungrouping output (override with `.groups` argument)"},{"path":"visualizing-trends-and-relationships.html","id":"lollipop-plot","chapter":"19 Visualizing Trends and Relationships","heading":"19.2.2.4 Lollipop plot","text":"lollipop plot barchart’s fun cousin. Rather big thick bar plot summary value big point draw thin line back ’s respective axis’ intercept. can either manually create lollipop using creative combination geoms, use geom incorporated another package. almost always recommend don’t recreate something . , use ggalt::geom_lollipop() function.Remember: pkgname::function(). pkgname installed, install install.packages(\"pkgname\").can copy previous barplot code replace geom produce lollipop plot! Since keeping med_bach x position need specify horizontal = TRUE geom_lollipop(). quirk geom easy one get past. recommend setting horizontal = FALSE get firmer understanding happening. nothing quite like purposefully breaking code figure happening!","code":"\nacs_housing %>% \n  group_by(county) %>% \n  summarise(med_bach = median(bach)) %>% \n  mutate(county = fct_reorder(county, med_bach)) %>% \n  ggplot(aes(med_bach, county)) + \n  ggalt::geom_lollipop(horizontal = TRUE)"},{"path":"visualizing-trends-and-relationships.html","id":"review","chapter":"19 Visualizing Trends and Relationships","heading":"19.3 Review:","text":"’ve now built repetoire different types visualizations can use analyses. ’ve built intuition types visualization suitable given types variables disposal.next chapter explore ways improving upon plots already know build. explore Layered Grammar Graphics improve upon charts using scales, facets, breifly touch upon coordinates.","code":""},{"path":"grammer-of-layered-graphics-ii.html","id":"grammer-of-layered-graphics-ii","chapter":"20 Grammer of layered graphics II","heading":"20 Grammer of layered graphics II","text":"’ve developed strong foundation building charts ground specifying defaults (data, aesthetic mappings), adding geom layers. order take charts next level need familiarize components Layered Grammar Graphics: scales, coordinates, facets.examples return commute dataset. also recreate two columns hh_inc_quin edu_attain.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\ncommute <- commute %>% \n  mutate(hh_inc_quin = ntile(med_house_income, 5),\n         edu_attain = bach + master)"},{"path":"grammer-of-layered-graphics-ii.html","id":"scales","chapter":"20 Grammer of layered graphics II","heading":"20.1 Scales","text":"Recall Grammar Layered Graphics supply aesthetic mappings axes filled automatically.specifying defaults ggplot() call, implicitly providing x y axes. mappings, ggplot2 able identify type variable mapped aesthetic values. inference makes possible us plot without explicitly state axes .chart, column mapped continuous variable. able manually specify scale type using various scale_*_type() layers ggplot2. layers follow general format first specifying scale followed aesthetic ’re scaling, data type. example, change med_house_income axis discrete axis can apply layer scale_x_discrete()lost axis labels! ggplot2 considers integers floating point (numbers decimals) continuous categorical variables discrete.Nonetheless, lot functions disposal alter axes liking!","code":"\n(p <- ggplot(commute, aes(med_house_income, bach)))\np <- ggplot(commute, aes(med_house_income, bach)) +\n  geom_point(size = 1/3)\np +\n  scale_x_discrete() \n#> Warning: Removed 8 rows containing missing values (geom_point)."},{"path":"grammer-of-layered-graphics-ii.html","id":"transformations","chapter":"20 Grammer of layered graphics II","heading":"20.1.1 Transformations","text":"data exploration, come across non-normal distributions data. example income almost always right skewed displays sort log-normal-ish behavior. may want actually change underlying values variable, want apply transformations purposes visualization. cases, can apply scale transformations.example, visualization income education slight right skew med_house_income. graphic doesn’t justify logarithmic transformation, may benefit square root transformation. can apply scale_x_sqrt().can apply log10 transformation well scale_*_log10().correction. slight upward arch original plotting now inverted. Nonetheless, hope point made.addition applying transformations, want control limits graph. example, say want y axis include possible values [0,1]. can tell ggplot range values want axes contain lims(). lims() takes name-value pair name aesthetic value numeric vector two elements—first value origin second extent axis64.Note: [0,1] means 0 inclusive 1 inclusive.can modify y axis limits [0,1] adding lims() layer set y aesthetic c(0, 1).graph get expand y axis limits definitely contains bit much white space. expanding grid, can see sort flattening education around $150,000 income still continues increase. Perhaps omit values, relationship may seem even stronger. Let’s experiment .Set x axis limit [0, 150000]Set y axis limits [0, 0.5]changing extent axes relationship seems much robust! visualization spur validation ACS data.Remember, ACS data come samples sometimes samples small. small sample sizes, may well get values properly representative. decide whether include exclude values!","code":"\np +\n  scale_x_sqrt() \n#> Warning: Removed 8 rows containing missing values (geom_point).\np +\n  scale_x_log10() \n#> Warning: Removed 8 rows containing missing values (geom_point).\np +\n  lims(y = c(0, 1))\n#> Warning: Removed 8 rows containing missing values (geom_point).\np +\n  lims(\n    x = c(0, 150000),\n    y = c(0, .5)\n    )"},{"path":"grammer-of-layered-graphics-ii.html","id":"labeling","chapter":"20 Grammer of layered graphics II","heading":"20.1.2 Labeling","text":"plots create, lovely , somewhat lacking labeling department. put money publication accept plots labels ones sole reason axes titles scale labels hard interpret.’ve already used extra clear, add titles axis labels plots (adjusting scale labels) use labs() layer. labs(), can label aesthetic mapped well adding title, subtitle, caption, tag.Let’s add titles labels plot.Friends, ’s looking pretty good. just two changes need make: axes labels! x y axes labels meant illustrate dollar amounts percentages respectively. change scale labels. use helper functions package scales{scales} provides handy functions taking variable altering labeling match format. case, interested printing med_house_income dollar format, .e. 2000 becomes \\$2,000, bach percentage, .e. .4 becomes %40. alter labels use scales::dollar(), scales::percent() respectively.Isn’t nice well named functions can sometimes?produce examples outlined call function :Now understanding function behaves, actually change labels? come full circle back scale_*_continuous() layer. mentioned earlier, ggplot() handle making scales us. ggplot() doesn’t know want label variables appear axes. now impetus us make changes manually. change axis labels specify axis altering using proper scale layer—.e. scale_y_ scale_x_. , layer set labels argument respective labeling function want—e.g. scales::percent scales::dollar.Note: append parentheses like normally get error. case want ignore present, R try evaluate function. Rather, interested providing function object labels argument rather provide vector output.addition able control defaults, layers, now scales well equipped create manipulate plots.","code":"\np <- ggplot(commute, aes(med_house_income, bach)) +\n  geom_point(size = 1/3) +\n    labs(\n       y = \"% of population with a Bachelor's Degree\",\n       x = \"Median Household Income\",\n       title = \"Relationship between Education and Income\",\n       subtitle = \"All Massachusetts Tracts\",\n       caption = \"Source: Boston Area Research Initiative\\nvia US Census Bureau\"\n      )\n\np\nscales::dollar(2000)\n#> [1] \"$2,000\"\nscales::percent(.4)\n#> [1] \"40%\"\np + \n  scale_x_continuous(labels = scales::dollar) + \n  scale_y_continuous(labels = scales::percent)\n#> Warning: Removed 8 rows containing missing values (geom_point)."},{"path":"grammer-of-layered-graphics-ii.html","id":"coordinates","chapter":"20 Grammer of layered graphics II","heading":"20.2 Coordinates","text":"’re likely create 98% visualizations without ever manipulating coordinates, still good knowledge !mentioned alluded , working two-dimensional space—meaning x y coordinates. working two-dimensions, Cartesian plane natural choice coordinate reference system. plots, default. Behind scenes, ggplot essentially adding coord_cartesian() layer plot.Think much way scales inferred., however, find need alter manipulate coordinate system tools available us. ’ve actually already used one, coord_flip(). Like, scales, coordinate based functions prefixed coord_(). need use coordinate layers, essentially change aspect ratio plots.encounter coordinates much talk spatial data. now, though, need know exist major underlying part plots.","code":""},{"path":"grammer-of-layered-graphics-ii.html","id":"facets","chapter":"20 Grammer of layered graphics II","heading":"20.3 Facets","text":"last portion grammar visit faceting. facet plot creating called “small multiples,” term coined prominent Edward Tufte. faceting, words, creates graph unique level categorical variable. Think like group_by() plotting.two types faceting can : wrapped grid. done facet_wrap() facet_grid() respectively. reference documentation sums differences best:“facet_grid() forms matrix panels defined row column faceting variables. useful two discrete variables, combinations variables exist data. one variable many levels, try facet_wrap().”65Let’s look facet_wrap() first. create faceting, need add facet_wrap() layer existing plot. one argument required fulfill facet argument. facet expects set variables defined vars() function. vars() function used throughout tidyverse specify columns referenced used within context function ’s used .recreate plot faceting county, add facet_wrap(vars(county)) layer plot.facet_wrap() able explicitly state many rows columns plots . defaults may nice, ’s always good explicit expectations! set nrow ncol argument . Since example defaulted ncol = 3, let’s try three rows.grid works little bit differently. Rather specifying columns facet number rows columns, can create grid (matrix) small multiples. facet_grid() use rows cols.can recreate graphs passing vars(county) either rows cols. facet_grid() shines data every pairing two categorical variables. example can create facet combination county hh_inc_quin.create facets, panel shares scales. can change setting scales argument one \"free\", \"free_x\", \"free_y\". essence, “free ” scales panel. can choose share scales x axis setting scales = \"free_y\" vice versa.Note: behavior scales = \"free\" changes behavior set context facet_wrap() vs facet_grid(). former frees scales panel. latter frees scales either column row panels.also important note gone means exhaustive. , minimum, familiarize scale_*_reverse(), scale_*_binned() spare time. dozens, hundreds, ggplot2 functions suit every whim. , already briefly seen, ggplot function universe relegated just ggplot2. many packages built custom geoms enhancements may benefit .","code":"\np <- commute %>% \n  filter(!is.na(hh_inc_quin)) %>% \n  ggplot(aes(med_house_income, edu_attain)) +\n  geom_point() \n\np +\n  facet_wrap(vars(county))\np +\n  facet_wrap(vars(county), nrow = 3)\np +\n  facet_grid(cols = vars(county),\n             rows = vars(hh_inc_quin))\np +\n  facet_grid(cols = vars(county),\n             rows = vars(hh_inc_quin),\n             scales = \"free\")"},{"path":"visualizing-beyond-2-dimensions.html","id":"visualizing-beyond-2-dimensions","chapter":"21 Visualizing beyond 2-dimensions","heading":"21 Visualizing beyond 2-dimensions","text":"duration last three chapters cultivated fundamental understanding grammar graphics discussed craft univariate visualizations explore bivariate relationships. graphics gone beyond two-dimensions. utilized two aesthetics map. Howver, many can incorporate visualizations turn enable us explore three variables .Note 3D, rather visualize three variables.improve graphics utilize color, shape, size aesthetics, well faceting. course, begs question aesthetic choose? Well, depends upon type data visualizing. aesthetic serves different purposes can used different type variable.general can use mappings:color -> continuous discreteshape -> discretesize -> continuous","code":""},{"path":"visualizing-beyond-2-dimensions.html","id":"color","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.1 Color","text":"Let us first take look use color. Color , position, easiest visual cue humans distinguish (viz book coffee table) . also rather versatile visual cue can used address continuous discrete variables. first explore use color discrete measurements. context, necessarily mean discrete integers, less groups. necessarily order scale implied data. can however indicative order—think example age groups. explore use color groups discrete data, look Boston ecometrics social disorder discussed previously (O’Brien 2015 CITE NEEDED). Ecometrics stored uitk package object ecometricsAt point learning, think appropriate introduce new package can used quickly summarize visualize data. called skimr. Within package function called skim(). package really useful quickly getting understanding dataset provides useful summary statistics variable well histogram numeric columns.simple graphic evaluate raw counts year. simple bar chart look like ., aware different measurements. described previously can seen .can partition visualization illustrate number counts per ecometric per year? can use color—measurement receive ’s color. make easier determine frequency ecometric occurs. setting fill rather color working polygon shape. color used working lines points. useful trick think color border fill body fill.mapping fill measure variable able create stacked bar chart! apparent violence frequent ecometrics, followed private conflict, social disorder, guns.One downsides stacked barchart difficult compare sizes group relative ones adjacent. example comparing guns social disorder made difficult private conflict situated . can adjust chart bar situated next eachother. setting argument position = \"dodge\" within geom_col() layer.dodged bar chart makes much easier compare heights bar. now creating somewhat cluttered graphic. situation multiple groups subgroups, often preferred utilize facetting important thing graphic easy consume. rather make four plots one messy plot.Let’s facet measure tell facet_wrap() create one row.awesome! four different plots one measurement extremely easy see ecoemtrics trended four year period. seems like steady decrease! plot, however, labeling ecometrics twice: panel label legend. Since facet labeled individually situated next ecometrics, color becomes redundant. Unless important reason visualize color faceting, likely needed. , final visualization look like .","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\necometrics\n#> # A tibble: 68 x 4\n#>    type                measure   year     n\n#>    <chr>               <chr>    <dbl> <dbl>\n#>  1 armed robbery       violence  2015   430\n#>  2 armed robbery       violence  2016   445\n#>  3 armed robbery       violence  2017   413\n#>  4 armed robbery       violence  2018   307\n#>  5 assault and battery guns      2015   122\n#>  6 assault and battery guns      2016   190\n#>  7 assault and battery guns      2017   162\n#>  8 assault and battery guns      2018   169\n#>  9 assault and battery violence  2015  5770\n#> 10 assault and battery violence  2016  6021\n#> # … with 58 more rows\nskimr::skim(ecometrics)#> ── Data Summary ────────────────────────\n#>                            Values    \n#> Name                       ecometrics\n#> Number of rows             68        \n#> Number of columns          4         \n#> _______________________              \n#> Column type frequency:               \n#>   character                2         \n#>   numeric                  2         \n#> ________________________             \n#> Group variables            None      \n#> \n#> ── Variable type: character ────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate   min   max empty n_unique whitespace\n#> 1 type                  0             1     5    34     0       15          0\n#> 2 measure               0             1     4    16     0        4          0\n#> \n#> ── Variable type: numeric ──────────────────────────────────────────────────────\n#>   skim_variable n_missing complete_rate  mean      sd    p0   p25   p50   p75\n#> 1 year                  0             1 2016.    1.13  2015 2016. 2016. 2017.\n#> 2 n                     0             1 1929. 1857.      68  786  1230  2690.\n#>    p100 hist \n#> 1  2018 ▇▇▁▇▇\n#> 2  7392 ▇▂▁▁▁\nggplot(ecometrics, aes(year, n)) +\n  geom_col()\ndistinct(ecometrics, measure)\n#> # A tibble: 4 x 1\n#>   measure         \n#>   <chr>           \n#> 1 violence        \n#> 2 guns            \n#> 3 private conflict\n#> 4 social disorder\nggplot(ecometrics, aes(year, n, fill = measure)) +\n  geom_col()\nggplot(ecometrics, aes(year, n, fill = measure)) +\n  geom_col(position = \"dodge\")\nggplot(ecometrics, aes(year, n, fill = measure)) +\n  geom_col() + \n  facet_wrap(\"measure\", nrow = 1)\nggplot(ecometrics, aes(year, n)) +\n  geom_col() + \n  facet_wrap(\"measure\", nrow = 1)"},{"path":"visualizing-beyond-2-dimensions.html","id":"continuous-color-scales","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.1.1 Continuous color scales","text":"cases variable interest continuous numeric one, using continuous color scale. scales change one color another illustrate range values—example use visualize probabilities 0 - 1. example one color palettes.Often ’ll encounter visualizations use rainbow color palette number colors. illustrate range values. recommended. looking color, best able detect changes luminescence (perceived brightness) saturation.66 , work color palettes easiest interpret. First ’ll visualize changing saturation brightness looks like.image 10 colors. Starting left color yellow (hex code #FFFF00FF). step desaturated 10 percent ending color grey (hex code #808080FF). can think saturation much color .example changing brightness can look like.expand usage brightness ends spectrum get .may find course work individuals use color palette like .strongly advise . colors make extremely difficult tell changing values. Consider moment try tell difference numeric value magenta maroon. rather difficult. Moreover, color palette accessible color deficient. approximation varying types color blindness might see.Compare earlier example dark light yellows.much accessible color deficient well providing clearer understanding range values.","code":"\nprismatic:::plot.colors(rainbow(20))"},{"path":"visualizing-beyond-2-dimensions.html","id":"example","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.1.1.1 Example","text":"explore R, return commute data set created BARI’s Census Indicators early . visualize relationship commuting automobile median household income. Moreover, color point rate Bachelor’s degree attainment. educational attainment tends increase income, expect coloring somewhat follow household income.Create visualization following steps.Use object commutePlot med_house_income (y axis) by_autoColor plot bachAdd appropriate geometryWonderful! anticipated, color points darkest bottom median household income lowest. income increases, brightness saturation color scale. return grammar graphics, can take control scales. case, color scales. One ways can change color scale add layer scale_color_gradient() manually choose colors use scale. provides lot flexibility information designer. Think can use color represent thatyou visualizing. , can use colors adhere color palette organization working .scale_color_gradient() allows us provide colors low values high values. general, choose darker color represent lower values brighter colors higher values. add layer provide color codes.can find color codes via google searching “color picker.” , can use number free color palette generators online. personally fan coolors.co.’ve changed plot transition dark red white values increase.Try modifying plot picking two colors think good job visualizing range values.","code":"\n(p <- ggplot(commute, aes(by_auto, med_house_income, color = bach)) +\n  geom_point())\np +\n  scale_color_gradient(low = \"#360002\", high = \"white\")"},{"path":"visualizing-beyond-2-dimensions.html","id":"diverging-colors","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.1.1.2 Diverging colors","text":"many time variable interest middle value illustrating center well deviation center important. visually represent use called diverging color scale. Diverging color scales characterized middle color ends spectrum originate. center represent middle value. contextualize z-scores color palette , center 0 negative scores trend towards red. Whereas trended positive become blue.One thing wary using divergent color scale necessary. easy trap fall since ’re pretty cool. Remember, use diverging color palettes existing middle value.","code":""},{"path":"visualizing-beyond-2-dimensions.html","id":"example-1","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.1.1.3 Example","text":"Take ecometrics . Say interested annual deviation sample mean—average years—ecometric. perfect use case diverging color scale. require bit computational creativity. lets work .Let’s think measures need calculate. need :Find number counts year ecometric.Find average count years ecometric.Identify deviation mean.work sequentially. First foremost need calculate total number crime reports ecometric year. dataset one observation per year per ecometric. can see running quick count.need tidy ensure row one observation—case one ecometric per year—total count. logic accomplish first group_by() measure year sum n values. :One dplyr quirks summarise(), one level grouping removed. already performed aggregate measure last level grouping now unit analysis (row). Since currently grouped measure level row represent one year already ready calculate average n value group. Rather using summarise() used summary statistics, use mutate() create column called avg average n value group. want able perform column-wise operation subtract average row’s n value.continue creating three new columns. first avg set mean n group. Since resulting value mean(n) single value, observation group gets value. Second, create new column called deviation subtracts new column created n column. lastly created new column contain name ecometric well year. done ensure visualization row can plotted.snuck new package right . glue package let’s us incorporate R expressions plain text. Whatever inside brackets { run R expression. since referenced columns measure year \"{measure}: {year}\" value name look something like social disorder: 2017.Now data frame values want plot, let’s go ahead ! use lollipop chart .Remember geom_lollipop() comes ggalt package. can use function without loading whole package referencing pkgname::function_name().Within chart map devation color aesthetic.\ngraphic doesn’t take account middle value 0. , need tell ggplot2 diverging color scale. working normal continuous color scale used scale_color_gradient(). Instead, since middle value, create diverging color gradient use scale_color_gradient2(). adds two arguments mid midpoint. former color middle value. second middle value maps —defaults 0, fine example.Now add colors—red, yellow-ish beige, blue—diverging color scale plot!Information design seemingly endless field touched small amount. R community put lot work enabling use color visualization purposes. images color palettes created help wonderful packages prismatic paletteerTo explore color check packages prismatic paletteer Emil Hvitfeld.","code":"\ncount(ecometrics, measure, year)\n#> # A tibble: 16 x 3\n#>    measure           year     n\n#>    <chr>            <dbl> <int>\n#>  1 guns              2015     5\n#>  2 guns              2016     5\n#>  3 guns              2017     5\n#>  4 guns              2018     5\n#>  5 private conflict  2015     4\n#>  6 private conflict  2016     4\n#>  7 private conflict  2017     4\n#>  8 private conflict  2018     4\n#>  9 social disorder   2015     3\n#> 10 social disorder   2016     3\n#> 11 social disorder   2017     3\n#> 12 social disorder   2018     3\n#> 13 violence          2015     5\n#> 14 violence          2016     5\n#> 15 violence          2017     5\n#> 16 violence          2018     5\necometrics %>% \n  group_by(measure, year) %>% \n  summarise(n = sum(n))\n#> `summarise()` regrouping output by 'measure' (override with `.groups` argument)\n#> # A tibble: 16 x 3\n#> # Groups:   measure [4]\n#>    measure           year     n\n#>    <chr>            <dbl> <dbl>\n#>  1 guns              2015  3146\n#>  2 guns              2016  3111\n#>  3 guns              2017  3190\n#>  4 guns              2018  2585\n#>  5 private conflict  2015  8063\n#>  6 private conflict  2016  7807\n#>  7 private conflict  2017  7410\n#>  8 private conflict  2018  6592\n#>  9 social disorder   2015  5043\n#> 10 social disorder   2016  4707\n#> 11 social disorder   2017  4876\n#> 12 social disorder   2018  4168\n#> 13 violence          2015 18621\n#> 14 violence          2016 18080\n#> 15 violence          2017 17172\n#> 16 violence          2018 16630\nannual_metrics <- ecometrics %>% \n  # group by measure and year\n  group_by(measure, year) %>% \n  # find the total n for each measure and year\n  # summarise loses the last level of grouping\n  summarise(n = sum(n)) %>% \n  mutate(\n    # calculate average `n` by ecometric\n    avg = mean(n),\n    # calculate the deviation from the mean \n    deviation = n - avg,\n    # creating a new column that combines the name of the ecometric and the year\n    name = glue::glue(\"{measure}: {year}\"),\n  ) \n(p <- ggplot(annual_metrics, aes(y = deviation, x = name, color = deviation)) + \n  ggalt::geom_lollipop(size = 2) + \n  coord_flip())\n#> Registered S3 methods overwritten by 'ggalt':\n#>   method                  from   \n#>   grid.draw.absoluteGrob  ggplot2\n#>   grobHeight.absoluteGrob ggplot2\n#>   grobWidth.absoluteGrob  ggplot2\n#>   grobX.absoluteGrob      ggplot2\n#>   grobY.absoluteGrob      ggplot2\np + \n  scale_color_gradient2(low = \"#A12106\", mid = \"#B2CEB7\", high = \"#004C76\")"},{"path":"visualizing-beyond-2-dimensions.html","id":"shape-and-size","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.2 Shape and Size","text":"spent fair amount time looking number ways color can used. Color can used visualize wide variety deserving long section. briefly look two aesthetics: shape color.","code":""},{"path":"visualizing-beyond-2-dimensions.html","id":"shape","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.2.1 Shape","text":"Shape another useful way visualize groups categories data. general use shape color option us. Variations shapes can difficult discern—particularly number groups visualized reaches beyond ~4. Moreover, shapes, depending choice intricacy can become overly distracting can detract visualization whole. heuristic, map shape color variable.illustrate use shape use data Inside Airbnb.67 dataset contains Airbnb listings Boston well. dataset can found object airbnb. go dataset depth chapter spatial analysis.visualization read dataset, filter just \"Back Bay\", plot points based place space—aka latitude longitude. also map shape room_type. show us spatial distribution Airbnbs well different kinds.Note normally say “latitude longitude,” represent y x axes respectively. sure put latitude y aesthetic position longitude.changing point’s shape associated room type can get somewhat better idea spatial distribution type.can tell visualization? looks like Airbnbs overwhelmingly entire home apartment. can infer Airbnbs _are not_being used additional revenue residents. rather Airbnb might unit housing longer available Boston residents. increasing demand housing? rise Airbnbs creating shortage housing can one factors behind rising Boston rents?visualization good, lets compare use color.prefer? feel better job getting message across? , delving world design never correct answer. sometimes may consensus. just best ask others thoughts visualizations.","code":"\n# filter to backbay\nbb <- airbnb %>% \n  filter(neighborhood == \"Back Bay\") \n\n# plot the points by shape\nggplot(bb,  aes(longitude, latitude, shape = room_type)) + \n  geom_point(alpha = .5, size = 3) "},{"path":"visualizing-beyond-2-dimensions.html","id":"size","chapter":"21 Visualizing beyond 2-dimensions","heading":"21.2.2 Size","text":"Moving onto size. Size aesthetic one hardest humans properly distinguish . , size usually used comparing observations large discrepancies . ggplot rather straight forward difference , set aesthetic size column (must numeric).visualization use data Analyze Boston’s legacy database. Analyze Boston previously released signals Big Belly trashcans year 201468. words data dictionary:Bigbelly’s contain compactors crush waste within waste bins order reduce volume required store waste deposited Bigbelly. compactors able provide average compaction ratio 5:1, meaning Bigbelly can store equivalent 150G (gallons) uncompactedwaste within standard 30G waste bin. compactor Bigbelly run detected waste within bin reached certain level. compaction cycle waste crushed occupy smaller amount space within waste bin. cycles repeated waste added Bigbelly. approximately 150G uncompacted waste added Bigbelly compactor able compress waste , condition detected thewaste bin considered full.sake example, created aggregate count signals Big Belly receptacles 2014. can found object big_belly.Similar Airbnb data, visualize latitude longitude (lat long respectively) mapping size aesthetic n.","code":"\nglimpse(big_belly)\n#> Rows: 147\n#> Columns: 4\n#> $ description <chr> \"1 North Market Street  (in front of McCormick & Schmicks…\n#> $ n           <dbl> 47, 179, 257, 108, 171, 93, 27, 173, 55, 64, 126, 190, 19…\n#> $ lat         <dbl> 42.36034, 42.35653, 42.35634, 42.35557, 42.35520, 42.3545…\n#> $ long        <dbl> -71.05582, -71.06190, -71.06203, -71.06288, -71.06321, -7…\nggplot(big_belly, aes(long, lat, size = n)) +\n  geom_point(alpha = 1/2)"},{"path":"visualizing-through-time.html","id":"visualizing-through-time","chapter":"22 Visualizing through time","heading":"22 Visualizing through time","text":"Throughout section, one type visualization missing repetoire—timeseries plot. time series data rather cumbersome work . Time series unique observation represents point time. order inherent data. natural ordering data makes problem bit trickier. now visual tools principles sorted , can apply time series data visualize well.section work 2014 Big Belly data . goal visualize reports fullness course year. chapter first learn work dates. use new skills aggregate observations view time. Next, discuss traditional methods time series visualization. finally quickly touch use animation viewing time-series.","code":""},{"path":"visualizing-through-time.html","id":"working-with-dates","chapter":"22 Visualizing through time","heading":"22.1 Working with dates","text":"tibble big_belly_raw contains data work . Assign big_belly_raw tibble big_belly preview glimpse().can see data rather large contains column type <dttm> new us. <dttm> way tibble represents column type date time.Note formal class date time data POSIXct. Working date times computer tricky process.Date time objects follow format YYYY-MM-DD HH:MM:SS Y year, M month, D day, H hours, M minutes, S seconds.begs question “can aggregate based time?” easiest way group observations interval. solicit help package lubridate. lubridate part tidyverse, doesn’t loadwith tidyverse. , load package .package contains dozens useful functions working dates R. means go .explore functions package click Packages pane search package interested . Click hyper link package name exported functions objects documented ., however quickly touch ymd(), month(), floor_date().first useful example since read_csv() already parsed column date time us, important nonetheless. ymd() convert character string date object. letters stand year, month, date. function parses dates format yup, guessed , year-month-day. example:others mdy() dmy() can use parse dates well.next time trying parse date think way formatted. year precede follow month?Immediately useful us month() function. extract month component given date.returned value 1 January represented 1. wished return full name month set argument label = TRUE.Furthermore, can tell lubridate abbreviate months setting abbr = FALSE.can use function extract months Big Belly dataset. extremely useful aggregation. one limitation, however, another year present grouping observations years month bucket. avoid , can use function floor_date(). floor_date() takes date object returns closest date rounding given unit. may best explained example. Given date March 15th, 2020, let us round near units \"week\", \"month\", \"year\".new tools lets modify big_belly tibble creating two new columns month (labels) week using month() floor_date() respectively assign result object called bb.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\nbig_belly <- big_belly_raw\n\nglimpse(big_belly)## Rows: 51,440\n## Columns: 6\n## $ description <chr> \"Atlantic & Milk\", \"1330 Boylston @ Jersey Street\", \"SE B…\n## $ timestamp   <dttm> 2014-01-01 00:41:00, 2014-01-01 01:19:00, 2014-01-01 01:…\n## $ fullness    <chr> \"YELLOW\", \"YELLOW\", \"YELLOW\", \"RED\", \"GREEN\", \"YELLOW\", \"…\n## $ collection  <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FALS…\n## $ lat         <dbl> 42.35870, 42.34457, 42.34818, 42.34818, 42.34933, 42.3481…\n## $ long        <dbl> -71.05144, -71.09783, -71.09744, -71.09744, -71.07702, -7…\nlibrary(lubridate)\nymd(\"2020 01 20\")## [1] \"2020-01-20\"\nymd(\"2020 jan. 20\")## [1] \"2020-01-20\"\nymd(\"20, january-20\")## [1] \"2020-01-20\"\nmonth(\"2020-01-01\")## [1] 1\nmonth(\"2020-01-01\", label = TRUE)## [1] Jan\n## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nmonth(\"2020-01-01\", label = TRUE, abbr = FALSE)## [1] January\n## 12 Levels: January < February < March < April < May < June < ... < December\n# create date object\nmar_15 <- ymd(\"2020-03-15\")\n\n# round to week\nfloor_date(mar_15, unit = \"week\")## [1] \"2020-03-15\"\n# round to month\nfloor_date(mar_15, unit = \"month\")## [1] \"2020-03-01\"\n# round to year\nfloor_date(mar_15, unit = \"year\")## [1] \"2020-01-01\"\nbb <- big_belly %>% \n  mutate(\n    month = month(timestamp, label = TRUE),\n    week = floor_date(timestamp, unit = \"week\")\n  )\n\nslice(bb, 1:5)## # A tibble: 5 x 8\n##   description timestamp           fullness collection   lat  long month\n##   <chr>       <dttm>              <chr>    <lgl>      <dbl> <dbl> <ord>\n## 1 Atlantic &… 2014-01-01 00:41:00 YELLOW   FALSE       42.4 -71.1 Jan  \n## 2 1330 Boyls… 2014-01-01 01:19:00 YELLOW   FALSE       42.3 -71.1 Jan  \n## 3 SE Brookli… 2014-01-01 01:32:00 YELLOW   FALSE       42.3 -71.1 Jan  \n## 4 SE Brookli… 2014-01-01 01:34:00 RED      FALSE       42.3 -71.1 Jan  \n## 5 Huntington… 2014-01-01 02:10:00 GREEN    TRUE        42.3 -71.1 Jan  \n## # … with 1 more variable: week <dttm>"},{"path":"visualizing-through-time.html","id":"standard-visual-approach","chapter":"22 Visualizing through time","heading":"22.2 Standard visual approach","text":"visualize time need consider humans cognitively visually perceive time. Time linear. follow path start end. want (almost) always plot time dimension horizontal x axis time proceeds forwards . Perhaps product restrictions many variations can visualize time. Two plots reign time-series: line bar plot.can take bb tibble create barchart total number observations per month.can add characters code chunk can compare fullness months well.continue, address elephant room. just made plot legend makes sense. GREEN mapped red, RED mapped green, YELLOW mapped blue. ’s good. can fix adding manually mapping color using one scale functions scale_fill_manual(values = c(\"green\", \"red\", \"yellow\")). Now back task hand—time series.stacked barchart runs limitations mentioned previously. given number different time groups creating dodged bar chart become cluttered putting 36 total bars . consider faceting.\nokay, , still cluttered. scenario consider use line graph. line graph de facto time series plot. lines connect points time. Achieving ggplot rather straight forward. put time dimension x axis add geom_line() layer. default geom_line() doesn’t know points connect one per x value. case need tell ggplot points connect setting group aesthetic. Since want group together lines fullness level set group well color fullness. can step plotting game alsoWe can step plotting game also adding layer points top line accentuate lines.\nthree types graphs may ever truly need visualizing time. sometimes ’s nice go top add frills. Often dynamic visualization may persuasive static one., can use animation gganimate package.","code":"\ncount(bb, month) %>% \n  ggplot(aes(month, n)) +\n  geom_col()\ncount(bb, month, fullness) %>% \n  ggplot(aes(month, n, fill = fullness)) +\n  geom_col()\ncount(bb, month, fullness) %>% \n  ggplot(aes(fullness, n, group = month)) +\n  geom_col() + \n  facet_wrap(\"month\")\ncount(bb, month, fullness) %>% \n  ggplot(aes(month, n, group = fullness, color = fullness)) +\n  geom_line()\ncount(bb, month, fullness) %>% \n  ggplot(aes(month, n, group = fullness, color = fullness)) +\n  geom_line() + \n  geom_point()"},{"path":"visualizing-through-time.html","id":"animation-as-time","chapter":"22 Visualizing through time","heading":"22.3 Animation as time","text":"2008 researchers Microsoft scholar Georgia Institute Technology published paper titled Effectiveness Animation Trend Visualization69. paper explored reaction talk 2006 captured attention many using animation visualize global trends health. TED talk Hans Rosling moving utilization data visualization70. ? paper presented interactive static visualizations discover people’s perspective animation alternatives. found “users really liked animation view: Study participants described ”fun“,”exciting“, even”emotionally touching.\" time, though, participants found confusing: “dots flew everywhere.”71We now scientific backing animation pretty good also ’re unsure—seems line rest science. take quote measure caution. can creaate animations also careful use . remainder chapter learn extension ggplot2, gganimate. gganimate extends ggplot extending grammar include elements pertaining solely animation.working gganimate need think context frames. frame cross-section data. can think frame still film. context time-series frame period time. continue Big Belly example , frame can thought month.create animations need install four packages: gganimate, gifski (brilliant name), png, transformr. can running install.packages(c(\"gganimate\", \"gifski\", \"png\", \"transformr\")).extending ggplot2, gganimate adds three main types layers ggplot. importantly transition_*()s enter_*() exit_*() layers. chapter address transition_*() layers. transitions used move time dimension. working time data use transition_states() transition_reveal(). best used bar charts line plot respectively.Since changing frames time dimension—month case—start building ggplot without included. , create animation counts fullness month actually just start visualizing total counts.\nneeded add animation plot transition layer. use bar chart can use transition_states(states) states unquoted name column wil transitioned .awesome! ’ve created animation 😮. bars change heigh change state. bummer don’t know states animation going ! animation made four temporary variables made available labeling. documentation transition_states() notes following variables available72.transitioning boolean indicating whether frame part transitioning phaseprevious_state name last state animation atclosest_state name state closest framenext_state name next state animation part ofWe can add label layer make much informative. reference variables can use glue like quote strings variables.ts scaffolding great visualization. can take existing plot add adjust layers adjust styling engaging.can also add animation previous line plot. can use transition_reveal() layer reveal changes time.Note error. telling us since x, time dimension actually treated factor, move along axis. error message informative enough let us know need change month numeric value. way address yet maintain nice labeling x axis can create another column called month_integer jsut integer value month—Jan 1 etc. can provide value transition_reveal(). Moreover transition_reveal() creates frame_along temporary variable ’d like use .animation great job illustrating changing slopes fullness measures.may seem little underwhelming, two functions can provide functionality need creating animations. Explore functions package explore can add animation make truly .help(package = \"gganimate\") bring help documentation package., can conclude section. Congratulations!","code":"\n(p <- count(bb, month, fullness) %>% \n  ggplot(aes(fullness, n)) +\n  geom_col())\nlibrary(gganimate)\n\np + transition_states(month)  \np + \n  transition_states(month)  +\n  labs(title = \"{closest_state}\")count(bb, month, fullness) %>% \n  ggplot(aes(month, n, group = fullness, color = fullness)) +\n  geom_line() + \n  geom_point() +\n  transition_reveal(month)\n\nError: along data must either be integer, numeric, POSIXct, Date, difftime, orhms\ncount(bb, month, fullness) %>% \n  mutate(month_integer = as.integer(month)) %>% \n  ggplot(aes(month, n, group = fullness, color = fullness)) +\n  geom_line() + \n  geom_point() +\n  transition_reveal(month_integer)"},{"path":"visualization-review.html","id":"visualization-review","chapter":"23 Visualization Review","heading":"23 Visualization Review","text":", end third longest section Urban Informatics Toolkit. ’ve covered lot ground rather quickly let’s recap.started going grammar graphics. grammar used define components visualization ggplot2 R implementation graphics. grammar five main components:Defaults:\nData\nMapping\nDataMappingLayers:\nData\nMapping\nGeom\nStat\nPosition\nDataMappingGeomStatPositionScalesCoordinatesFacetsWe build plots providing data either ggplot() sets defaults, data can provided directly geom layers. Remember geom_*() layers creates geometry plots. Without populate graphic. layers figure positions, scales, coordinates data. can also adjust fit preference using scale_*() coord_() layerss desire.Following , explored ways univariate bivariate relationships can explored visually. explored use number different plots can added repertoire. looked can use faceting, color, shape, size explore beyond two dimensions. finally, briefly looked use animation explore data time.visualization strategies can used either independently combination create compelling graphics tells story data.next section cover advanced disparate topics important tool kit. may good take break right now continue.hydrated?","code":""},{"path":"multiple-data-sets.html","id":"multiple-data-sets","chapter":"24 Multiple data sets","heading":"24 Multiple data sets","text":"Many times working just one data set suffice. often data working need supplemented data sets. datasets shared relation enables us join together. join way combining columns two sets data ensuring rows properly aligned.relationship joins tables together expressed data called common identifier. variable exists datasets—perhaps, different name—can used reference associate rows table together.Recall definition tidy data. definition used characterized data inside single table. actually another rule: “observational unit forms table.” , previous definition row single observation. extend definition still case create distinctions “observational units[s].”explore concept use data Airbnb listings Boston. data come Inside Airbnb. Inside Airbnb collects Airbnb’s public listing makes available “non-commercial set tools data allows explore Airbnb really used cities around world”73.Inside Airbnb data another example harnessing naturally occurring data. listings generated intent data, virtue existence, become data. able harness learn shifting neighborhood dynamic cities. can tell us something short term rentals locale people may visiting .first look two data sets: listings, hosts.Notice row table observation different phenomenon. observation listings rentable dwelling row hosts different individual. One host may multiple listings. sort relationship called one many meaning host one listings. rather evident number observations table—3,799 1,335 respectively.associate host information listings information need join two datasets together. often common identifier type id (often referred keys). Identifiers meant unique observational unit. look variables tibbles notice listings contains host_id hosts id column. Fortunately first two rows dataset share identifiers.get joining datasets, need understand different types joins available us. joins consider two tables: one left hand side right hand side referred documentation x y respectively. plethora joins possible want keep focus four types. left, right, inner, anti joins.left right join identical, difference table may consider target join. case left join, “return rows x, columns x y. Rows x match y NA [missing] values new columns. multiple matches x y, combinations matches returned.”Simply, left join never lose rows x. happens match y missing values. reverse true right join. right join ensures rows y remain end. duplicate matches x y returned.sake example, let’s reduce data first five rows columns . Hosts data stored tibble h listings l.h one observation per host meaning 5 distinct hosts dataset. general one id per observation table, generally referred primary key used identify observations table. case, id primary key h. Whereas look l, multiple observations host_id.ID used identify observations another table, referred foreign key. , host_id used connect l h table foreign key.Say want know host information associated listing need join h l. situations want keep records listings keep host information relevant. left join l left hand table h right hand table provide us exactly need.actually perform join, use function left_join() dplyr. three arguments need fulfill: x, y, . , table left right? , column(s) act common identifier.Fulfilling x y arguments straightforward, simply supply relevant tibbles. Marking common identifier little bit tricky. expects character vector name common identifier. However, cases, column names differ table. connect need create called named vector.general create vector c() element separated comma. create vector names identifier (left right) look likeThis vector length 2. need create vector length 1 element named. can name vector elements like c(\"element_name\" = \"element\"). left_join() name element identifier left hand table element name identifier right hand table. Putting together looks likeNow pieces need can perform join.missing values join host_id counter-part host table. Now, say perform right join keep tables exactly . expect look like?two key differences . first , since right hand table unmatched ids, anticipate missing values columns l—price neighborhood. Moreover, since multiple matches left hand table, returned observation per match.naturally brings us inner join. inner join return row matching observations tables.Oftentimes, time, inner join behaves much like left right join one tables observation matched. general recommend performing left right joins can better keep track missingness.last join cover anti join. anti join rather unique used find matches two tables. using anti join returned observations x matches y. previous right join, know matches two host ids h. find using join can put h x position l y position. Note since switching order tibbles need rearrange vector supplied .reverse , receive empty tibble.Anti joins often good way sanity check data looking completeness missingness. sure keep one back pocket!","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\nlistings <- uitk::airbnb_listings\n\nglimpse(listings)\n#> Rows: 3,799\n#> Columns: 7\n#> $ id               <dbl> 3781, 5506, 6695, 8789, 10730, 10813, 10986, 16384, …\n#> $ neighborhood     <chr> \"East Boston\", \"Roxbury\", \"Roxbury\", \"Downtown\", \"Do…\n#> $ room_type        <chr> \"Entire home/apt\", \"Entire home/apt\", \"Entire home/a…\n#> $ price            <dbl> 125, 145, 169, 99, 150, 179, 125, 50, 154, 115, 148,…\n#> $ minimum_nights   <dbl> 28, 3, 3, 91, 91, 91, 91, 91, 29, 1, 2, 2, 2, 6, 3, …\n#> $ availability_365 <dbl> 68, 322, 274, 247, 29, 0, 364, 365, 304, 285, 62, 24…\n#> $ host_id          <dbl> 4804, 8229, 8229, 26988, 26988, 38997, 38997, 23078,…\nhosts <- uitk::airbnb_hosts\n\nglimpse(hosts)\n#> Rows: 1,335\n#> Columns: 9\n#> $ id              <dbl> 4804, 8229, 26988, 38997, 23078, 71783, 85130, 85770,…\n#> $ name            <chr> \"Frank\", \"Terry\", \"Anne\", \"Michelle\", \"Eric\", \"Lance\"…\n#> $ since_year      <dbl> 2008, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010,…\n#> $ since_month     <chr> \"12\", \"02\", \"07\", \"09\", \"06\", \"01\", \"02\", \"02\", \"03\",…\n#> $ since_day       <chr> \"03\", \"19\", \"22\", \"16\", \"24\", \"19\", \"24\", \"26\", \"23\",…\n#> $ response_rate   <chr> \"100%\", \"100%\", \"100%\", \"92%\", \"50%\", \"98%\", \"66%\", \"…\n#> $ acceptance_rate <chr> \"50%\", \"100%\", \"84%\", \"17%\", NA, \"98%\", \"97%\", \"100%\"…\n#> $ superhost       <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n#> $ n_listings      <dbl> 5, 2, 9, 13, 3, 40, 7, 4, 1, 2, 1, 1, 5, 1, 1, 1, 1, …\nh <- slice(hosts, 1:5) %>% \n  select(id, name, n_listings)\n\nl <- slice(listings, 1:5) %>% \n  select(host_id, price, neighborhood)\n\nh\n#> # A tibble: 5 x 3\n#>      id name     n_listings\n#>   <dbl> <chr>         <dbl>\n#> 1  4804 Frank             5\n#> 2  8229 Terry             2\n#> 3 26988 Anne              9\n#> 4 38997 Michelle         13\n#> 5 23078 Eric              3\nl\n#> # A tibble: 5 x 3\n#>   host_id price neighborhood\n#>     <dbl> <dbl> <chr>       \n#> 1    4804   125 East Boston \n#> 2    8229   145 Roxbury     \n#> 3    8229   169 Roxbury     \n#> 4   26988    99 Downtown    \n#> 5   26988   150 Downtown\ncount(l, host_id)\n#> # A tibble: 3 x 2\n#>   host_id     n\n#>     <dbl> <int>\n#> 1    4804     1\n#> 2    8229     2\n#> 3   26988     2\nc(\"host_id\", \"id\")\n#> [1] \"host_id\" \"id\"\nc(\"host_id\" = \"id\")\n#> host_id \n#>    \"id\"\nleft_join(l, h, by = c(\"host_id\" = \"id\"))\n#> # A tibble: 5 x 5\n#>   host_id price neighborhood name  n_listings\n#>     <dbl> <dbl> <chr>        <chr>      <dbl>\n#> 1    4804   125 East Boston  Frank          5\n#> 2    8229   145 Roxbury      Terry          2\n#> 3    8229   169 Roxbury      Terry          2\n#> 4   26988    99 Downtown     Anne           9\n#> 5   26988   150 Downtown     Anne           9\nright_join(l, h, by = c(\"host_id\" = \"id\"))\n#> # A tibble: 7 x 5\n#>   host_id price neighborhood name     n_listings\n#>     <dbl> <dbl> <chr>        <chr>         <dbl>\n#> 1    4804   125 East Boston  Frank             5\n#> 2    8229   145 Roxbury      Terry             2\n#> 3    8229   169 Roxbury      Terry             2\n#> 4   26988    99 Downtown     Anne              9\n#> 5   26988   150 Downtown     Anne              9\n#> 6   38997    NA <NA>         Michelle         13\n#> 7   23078    NA <NA>         Eric              3\ninner_join(l, h, by = c(\"host_id\" = \"id\"))\n#> # A tibble: 5 x 5\n#>   host_id price neighborhood name  n_listings\n#>     <dbl> <dbl> <chr>        <chr>      <dbl>\n#> 1    4804   125 East Boston  Frank          5\n#> 2    8229   145 Roxbury      Terry          2\n#> 3    8229   169 Roxbury      Terry          2\n#> 4   26988    99 Downtown     Anne           9\n#> 5   26988   150 Downtown     Anne           9\nanti_join(h, l, by = c(\"id\" = \"host_id\"))\n#> # A tibble: 2 x 3\n#>      id name     n_listings\n#>   <dbl> <chr>         <dbl>\n#> 1 38997 Michelle         13\n#> 2 23078 Eric              3\nanti_join(l, h, by = c(\"host_id\" = \"id\"))\n#> # A tibble: 0 x 3\n#> # … with 3 variables: host_id <dbl>, price <dbl>, neighborhood <chr>"},{"path":"multiple-data-sets.html","id":"exercise-2","chapter":"24 Multiple data sets","heading":"24.1 Exercise","text":"exercise goal identify average price availability Airbnb rentals host. plotLoad reviews dataset {uitk}Count total number reviews listing, create column n_reviewsjoin listingsjoin host, name airbnb_full.","code":"\nreviews <- uitk::airbnb_reviews\n\nairbnb_full <- group_by(reviews, listing_id) %>% \n  summarise(n_reviews = n()) %>% \n  left_join(listings, by = c(\"listing_id\" = \"id\")) %>% \n  left_join(hosts, by = c(\"host_id\" = \"id\"))\n#> `summarise()` ungrouping output (override with `.groups` argument)\n\nglimpse(airbnb_full)\n#> Rows: 2,668\n#> Columns: 16\n#> $ listing_id       <dbl> 3781, 5506, 6695, 8789, 10730, 10813, 18711, 22195, …\n#> $ n_reviews        <int> 2, 26, 30, 2, 4, 35, 9, 11, 23, 21, 3, 20, 10, 15, 1…\n#> $ neighborhood     <chr> \"East Boston\", \"Roxbury\", \"Roxbury\", \"Downtown\", \"Do…\n#> $ room_type        <chr> \"Entire home/apt\", \"Entire home/apt\", \"Entire home/a…\n#> $ price            <dbl> 125, 145, 169, 99, 150, 179, 154, 115, 148, 275, 95,…\n#> $ minimum_nights   <dbl> 28, 3, 3, 91, 91, 91, 29, 1, 2, 2, 6, 3, 2, 2, 2, 5,…\n#> $ availability_365 <dbl> 68, 322, 274, 247, 29, 0, 304, 285, 62, 247, 197, 80…\n#> $ host_id          <dbl> 4804, 8229, 8229, 26988, 26988, 38997, 71783, 85130,…\n#> $ name             <chr> \"Frank\", \"Terry\", \"Terry\", \"Anne\", \"Anne\", \"Michelle…\n#> $ since_year       <dbl> 2008, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 2010…\n#> $ since_month      <chr> \"12\", \"02\", \"02\", \"07\", \"07\", \"09\", \"01\", \"02\", \"02\"…\n#> $ since_day        <chr> \"03\", \"19\", \"19\", \"22\", \"22\", \"16\", \"19\", \"24\", \"26\"…\n#> $ response_rate    <chr> \"100%\", \"100%\", \"100%\", \"100%\", \"100%\", \"92%\", \"98%\"…\n#> $ acceptance_rate  <chr> \"50%\", \"100%\", \"100%\", \"84%\", \"84%\", \"17%\", \"98%\", \"…\n#> $ superhost        <dbl> 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0…\n#> $ n_listings       <dbl> 5, 2, 2, 9, 9, 13, 40, 7, 4, 1, 1, 1, 5, 5, 1, 1, 2,…\nhost_summary <- left_join(listings, hosts, by = c(\"host_id\" = \"id\")) %>% \n  group_by(superhost) %>% \n  summarise(avg_avail = mean(availability_365),\n            avg_price = mean(price),\n            avg_min_nights = mean(minimum_nights))\n#> `summarise()` ungrouping output (override with `.groups` argument)"},{"path":"statistics.html","id":"statistics","chapter":"25 Statistics","heading":"25 Statistics","text":"course analyses want need conduct statistical tests. important equipped perform tests R well. Teaching statistical concepts outside scope book. introduction statistical concepts using R, recommend reading David Dalpiaz’s free open R Statistical Learning74. section review implement statistical tests extract useful information well. cover t-tests, ANOVA, linear regression.explore statistics use data Inside Airbnb. interest relationship price superhosts, price room type, finally superhosts room type contribute price.","code":""},{"path":"statistics.html","id":"the-data-1","chapter":"25 Statistics","heading":"25.1 The data","text":"use data hosts listings datasets. former contains superhost data later price room type information.order join two tibbles together need figure common identifiers . listings can infer primary key id foreign key host_id. hosts tibble one id column. Clearly join needs host_id id listings hosts respectively. perform left join select columns price, room_type superhost assign object airbnb.can engage statistical testing due diligence first visualize relationship test . comparing group continuous variable boxplot suffice.Note setting group superhost dummy coded numeric value. ggplot attempting consider numeric rather categorical.Clearly noticeable outliers $2,500 mark. Let’s filter values get testing ’re , let’s convert superhost factor factor(). Save filtered results bnb_filt. Recreate visualization new object.first thing may notices longer specify group aesthetic converted superhost non-numeric format. visualization looks like superhost necessarily increase price listing. can now test means using t.test(). number ways can use function generalizable way use called formula interface.","code":"\nlibrary(tidyverse)\nlibrary(uitk)\n\nlistings <- airbnb_listings\nhosts <- airbnb_hosts\n\nglimpse(listings)## Rows: 3,799\n## Columns: 7\n## $ id               <dbl> 3781, 5506, 6695, 8789, 10730, 10813, 10986, 16384, …\n## $ neighborhood     <chr> \"East Boston\", \"Roxbury\", \"Roxbury\", \"Downtown\", \"Do…\n## $ room_type        <chr> \"Entire home/apt\", \"Entire home/apt\", \"Entire home/a…\n## $ price            <dbl> 125, 145, 169, 99, 150, 179, 125, 50, 154, 115, 148,…\n## $ minimum_nights   <dbl> 28, 3, 3, 91, 91, 91, 91, 91, 29, 1, 2, 2, 2, 6, 3, …\n## $ availability_365 <dbl> 68, 322, 274, 247, 29, 0, 364, 365, 304, 285, 62, 24…\n## $ host_id          <dbl> 4804, 8229, 8229, 26988, 26988, 38997, 38997, 23078,…\nglimpse(hosts)## Rows: 1,335\n## Columns: 9\n## $ id              <dbl> 4804, 8229, 26988, 38997, 23078, 71783, 85130, 85770,…\n## $ name            <chr> \"Frank\", \"Terry\", \"Anne\", \"Michelle\", \"Eric\", \"Lance\"…\n## $ since_year      <dbl> 2008, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 2010,…\n## $ since_month     <chr> \"12\", \"02\", \"07\", \"09\", \"06\", \"01\", \"02\", \"02\", \"03\",…\n## $ since_day       <chr> \"03\", \"19\", \"22\", \"16\", \"24\", \"19\", \"24\", \"26\", \"23\",…\n## $ response_rate   <chr> \"100%\", \"100%\", \"100%\", \"92%\", \"50%\", \"98%\", \"66%\", \"…\n## $ acceptance_rate <chr> \"50%\", \"100%\", \"84%\", \"17%\", NA, \"98%\", \"97%\", \"100%\"…\n## $ superhost       <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1,…\n## $ n_listings      <dbl> 5, 2, 9, 13, 3, 40, 7, 4, 1, 2, 1, 1, 5, 1, 1, 1, 1, …\nairbnb <- left_join(listings, hosts, by = c(\"host_id\" = \"id\")) %>% \n  select(price, room_type, superhost) \n\nglimpse(airbnb)## Rows: 3,799\n## Columns: 3\n## $ price     <dbl> 125, 145, 169, 99, 150, 179, 125, 50, 154, 115, 148, 275, 2…\n## $ room_type <chr> \"Entire home/apt\", \"Entire home/apt\", \"Entire home/apt\", \"E…\n## $ superhost <dbl> 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0,…\nggplot(airbnb, aes(price, superhost, group = superhost)) +\n  geom_boxplot()\nbnb_filt <- filter(airbnb, price < 2500) %>% \n  mutate(superhost = factor(superhost))\n\nggplot(bnb_filt, aes(price, superhost)) +\n  geom_boxplot()"},{"path":"statistics.html","id":"the-formula-interface","chapter":"25 Statistics","heading":"25.2 The formula interface","text":"formula interface way defining statistical formulae. maybe bit clearly let’s us tell R columns use fitting model75. general format takes y ~ x reads y function x. case t-test y variable testing means x group compare. data already tidy format—like Airbnb data—rather easy adhere .","code":""},{"path":"statistics.html","id":"t-tests","chapter":"25 Statistics","heading":"25.3 T-tests","text":"perform t-test use t.test() function arguments formula data. example call looks like t.test(y ~ x, data = df). case y price variable interest dependent variable. Since curious price changes superhost status, put superhost x spot.Conduct t-test store results price_t. Print afterwards.somewhat cluttered buch numbers words. can see t-value (t = 2.3261), degrees freedom (df = 2029.4), p-value (p-value = 0.02011). test can tell alpha level 0.05 can reject null hypothesis.","code":"\n(price_t <- t.test(price ~ superhost, data = bnb_filt))## \n##  Welch Two Sample t-test\n## \n## data:  price by superhost\n## t = 2.3261, df = 2029.4, p-value = 0.02011\n## alternative hypothesis: true difference in means is not equal to 0\n## 95 percent confidence interval:\n##   1.949967 22.904491\n## sample estimates:\n## mean in group 0 mean in group 1 \n##        175.1989        162.7717"},{"path":"statistics.html","id":"tidying-up-after-our-models","chapter":"25 Statistics","heading":"25.3.1 Tidying up after our models","text":"useful, ’re going , point, want extract statistics usable format. Enter broom. documentation:\"broom summarizes key information models tidy tibble()s. broom provides three verbs make convenient interact model objects.tidy() summarizes information model componentsglance() reports information entire modelaugment() adds informations observations dataset76Make sure broom installed install.packages(\"broom\"). installed use function tidy() price_t object,result tibble can easily manipulated worked . naturally want explore beyond just two groups. case must perform analysis variance (ANOVA).","code":"\nbroom::tidy(price_t)## # A tibble: 1 x 10\n##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high\n##      <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>\n## 1     12.4      175.      163.      2.33  0.0201     2029.     1.95      22.9\n## # … with 2 more variables: method <chr>, alternative <chr>"},{"path":"statistics.html","id":"anova","chapter":"25 Statistics","heading":"25.4 ANOVA","text":"ANOVA test used case t-tests . , used want know difference means groups two groups. perform ANOVA use aov() function—initialism aanalysis variance—arguments used t.test(). difference x column two groups—room_type. can fit ANOVA model price y room_type x.print ANOVA model object actually don’t see results anticipating. get thos pass model object function summary().Now find results test: p < 0.001. may notice already inconsistency ways models interacted . use broom, one common way working model objects. ’d like access model results consistent way can use broom::tidy().Remember though ANOVA tests variation two groups. results test tell us groups different. turn Tukey’s Honestly Significant Difference (HSD). Tukey’s HSD creates set confidence intervals compare unique combination variables. perform test R pass ANOVA model object function TukeyHSD()results test show rather significant difference hotel rooms entire homes apartments, private rooms entire homes, shared rooms entire homes, well private room hotel room. use broom ggplot can begin visualize restuls.tidied HSD object can create graph point estimates error bars.personally like call Tie Fighter plots resemble space ships Star Wars.can plotting point estimates comparison.Next can add horizontal error bar (geom_errorbarh()) layer plot. layer requires additional aesthetics set layer . xmin xmax. Respectively used mark minimum maximum extents error bars. case HSD object, bounds th confidence intervals already calculated us can found columns conf.low conf.high. can pass xmin xmax aesthetic arguments.","code":"\n(price_aov <- aov(price ~ room_type, data = bnb_filt))## Call:\n##    aov(formula = price ~ room_type, data = bnb_filt)\n## \n## Terms:\n##                 room_type Residuals\n## Sum of Squares   15152015  70853604\n## Deg. of Freedom         3      3789\n## \n## Residual standard error: 136.7473\n## Estimated effects may be unbalanced\nsummary(price_aov)##               Df   Sum Sq Mean Sq F value Pr(>F)    \n## room_type      3 15152015 5050672   270.1 <2e-16 ***\n## Residuals   3789 70853604   18700                   \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nbroom::tidy(price_aov)## # A tibble: 2 x 6\n##   term         df     sumsq   meansq statistic    p.value\n##   <chr>     <dbl>     <dbl>    <dbl>     <dbl>      <dbl>\n## 1 room_type     3 15152015. 5050672.      270.  7.32e-159\n## 2 Residuals  3789 70853604.   18700.       NA  NA\n(price_hsd <- TukeyHSD(price_aov))##   Tukey multiple comparisons of means\n##     95% family-wise confidence level\n## \n## Fit: aov(formula = price ~ room_type, data = bnb_filt)\n## \n## $room_type\n##                                     diff        lwr         upr     p adj\n## Hotel room-Entire home/apt    -74.062500 -130.09198  -18.033019 0.0038353\n## Private room-Entire home/apt -132.658355 -144.68495 -120.631764 0.0000000\n## Shared room-Entire home/apt  -123.687500 -211.84397  -35.531026 0.0017890\n## Private room-Hotel room       -58.595855 -115.00228   -2.189435 0.0381673\n## Shared room-Hotel room        -49.625000 -153.58946   54.339462 0.6097851\n## Shared room-Private room        8.970855  -79.42567   97.367379 0.9937854\n(price_hsd_tidy <- broom::tidy(price_hsd))## # A tibble: 6 x 6\n##   term      comparison                   estimate conf.low conf.high adj.p.value\n##   <chr>     <chr>                           <dbl>    <dbl>     <dbl>       <dbl>\n## 1 room_type Hotel room-Entire home/apt     -74.1    -130.     -18.0      3.84e-3\n## 2 room_type Private room-Entire home/apt  -133.     -145.    -121.       1.38e-8\n## 3 room_type Shared room-Entire home/apt   -124.     -212.     -35.5      1.79e-3\n## 4 room_type Private room-Hotel room        -58.6    -115.      -2.19     3.82e-2\n## 5 room_type Shared room-Hotel room         -49.6    -154.      54.3      6.10e-1\n## 6 room_type Shared room-Private room         8.97    -79.4     97.4      9.94e-1\n(p <- ggplot(price_hsd_tidy, aes(estimate, comparison)) +\n  geom_point())\np +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))"},{"path":"statistics.html","id":"linear-regression","chapter":"25 Statistics","heading":"25.5 Linear regression","text":"want move inference linear models, turn lm() function. , like t.test() aov() functions requires formula data. difference formulas use bit complex often using many variables. predict y function multiple inputs declare inputs formula takes form y ~ x1 + x2 + .... create linear model predicts price function room type whether host superhost, formula look like price ~ room_type + superhost.output similar ANOVA model. Perhaps can visualize way?Unfortunately tidy don’t columns. However can ask explicitly setting argument conf.int = TRUE.can find possible arguments tidy() exported object broom::argument_glossary.Using similar structure , can create coefficient plot.Unlike t-test, linear model provides much information become useful goodness fit measures, residuals, predicted values. extract can use functions glance() augment() broom.","code":"\nprice_lm <- lm(price ~ room_type + superhost, data = bnb_filt)\n\nsummary(price_lm)## \n## Call:\n## lm(formula = price ~ room_type + superhost, data = bnb_filt)\n## \n## Residuals:\n##     Min      1Q  Median      3Q     Max \n## -219.01  -64.01  -26.01   20.99 1780.99 \n## \n## Coefficients:\n##                        Estimate Std. Error t value Pr(>|t|)    \n## (Intercept)            219.0073     3.0450  71.925  < 2e-16 ***\n## room_typeHotel room    -74.3532    21.8938  -3.396 0.000691 ***\n## room_typePrivate room -132.7222     4.7004 -28.237  < 2e-16 ***\n## room_typeShared room  -123.5526    34.3168  -3.600 0.000322 ***\n## superhost1               0.7244     4.9720   0.146 0.884172    \n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n## \n## Residual standard error: 136.8 on 3788 degrees of freedom\n## Multiple R-squared:  0.1762, Adjusted R-squared:  0.1753 \n## F-statistic: 202.5 on 4 and 3788 DF,  p-value: < 2.2e-16\nbroom::tidy(price_lm)## # A tibble: 5 x 5\n##   term                  estimate std.error statistic   p.value\n##   <chr>                    <dbl>     <dbl>     <dbl>     <dbl>\n## 1 (Intercept)            219.         3.04    71.9   0.       \n## 2 room_typeHotel room    -74.4       21.9     -3.40  6.91e-  4\n## 3 room_typePrivate room -133.         4.70   -28.2   2.34e-159\n## 4 room_typeShared room  -124.        34.3     -3.60  3.22e-  4\n## 5 superhost1               0.724      4.97     0.146 8.84e-  1\nbroom::tidy(price_lm, conf.int = TRUE) %>% \n  ggplot(aes(estimate, term)) +\n  geom_point() + \n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\nbroom::glance(price_lm)## # A tibble: 1 x 11\n##   r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC\n##       <dbl>         <dbl> <dbl>     <dbl>     <dbl> <int>   <dbl>  <dbl>  <dbl>\n## 1     0.176         0.175  137.      203. 1.29e-157     5 -24035. 48081. 48118.\n## # … with 2 more variables: deviance <dbl>, df.residual <int>\nbroom::augment(price_lm) %>% \n  slice(1:5)## # A tibble: 5 x 10\n##   price room_type superhost .fitted .se.fit .resid    .hat .sigma .cooksd\n##   <dbl> <chr>     <fct>       <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>\n## 1   125 Entire h… 1            220.    4.66  -94.7 0.00116   137. 1.12e-4\n## 2   145 Entire h… 1            220.    4.66  -74.7 0.00116   137. 6.94e-5\n## 3   169 Entire h… 1            220.    4.66  -50.7 0.00116   137. 3.20e-5\n## 4    99 Entire h… 1            220.    4.66 -121.  0.00116   137. 1.81e-4\n## 5   150 Entire h… 1            220.    4.66  -69.7 0.00116   137. 6.04e-5\n## # … with 1 more variable: .std.resid <dbl>"},{"path":"spatial-analysis.html","id":"spatial-analysis","chapter":"26 Spatial Analysis","heading":"26 Spatial Analysis","text":"beginning book discussed benefits administrative data. One benefits mentioned administrative data inherently spatial. data tend spatial nature. events happen physical place. context administrative data, know data recorded level must fall within municipal boundaries. Often know location within municipality much finer scale—.e. block group, voting ward, even exact point location.Identifying whether data spatial easiest geographic components present latitude longitude. However, data can also spatial even isn’t explicitly stated. things can keep eye things like neighborhood, towns, county, etc. location specific. likely performing analyses rooted space accounting things counties perhaps without using geospatial techniques.section go basics geospatial analysis.","code":""},{"path":"spatial-analysis.html","id":"types-of-spatial-data","chapter":"26 Spatial Analysis","heading":"26.1 Types of spatial data","text":"Within field Geographic Information Systems (GIS) two general umbrellas data fall . vector raster data.Vector data find working frequently. simply put “points, lines, polygons.” basis vector data coordinate point. Just like scatter plots built, coordinate point combination x y value (longitude latitude respectively). combination x y tell us something . combining two points can trace along path—think connect dots diagrams restaurants kid—create line segments. , however, point lines close, now polygon.umbrella data known raster data. Raster data deal complex data easily captured single point. Rasters used “represent spatially continuous phenomenon.”77 Raster analysis done evaluate things like changing vegetation, elevation slope modeling, analysing reflective surfaces, among much . Raster analysis typically relies satellite imagery LiDAR laser point cloud data. Raster analysis extremely complex topic requires devoted attention. , cover book. , know exists !","code":""},{"path":"spatial-analysis.html","id":"working-with-spatial-data","chapter":"26 Spatial Analysis","heading":"26.2 Working with spatial data","text":"Working spatial data made rather straightforward sf package.sf shorthand simple features. sf let’s us represent physical objects phenomena occur real world data. built upon international standard “describes objects can stored retrieved databases, geometrical operations defined .” (sf vinette, 1).simple features representation vector data. composed points. points usually represented two-dimensionally longitude latitude (x y). can associate third dimension, usually altitude, desired extend three dimensions: longitude, latitude, altitude (x, y, & z). cases, however, used.section working locations Airbnb dataset. locations contains longitude latitude Airbnb listings Boston. example point data (contain two-dimension).","code":""},{"path":"spatial-analysis.html","id":"creating-simple-features-from-a-tibble","chapter":"26 Spatial Analysis","heading":"26.3 Creating simple features from a tibble","text":"Begin installing sf (simple-features) package loading locations Airbnb dataset.far everything . read dataset created tibble. next step make tibble simple feature. Fortunately, sf keeps process rather simple us representing spatial data native R data formats—namely, data frame. make simple features existing tibble, need cast object sf object. st_as_sf().Generally cast objects use functions like .integer() as_tibble(). , prefixed st_. stands spatial transformation. transformations prefixed —effort keep continuity GIS tools. functions cast objects classes function arguments. st_as_sf() unfortunately read mind aware geometry tibble. , need use coords argument st_as_sf(). coords gives us ability tell sf columns contain coordinate points. point data need provide character vector length two x y dimensions aka longitude latitude.Note likely used saying lat, long, actually maps y, x. something trips everyone ! Just make sure put longitude x spot latitude y spot.convert locations data frame simple feature use st_as_sf() set coords argument c(\"longitude\", \"latitude\").Now successfully created simple feature can see longer columns longitude latitude geometry column instead. Notice printed, object tells us type geometry working , ’s dimensions, bounding box points.bounding box furthest extent data reaches latiude longitude.printed object informs us actually two missing pieces information epsg proj4string. failed specify coordinate reference system (CRS). book intended introduction GIS, still worth briefly expanding upon. use CRS trying place points two-dimensions Earth round! Try peeling orange laying peel flat. ’s impossible. now way visualize circle rectangle without introducing error. CRS accounts . many CRS type map projection type unit. , , frustration may encounter working spatial data due mismatching CRS.Fortunately likely working data collected using WGS84 reference system. CRS used define global reference system used consistently throughout government agencies, typically online data recording. Airbnb data uses references system.online data sources use reference system well. example Google Twitter provide data using CRS. times likely encounter CRS isn’t WGS84 working data local agencies need highly accurate tailored spatial data. agencies like water departments, forestry groups, etc.ensure data properly represented space, need provide CRS creation simple features. specifying crs argument. crs accept number indicates projection using. many CRS identifiers commit memory. information usually recorded original data source. sure confirm spatial dimensions! WGS84, CRS identifier 4326. probably worth committing memory.https://confluence.qps.nl/qinsy/latest/en/world-geodetic-system-1984-wgs84-182618391.html#id-.WorldGeodeticSystem1984(WGS84)v9.1-WGS84definitionsWe now create object called loc_sf using st_as_sf() providing coords crs.Since sf object also data frame able perform operations may normal tibble selecting columns, joining, mutating etc.Notice keeps geometry even counting. data spatial, incorporate geometry computations. often times leads slower processing times. immediately need geometry, recommendation join back late possible. can cast sf object tibble as_tibble()Notice now lose geometry column. stopped keeping track geometry.","code":"\n# install.packages(\"sf\")\nlibrary(sf)\nlibrary(tidyverse)\nlibrary(uitk)\n\n# rename for simplicity :) \nlocations <- airbnb_locations\n\nhead(locations)## # A tibble: 6 x 3\n##      id longitude latitude\n##   <dbl>     <dbl>    <dbl>\n## 1  3781     -71.0     42.4\n## 2  5506     -71.1     42.3\n## 3  6695     -71.1     42.3\n## 4  8789     -71.1     42.4\n## 5 10730     -71.1     42.4\n## 6 10813     -71.1     42.3\nst_as_sf(locations,\n         coords = c(\"longitude\", \"latitude\"))## Simple feature collection with 3799 features and 1 field\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549\n## CRS:            NA\n## # A tibble: 3,799 x 2\n##       id             geometry\n##    <dbl>              <POINT>\n##  1  3781 (-71.02991 42.36413)\n##  2  5506 (-71.09559 42.32981)\n##  3  6695 (-71.09351 42.32994)\n##  4  8789 (-71.06265 42.35919)\n##  5 10730  (-71.06185 42.3584)\n##  6 10813 (-71.08904 42.34961)\n##  7 10986 (-71.05075 42.36352)\n##  8 16384  (-71.07132 42.3581)\n##  9 18711 (-71.06096 42.32212)\n## 10 22195  (-71.0793 42.34558)\n## # … with 3,789 more rows\nloc_sf <- st_as_sf(locations,\n         coords = c(\"longitude\", \"latitude\"),\n         crs = 4326)\n\nloc_sf## Simple feature collection with 3799 features and 1 field\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549\n## CRS:            EPSG:4326\n## # A tibble: 3,799 x 2\n##       id             geometry\n##  * <dbl>          <POINT [°]>\n##  1  3781 (-71.02991 42.36413)\n##  2  5506 (-71.09559 42.32981)\n##  3  6695 (-71.09351 42.32994)\n##  4  8789 (-71.06265 42.35919)\n##  5 10730  (-71.06185 42.3584)\n##  6 10813 (-71.08904 42.34961)\n##  7 10986 (-71.05075 42.36352)\n##  8 16384  (-71.07132 42.3581)\n##  9 18711 (-71.06096 42.32212)\n## 10 22195  (-71.0793 42.34558)\n## # … with 3,789 more rows\ncount(loc_sf)## although coordinates are longitude/latitude, st_union assumes that they are planar## Simple feature collection with 1 feature and 1 field\n## geometry type:  MULTIPOINT\n## dimension:      XY\n## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549\n## CRS:            EPSG:4326\n## # A tibble: 1 x 2\n##       n                                                                 geometry\n## * <int>                                                         <MULTIPOINT [°]>\n## 1  3799 ((-71.1728 42.34835), (-71.17174 42.34854), (-71.17173 42.34923), (-71.…\nas_tibble(loc_sf) %>% \n  count()## # A tibble: 1 x 1\n##       n\n##   <int>\n## 1  3799"},{"path":"spatial-analysis.html","id":"plotting-sf-objects-with-ggplot","chapter":"26 Spatial Analysis","heading":"26.4 Plotting sf objects with ggplot","text":"Plotting sf objects made rather straightforward ggplot2. Since sf objects contain ton spatial information inferred ggplot. , required map aesthetics x y. simple provide just data argument ggpplot add geom_sf() layer. Inside geom_sf() can provide arguments may like color, size, shape, etc. passed underlying geom_*—case points, geom_point().great can already somewhat see shape Boston Suffolk County. Since Airbnb points located space, know able associate respective Census tracts.need another spatial data set contains shapes tract. next section read dataset containing shapes tract Suffolk county. Following perform spatial join associate points tracts.","code":"\nggplot(loc_sf) +\n  geom_sf(shape = \".\")"},{"path":"spatial-analysis.html","id":"connecting-points-to-polygons","chapter":"26 Spatial Analysis","heading":"26.5 Connecting points to polygons","text":"Now point locations Airbnb listing need identify tracts belong . data folder file called suffolk_acs.geojson. common spatial data format based json. difference geojson contains lot fields specific spatial data.Reading data format just easy reading csv file. Using sf::read_sf() can pass path geojson file returned sf object.first things ’ll notice looks similar loc_sf object , importantly, CRS picked us! briefly look hood file, can see third line CRS stated. don’t need understand happening . Just know sometimes spatial data sets already information .Let’s see file looks like!Wonderful! two stylist adjustments ’d make visualizing little easier. first change line width something thinner, adjust transparency tracts little lighter. makes map bit easier read .Now, understanding grammar graphics comes handy. now two different data sets good visualize together. Recall specify data top level ggplot() call sets default every single layer. multiple objects may cause conflicts. know, however, can set data per layer. , taking two points together, can plot loc_sf acs_tracts graph set data argument respective geom_sf() layer.plot can get sense density Airbnb listings Boston. seems greater density near Back Bay Beacon Hill. great able know many listings tract average listing price. , need perform two joins. first one spatial—joining point polygon based tract point intersects. second join listings information spatially joined data set. utilized data three different sources!","code":"\nacs_tracts <- read_sf(\"data/suffolk_acs.geojson\")\n\nacs_tracts## Simple feature collection with 203 features and 1 field\n## geometry type:  MULTIPOLYGON\n## dimension:      XY\n## bbox:           xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012\n## CRS:            4326\n## # A tibble: 203 x 2\n##    fips                                                                 geometry\n##    <chr>                                                      <MULTIPOLYGON [°]>\n##  1 250250921… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29301, -7…\n##  2 250251006… (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28961, -7…\n##  3 250250101… (((-71.11093 42.35047, -71.11093 42.3505, -71.11092 42.35054, -71…\n##  4 250250704… (((-71.06944 42.346, -71.0691 42.34661, -71.06884 42.3471, -71.06…\n##  5 250251401… (((-71.13397 42.25431, -71.13353 42.25476, -71.13274 42.25561, -7…\n##  6 250259812… (((-71.04707 42.3397, -71.04628 42.34037, -71.0449 42.34153, -71.…\n##  7 250250511… (((-71.01324 42.38301, -71.01231 42.38371, -71.01162 42.3842, -71…\n##  8 250259816… (((-71.00113 42.3871, -71.001 42.38722, -71.00074 42.3875, -71.00…\n##  9 250250909… (((-71.05079 42.32083, -71.0506 42.32076, -71.05047 42.32079, -71…\n## 10 250251103… (((-71.11952 42.28648, -71.11949 42.2878, -71.11949 42.28792, -71…\n## # … with 193 more rows## {\n## \"type\": \"FeatureCollection\",\n## \"name\": \"suffolk_acs\",\n## \"crs\": { \"type\": \"name\", \"properties\": { \"name\": \"urn:ogc:def:crs:OGC:1.3:CRS84\" } },\n  ggplot(acs_tracts) +\n  geom_sf()\nggplot(acs_tracts) +\n  geom_sf(lwd = 0.25, alpha = 0.5)\nggplot() +\n  geom_sf(data = acs_tracts, lwd = 0.25, alpha = 0.5) +\n  geom_sf(data = loc_sf, shape = \".\")"},{"path":"spatial-analysis.html","id":"spatial-joins","chapter":"26 Spatial Analysis","heading":"26.5.1 Spatial Joins","text":"Like regular join, intent behind spatial join add attributes one data source another. utility spatial join comes shared attribute space. often want perform spatial join looking called intersection. essentially two spatial features touch manner. type spatial join find useful nearest neighbor take attributes next closest object.perform spatial join use function sf::st_join() three main arguments: x, y, join. default join type st_intersects join attributes (columns) y x intersects (meaning touches within). ordering x y important difference left right join. ordering also determines type geometry returned. Whatever type geometry x position returned.Let’s try using st_join() join tract level information point data.great! now fips (census tract code) associated listing id. happens plot data?. like moment plot tracts color number listings contained . means need change order join.Notice 3,814 rows! well original 193 tracts. tried plotting right away may overwork R. need count number observations per fips code first.counting now original 203 rows. spatial equivalent summarizing data geometries aggregated rows (fips) dissolved. Dissolving combines geometries shared attribute—fips case—single geometry. Since fips geometry,resultant geometries unaffected. aware behavior grouping summarizing sf objects!Let’s try plotting counts now.","code":"\npoints_join <- st_join(loc_sf, acs_tracts) ## although coordinates are longitude/latitude, st_intersects assumes that they are planar\n## although coordinates are longitude/latitude, st_intersects assumes that they are planar\npoints_join## Simple feature collection with 3799 features and 2 fields\n## geometry type:  POINT\n## dimension:      XY\n## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549\n## CRS:            EPSG:4326\n## # A tibble: 3,799 x 3\n##       id             geometry fips       \n##  * <dbl>          <POINT [°]> <chr>      \n##  1  3781 (-71.02991 42.36413) 25025051200\n##  2  5506 (-71.09559 42.32981) 25025081400\n##  3  6695 (-71.09351 42.32994) 25025081400\n##  4  8789 (-71.06265 42.35919) 25025030300\n##  5 10730  (-71.06185 42.3584) 25025030300\n##  6 10813 (-71.08904 42.34961) 25025010104\n##  7 10986 (-71.05075 42.36352) 25025030300\n##  8 16384  (-71.07132 42.3581) 25025020101\n##  9 18711 (-71.06096 42.32212) 25025090700\n## 10 22195  (-71.0793 42.34558) 25025010600\n## # … with 3,789 more rows\nggplot(points_join) +\n  geom_sf(shape = \".\")\npolygon_join <- st_join(acs_tracts, loc_sf)\npolygon_join## Simple feature collection with 3814 features and 2 fields\n## geometry type:  MULTIPOLYGON\n## dimension:      XY\n## bbox:           xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012\n## CRS:            4326\n## # A tibble: 3,814 x 3\n##    fips                                                         geometry      id\n##  * <chr>                                              <MULTIPOLYGON [°]>   <dbl>\n##  1 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  3.63e6\n##  2 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  7.22e6\n##  3 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  7.29e6\n##  4 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  3.36e7\n##  5 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  3.63e7\n##  6 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  3.89e7\n##  7 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  4.02e7\n##  8 25025092… (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29…  4.18e7\n##  9 25025100… (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28…  2.64e7\n## 10 25025100… (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28…  3.77e7\n## # … with 3,804 more rows\ntract_listings <- count(polygon_join, fips)## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\n## although coordinates are longitude/latitude, st_union assumes that they are planar\ntract_listings## Simple feature collection with 203 features and 2 fields\n## geometry type:  MULTIPOLYGON\n## dimension:      XY\n## bbox:           xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012\n## CRS:            4326\n## # A tibble: 203 x 3\n##    fips           n                                                     geometry\n##  * <chr>      <int>                                           <MULTIPOLYGON [°]>\n##  1 250250001…    53 (((-71.1609 42.35863, -71.16049 42.35881, -71.16021 42.3589…\n##  2 250250002…    18 (((-71.16782 42.35328, -71.16775 42.35351, -71.16764 42.353…\n##  3 250250002…     4 (((-71.16057 42.35267, -71.16018 42.35269, -71.16005 42.352…\n##  4 250250003…    19 (((-71.1748 42.35051, -71.17475 42.35066, -71.17471 42.3508…\n##  5 250250003…    19 (((-71.17458 42.35024, -71.17287 42.35005, -71.17283 42.350…\n##  6 250250004…    16 (((-71.15473 42.34121, -71.15455 42.34156, -71.15436 42.341…\n##  7 250250004…    23 (((-71.16613 42.34043, -71.16612 42.34059, -71.16611 42.340…\n##  8 250250005…    18 (((-71.16922 42.33807, -71.16909 42.33825, -71.16894 42.338…\n##  9 250250005…    16 (((-71.15336 42.33819, -71.15308 42.33833, -71.15296 42.338…\n## 10 250250005…     9 (((-71.1501 42.33719, -71.14984 42.33767, -71.14966 42.3379…\n## # … with 193 more rows\nggplot(tract_listings, aes(fill = n)) +\n  geom_sf(lwd = 0.25)"},{"path":"spatial-analysis.html","id":"resources","chapter":"26 Spatial Analysis","heading":"26.5.1.1 Resources","text":"http://wiki.gis.com/wiki/index.php/DissolveFor full list spatial joins data type recommend visiting https://desktop.arcgis.com/en/arcmap/latest/manage-data/tables/spatial-joins--feature-type.htm.https://geocompr.robinlovelace.net/https://rud./books/30-day-map-challenge/points-01.html","code":""}]
