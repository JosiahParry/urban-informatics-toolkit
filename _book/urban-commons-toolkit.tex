% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Urban Informatics Toolkit},
  pdfauthor={Josiah Parry},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\usepackage[]{natbib}
\bibliographystyle{apalike}

\title{Urban Informatics Toolkit}
\author{Josiah Parry}
\date{2020-04-20}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{welcome}{%
\chapter{Welcome!}\label{welcome}}

Welcome to the Urban Informatics Toolkit! This is an online book that is intended to jumpstart your work of analyzing and developing understanding of the urban commons. In it you will find material on the theoretical underpinnings of Urban Informatics and a (mostly) thorough introduction to the statistical programming language R to get you hands on and working with data that you will encounter in your research, coursework, and in the wild west of open data.

This book represent to me many things. Of which are the two years of study in the Urban Informatics program at Northeastern, nearly five years of self-directed learning of the R programming language, two years of teach R, and many, many, many hours of frustration trying to understand and grasp concepts that could have been distilled into simple and easy to understand language.

In the following chapters I will do my best to avoid overly technical and verbose descriptions of theory and technical concepts. I will attempt to explain everything in a manner that I would to my friends over a beer, coffee, or, as these strange times would have it, a zoom call.

By the end of these pages I hope that you have become self-sufficient in your analyses. My intention is not to teach you everything that you will need to know because this is \emph{impossible}. Data analytics are always changing and what is current will be dated---perhaps by tomorrow. In fact, whilst in the middle of writing this one of the most commonly used tools was changed in such a way that much of what I wrote became bad practice. This is all to say that the purpose of this book is to make you \textbf{self-sufficient}. The goal is to ensure that \emph{you} are able to understand the fundamental concepts of scientific inquiry in the big data era, how to think about data analysis, and be equipped with the fundamental knowledge of R to understand what is happening---to some degree---under the hood and what and why.

\hypertarget{structure-of-the-book}{%
\section{Structure of the book}\label{structure-of-the-book}}

This book is partitioned into four sections each with it's own theme. In the first section \emph{Foundations of Urban Informatics} we will first begin by exploring the nature of big data and its role in the field. Then, we review different approaches to scientific inquiry and seek to understand how big data has further enabled a newer approach. And finally, we review Broken Windows theory as well as the development of \textbf{ecometrics}.

The second section is dediated to introducing you to R as a programming language for data analysis. This is where we begin our technical work. We will ensure that you have all of the software and data that you will need to follow along with the exercises in this book. This section will be the most difficult to overcome. This is because you must learn an entirely new mental framework---even if you know how to program in another language. In this section we will work from reading in data to manipulating it while along the way learning some fundamental theory.

The third section is the largest and is dedicated entirely to information visualization. In this you will learn how to craft visualizations and understand when to make what kind of graphic. Furthermore we will dive into expanding upon traditional graphics by incorporating many different aesthetics. This section is expansive due to the importance of visualization. Visualization is our method of communication. While the written word is powerful, it requires more work to get someone to read than to look. If we can improve upon our visualization, we can make the work that we do more accessible to the public.

Finally, we will close by learning how to work with multiple datasets and spatial data. These are two of the more advanced topics and as such will be reserved for when the fundamentals have been reinforced. With regards to spatial analysis, many of you who may come from a geography or GIS background may find this section lacking. You would be right. Given the immense depth of the geospatial sciences, that is a topic that is deserving of its own book.

\hypertarget{considerations}{%
\section{Considerations}\label{considerations}}

Before we continue, I want to reitaterate that this book will not introduce you to everything that you will need to know. As the field continues to grow and as the number of tools available in R increase I will work to continually add to and improve this writing. If there are topics that you would like to see included or expanded upon, please submit an issue on GitHub \url{https://github.com/JosiahParry/urban-informatics-toolkit/issues} or reach out to me \href{mailto:josiah.parry@gmail.com?subject=\%5BUITK\%5D\%20Feedback}{directly}.

In this next chapter we will discuss big data.

Without further ado, let's get on with it!

\hypertarget{part-foundations-of-urban-informatics}{%
\part{Foundations of Urban Informatics}\label{part-foundations-of-urban-informatics}}

\hypertarget{the-utility-and-danger-of-big-data}{%
\chapter{The Utility and Danger of Big Data}\label{the-utility-and-danger-of-big-data}}

Urban Informatics is in a way a byproduct of the ``deluge'' or ``proliferation'' of data. In essence, as we as a society have progressed technologically, we have been able to capture and store data on a rather unprecedented scale. This has led to massive stores of data that are used primarily for record keeping that are updated at near-real-time. For example, think of every time you make a Facebook post or send a tweet. That post or tweet is subsequently recorded in a remote database to ensure that it can be accessed at a later time. Dan O'Brien notes that this characteristic of big data is of the utmost consequence for ``the advancement of science and policy in the digital age''\footnote{Page 59 of \href{https://www.hup.harvard.edu/catalog.php?isbn=9780674975293}{The Urban Commons: How data and technology can rebuild our communities.} by Daniel T. O'Brien.}. These naturally occurring data are so useful because they are essentially a track record of individuals' behavior over time. It is in a way as close as we can get to measuring behavior at real time.

If we turn to the urban context, the importance of big data may become ever more apparent. For example, Boston local government has been keeping detailed records of property assessments, taxes owed and paid, by whom, their demographic characteristics, when and even where down to the ward level (Boston Public Library, recently digitized records). These administrative records have been kept on ink and paper until just a few decades ago. Through digitization efforts, these data are now accessible to historians, urban scholars, local government, and the general public. Having such data accessible provides a way to quantitatively inspect the development of the city from its geography, its policies, its demography, and much more that we have yet to see uncovered.

There are a number of benefits that naturally occurring data provide. The first is that these are, in theory, comprehensive and contains information about all residents. Through administrative data we should, for example, be able to determine the number of employed tax paying citizens as well as the underemployed who receive government benefits. Additionally, since these data are already being collected, the associated costs are minimal. In contrast to empirically collected data, administrative data are not just a representation of a single moment in time, but rather continually changing and updating. And due to the fact that administrative data are collected at the municipal level we are inherently dealing with geospatial data---data that have a location associated it.

While there are many benefits to administrative big data, there are a couple of dangers we ought to be aware of. The first is that even though big data are comprehensive in theory, we cannot always take them as objective observations of the natural world. We must be cognizant of the fact that the biases that humans have are also represented in data. We cannot and should not separate theory from data. To take from Dan O'Brien

\begin{quote}
``. . . the very point of science is to explain why things work the wsay they do. . .If we limit our inquiries only to correlation and eschew explanation, we are no longer conducting science.''\footnote{Page 61 of The Urban Commons.}
--- Daniel T. O'Brien
\end{quote}

Furthermore, we should always be wary of the data that we use. Investigate its integrity, its source, its measurment constructs for what we may be using to understand one thing may be something else which we may not anticipate or expect. And laslty, always be aware of the ways in which you may be bringing in your own world views into your work as what we are after is not confirmation but understanding.

\hypertarget{data-in-the-municipal-context}{%
\chapter{Data in the municipal context}\label{data-in-the-municipal-context}}

The so called ``proliferation of data'' has created vast troves of data asking to be explored. We are, in essence, in the beginning of a new Gold Rush. But rather than discovering gold, today the gold is both being created an discovered. This explosion of data is the product of improved technology in both the collection and storage of data.

If we focus our gaze towards the municipal government, the story is similar, progress is slower, and the data are more familiar. Local governments have been collecting data for centuries but until recently it was not always accessible, or even considered ``data''. Take the city of Boston as an example. Since the 19th century boston has been issuing and recording building permits. Through a massive digitization effort these permits are now accessible in an online database \footnote{Boston Building Permits: \url{https://www.boston.gov/departments/inspectional-services/how-find-historical-permit-records}}. Not only are governments slowly turning to modern methods of data storage, but they are also creating applications to encourage citizens to engage with their local governments. Mobile and web applications will hopefully facilitate greater interaction between citizen and government \footnote{Some note about co-production.}. Each and every one of these citizen to government interactions are recorded and stored in database---though not all are open and accessible to the citizen scientist.

Boston has built a few mobile applications for its residents. Notable among these apps are the BOS:311\footnote{BOS:311: \url{https://itunes.apple.com/us/app/boston-citizens-connect/id330894558?mt=8}}, ParkBoston\footnote{ParkBoston: \url{https://apps.apple.com/us/app/parkboston/id953579075}}, the city's least favorite Boston PayTix\footnote{Boston PayTix: \url{https://apps.apple.com/us/app/boston-paytix/id1068651854}}, and the new Blue Bikes\footnote{BlueBikes}. Through BOS:311 residents can communicate directly to the Department of Public Works by recording an issue, it's location, and even an image of the issue. Blue Bikes trips, 311 requests, and much more are provided to the public via Analyze Boston, Boston's data portal\footnote{\url{https://data.boston.gov/}}.

This new availability of data has unintentionally altered the way in which scientists interact with data. For the purposes of scientific inquiry, scientists and analysts have historically been rather close to the data generation process. While we as residents and citizens interact with governmental agencies, it is not in the name of science. And the governmental agencies are engaing with residents in for the purpose of governance, not science. As such, much---if not all---of the open and public data that we interact within the urban informatics---and greater digital humanities---fields was not generated with the express purpose of being analyzed. This inherently changes the way in which analyses are approached.

In approaching data of this nature, researchers have began embracing a paradigm of \emph{exploratory data analysis} (EDA). EDA is extremely useful for developing insights from data in which there were no a priori\footnote{``Relating to or denoting reasoning or knowledge which proceeds from theoretical deduction rather than from observation or experience.'' \href{https://www.lexico.com/definition/a_priori}{Oxford Dictionary}} hypotheses. In their influential book \href{https://r4ds.had.co.nz}{\emph{R for Data Science}}\footnote{\href{https://www.hup.harvard.edu/catalog.php?isbn=9780674975293}{R for Data Science}}, Garret Grolemund and Hadley Wickham describe this inductive approach of exploratory data analysis.

\begin{quote}
``Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again.'' \footnote{\href{https://r4ds.had.co.nz/explore-intro.html}{Introduction - R for Data Science}}
\end{quote}

When researchers set out to test a hypothesis they often will become closely involved with the data generation process. In this scenario, researchers are more likely to have preconceived hypotheses and expectiation of what they may find hidden in their data.

This condition is often not the case when working with open data. We do not always know at the outset of what we are looking for. With open data---and any data really---you never know what you may find if you begin to dig. Whip out your hand shovel and prepare to upturn the soil. You might find seedlings that may sprout into your next study.

\hypertarget{approaches-to-and-schools-of-urban-informatics}{%
\chapter{Approaches to and Schools of Urban Informatics}\label{approaches-to-and-schools-of-urban-informatics}}

\hypertarget{scientific-approaches}{%
\section{Scientific approaches}\label{scientific-approaches}}

In the sciences generally, there are two approaches to scientific inquiry: inductive and deductive. These two approaches can be best characterized as ``bottom up'' and ``top down'' respectively. Each has their own origins, strengths, and weaknesses. An argument has been raging---in the scientific sense of raging---since the 17th century about which approach is the best one. My answer to this? Both, and neither. And you'll see why shortly.

Let's start by getting a grasp on deductive approaches (also referred to as deduction). With deduction we start with a theory about the workings of some observed phenomenon. From this theory, we create a hypothesis, then observe (or collect data on) the phenomenon. With this data we then confirm or refute our theory. This is how we all have, most likely, been taught about the scientific method.

Induction works in a reverse order. It works by looking at the natural world and doing just that, looking. It works by noticing something---a pattern, a unique occurrence---then noticing it again, and then again, and then under slightly different conditions. From those observations, we draw hypothesis. We then observe yet again and see if we can refute or add a little bit more credibility to our findings. Then from doing this time and again, we can build a theory. It's somewhat like Sherlock and Watson finding clues and then coming to (frighteningly specific) conclusions. These theories, no matter how we get there, are the frameworks that we use to try and explain phenomena that we see.

\hypertarget{the-chicago-school}{%
\section{The Chicago School}\label{the-chicago-school}}

Much of what is known as the urban sciences today can be traced back to the late 19th and early 20th centuries at the University of Chicago. The University of Chicago was at the time the epicenter of the new field of American Sociology which came to be known as the Chicago School. In that era, the social sciences were seeking to create grand theories of the world. Take, for example, the new field of Anthropology that crafted theories about the origins of the human race. The Chicago School ``fostered a very different view of sociology: first-hand data collection was to be emphasized; research on the particular case or setting was to be stressed; induction over deduction was to be promoted''\footnote{Turner, Jonathan H. \href{https://journals.sagepub.com/doi/10.2307/1389202}{``The mixed legacy of the Chicago school of sociology.''} Sociological perspectives 31, no. 3 (1988): 325-338.}.

Scholars such as Robert Park, Ernest Burgess, and Louis Wirth developed a number of micro-theories to understand the city. Most notably is the culminating work The City by Park and Burgess, a collection of essays that encapsulate decades of careful observation that led to a number of theories that still have influence today. Their body of work, important in so many ways, is an early paragon of an inductive approach to social research.

\hypertarget{complexity-and-santa-fe-institute}{%
\section{Complexity and Santa Fe Institute}\label{complexity-and-santa-fe-institute}}

On the other end of the spectrum rest the Santa Fe Institute (SFI) and their complexity science. The SFI's mission is to ``endeavor to understand and unify the underlying, shared patterns in complex physical, biological, social, cultural, technological, and even possible astrobiological worlds.''\footnote{\href{https://www.santafe.edu/about}{Santa Fe Institute}} Crudely, it is their goal to unify theory into one general master theory. Central to their theoretical focus is the the view that ``valuable ideas are often confined to the echo chambers of academia.''\footnote{\href{https://www.santafe.edu/about}{Santa Fe Institute}} In this view, they are not wrong.

Their work is important for bridging many so called gaps between disciplines. They apply biological theories of scaling to those of human development. And their findings have been fruitful! Their focus on the interconnectivity of theory of both the natural and social worlds is in some ways the messy work that must be done. This too is in the spirit of the Chicago School as illustrated by it's view of the City as an organism.

\hypertarget{a-hybrid-approach}{%
\section{A Hybrid Approach}\label{a-hybrid-approach}}

As part of this proliferation of big data we have more and better data within reach. As such we are able to, perhaps even encouraged to, take a much more hybrid approach. Within the these data are a multitude of opportunities to explore and glean patterns that we may have been so subtle that it hasn't been observed before. Or, alternatively, the data had not been collected before. This allows us to take a much more inductive approach where we can craft theories from the patterns that we observe, then we can test those theories in creative ways. It is here where, I believe, that the Boston Area Research Initiative (BARI) rests on the spectrum. This hybrid methodology incorporates both inductive and deductive approaches.

\includegraphics{static/hybrid-approach.png}

The above graphic is m best attempt to illustrate this hybrid model. We start with data. We use publicly available (open) administrative data to explore. We get our metaphorical hands dirty with the data. After we munge it, transform it, and rearrange it, we will walk away with some tidbit of information. From that we discover more. And at the end of the day we develop a theory---a framework for understanding what we have observed. Next, we then develop a hypothesis using that new theory and apply it to some other set of data or some new circumstance to test and refine the theories we have developed. In this we both create theory from observation, and test those theories on new and unexplored observations.

Dan O'Brien, Director of BARI, claims to track more with the Chicago School in their approach---and this is correct. But, BARI also actively seeks to evaluate existing theory. There are no better examples of the BARI approach than the development and use of \textbf{ecometrics} to understand the City and test existing theory.

In the next chapter we will learn about ecometrics, their origins, and their use in evaluating the prominent crimoinological \emph{Broken Windows} theory.

\hypertarget{ecometrics}{%
\chapter{Ecometrics}\label{ecometrics}}

Central to the work that is done at BARI are the development and utilization of \textbf{ecometrics}. Ecometrics represent a quantitative representation of physical and social environment. In the Urban Informatics context, ecometrics are created to extract \textbf{latent constructs}---or variables that can only be inferred from a dataset---that illustrate some physical or social phenomenon.

To understand this, we need to again contextualize these datasets. They \emph{are not} created with the intention of beng analyzed or to measure the blight of a neighborhood or the social unrest of a city. The data may tell a story of an underserved neighborhood or of a gilded community with a beautiful brick façade with next to no collective efficacy efforts. These datasets contain gems---beautifully inelegant snapshots of the societal quotidian. But measuring that? That's the tough part and that is why we create ecometrics. They provide us with a way to adapt existing data to address new problems.

Of the work that BARI conducts, the production of city-wide ecometrics of social and physical disorder are most emblematic of this hybrid approach. To understand this work we need to venture back to 1982 and an article from the Atlantic called \emph{Broken Windows}\footnote{Wilson, James Q., and George L. Kelling. ``Broken windows.'' Atlantic monthly 249, no. 3 (1982): 29-38. \url{https://www.theatlantic.com/magazine/archive/1982/03/broken-windows/304465/}.}.

\hypertarget{broken-windows-theory}{%
\section{Broken Windows Theory}\label{broken-windows-theory}}

During the beginning of the crack-cocaine epidemic George Kelling and James Wilson wrote a now {[}in{]}famous article titled \emph{Broken Windows} which outlined a new theory to explain the occurrence of crimes. The premise of this article is that the \emph{presence} of disorder is more concerning for a neighborhoods residents than the actual crime that occurs. Further, the ``visual cues of disorder \ldots{} begets predatory crime and neighborhood decline''\footnote{O'Brien, Daniel Tumminelli, and Robert J. Sampson. ``Public and private spheres of neighborhood disorder: Assessing pathways to violence using large-scale digital records.'' Journal of research in crime and delinquency 52, no. 4 (2015): 486-510. \url{https://journals.sagepub.com/doi/abs/10.1177/0022427815577835}.}.

Broken Windows captured the eyes of pundits and policy makers. The simplicity of the theory makes it easy to Broken windows has historically captured the attention of policy makers. The vast public support has led to a large body of work largely disputing the merits of this theory. In the process of doing so, much work has gone into actually quantifying disorder in a city. In a seminal article by Sampson and Raudenbush (1999), the practice of systematic social observation was created\footnote{Sampson, Robert J., and Stephen W. Raudenbush. ``Systematic social observation of public spaces: A new look at disorder in urban neighborhoods.'' American journal of sociology 105, no. 3 (1999): 603-651. \url{https://www.journals.uchicago.edu/doi/abs/10.1086/210356}.}. This is a process in which imagery of public spaces is taken and coded to identify disorder---i.e.~the presence of empty beer cans---which can then be quantitatively analyzed. This is an early example of an ecometric.

\hypertarget{quantifying-disorder}{%
\section{Quantifying Disorder}\label{quantifying-disorder}}

In 2015, O'Brien and Sampson published the article \emph{Public and Private Spheres of Neighborhood Disorder: Assessing Pathways to Violence Using Large-scale Digital Records}\footnote{O'Brien, Daniel Tumminelli, and Robert J. Sampson. ``Public and private spheres of neighborhood disorder: Assessing pathways to violence using large-scale digital records.'' Journal of research in crime and delinquency 52, no. 4 (2015): 486-510. \url{https://journals.sagepub.com/doi/abs/10.1177/0022427815577835}.}. This article epitomizes the hybrid approach to urban studies. In it, O'Brien and Sampson utilize 911 dispatches and 311 call data to create measures of both social and physical disorder. These measures were then used to put Broken Windows theory to the test. The process of using existing administrative datasets as a method of estimating social and physical phenomena illustrates the inductive approach. Whereas testing testing the efficacy of Broken Windows is indicative of the more traditional deductive process.

Quantifying disorder is no small task. In their 2015 paper the authors write

\begin{quote}
Taking up this challenge, O'Brien, Sampson, and Winship (2015) have proposed a methodology for ecometrics in the age of digital data, identifying three main issues with such data and articulating steps for addressing each. These are (1) identifying relevant content, (2) assessing validity, and (3) establishing criteria for reliability. \footnote{O'Brien, Daniel T., Chelsea Farrell, and Brandon C. Welsh. ``Looking through broken windows: the impact of neighborhood disorder on aggression and fear of crime is an artifact of research design.'' Annual Review of Criminology 2 (2019): 53-71. \url{https://www.annualreviews.org/doi/abs/10.1146/annurev-criminol-011518-024638?journalCode=criminol}.} \footnote{O'Brien, Daniel Tumminelli, and Robert J. Sampson. ``Public and private spheres of neighborhood disorder: Assessing pathways to violence using large-scale digital records.'' Journal of research in crime and delinquency 52, no. 4 (2015): 486-510. \url{https://journals.sagepub.com/doi/abs/10.1177/0022427815577835}.}
\end{quote}

The above is an astute summation of the problems that arise with big data and how they can be overcome. The biggest of concerns, as mentioned in the opening of this section, is the validity of the data we are using.

\hypertarget{defining-the-phenomenon}{%
\subsection{Defining the phenomenon}\label{defining-the-phenomenon}}

The method that they propose requires us to do three main The first is to clearly define the phenomenon that we are hoping to measure. Following, we must idenify the \textbf{relevant data}. For example, in O'Brien and Sampson (2015), they define five ecometrics as

\begin{quote}
\begin{itemize}
\tightlist
\item
  Public social disorder, such as panhandlers, drunks, and loud disturbances;
\item
  Public violence that did not involve a gun (e.g., fight);
\item
  Private conflict arising from personal relationships (e.g., domestic
  violence);
\item
  Prevalence of guns violence, as indicated by shootings or other inci-
  dents involving guns; and
\item
  Alcohol, including public drunkenness or public consumption of
  alcohol.\footnote{O'Brien, Daniel Tumminelli, and Robert J. Sampson. ``Public and private spheres of neighborhood disorder: Assessing pathways to violence using large-scale digital records.'' Journal of research in crime and delinquency 52, no. 4 (2015): 486-510. \url{https://journals.sagepub.com/doi/abs/10.1177/0022427815577835}.}
\end{itemize}
\end{quote}

These definitions provide clear pictures as to what is being measured. The next step is to surf through your data and do your best to match variables or observations to these measures. Then, through some process---usually factor analysis---ensure that these measures are truly relevant.

\hypertarget{validating-the-measure}{%
\subsection{Validating the measure}\label{validating-the-measure}}

Once an ecometric has been defined and properly measured, the next step is to validate it. I think of this process similar to ground truthing in the geospatial sciences. Often when geographic coordinates are recorded an individual will go to that physical location and ensure that whatever that was recorded to be there actually is. This is what we are doing with our ecometrics. We have developed our measures, but we need to compare that to some objective truth so to say.

In Sampson \& Raudenbush (1999), they develop measures of physical disorder through their systematic social observation\footnote{Sampson, Robert J., and Stephen W. Raudenbush. ``Systematic social observation of public spaces: A new look at disorder in urban neighborhoods.'' American journal of sociology 105, no. 3 (1999): 603-651. \url{https://www.journals.uchicago.edu/doi/abs/10.1086/210356}.}. But in order to validate their measures, they compared their results to those of a neighborhood audit. This audit served as their ground truth and was used to make any adjustments if needed.

\hypertarget{addressing-reliability}{%
\subsection{Addressing reliability}\label{addressing-reliability}}

This ecometric, like most others, are naturually time bound snapshots of the social and physical world. These measures will naturally change over time. Because of this it is useful to know both how reliable the measure will be for different periods of time\footnote{O'Brien, Daniel T., Chelsea Farrell, and Brandon C. Welsh. ``Looking through broken windows: the impact of neighborhood disorder on aggression and fear of crime is an artifact of research design.'' Annual Review of Criminology 2 (2019): 53-71. \url{https://www.annualreviews.org/doi/abs/10.1146/annurev-criminol-011518-024638?journalCode=criminol}.}. The authors do with with a bit of statistical finesse that is best left to them to explain. But what we are to take away is that ecometrics are often time variant and it is important for us to know at what time scale the ecometrics are intended for.

\hypertarget{part-toolkit-foundations}{%
\part{Toolkit Foundations}\label{part-toolkit-foundations}}

\hypertarget{the-basics}{%
\chapter{The basics}\label{the-basics}}

\hypertarget{what-is-r-and-why-do-i-care}{%
\section{What is R and why do I care?}\label{what-is-r-and-why-do-i-care}}

What is R? R is the 18th letter of the alphabet, the fourth letter in \emph{QWERTY}---like the keyboard---and, most importantly, R is a software package for statistical computing.

First, a brief history lesson. R is a descendant of the S statistical programming language whose naissance can be traced back to 1976 in Bell Laboratories\footnote{The S System. John Chambers. \url{https://web.archive.org/web/20181014111802/http://ect.bell-labs.com/sl/S/}.}. As S developed, people sought to commercialize the language. In 1993, the license as well as development and selling rights were given to a private company. From then on, and what is still the case today, S became available only as the commercialized S-PLUS\footnote{Statistical Sciences, Inc.~Douglas Martin. 1996. \url{https://github.com/JosiahParry/r-history/raw/master/lit/S/statsci-splus-death.pdf}.}.

Later, seeing a need for an improved statistical software environment, two researchers from the University of Auckland created a new statistical programming language, this became known as R. R was developed in the image of S. However, one important early decision to make the R-project free and open source changed its fate dramatically.

Today, the R-project is developed and maintained by a group known as the R Core who ``represent multiple statistical disciplines and are based at academic, not-for-profit and industry-affiliated institutions on multiple continents''\footnote{R: Software Development Life Cycle. A Description of R's Development, Testing, Release and Maintenance Processes. March 25, 2018. The R Foundation for Statistical Computing c/o Institute for Statistics and Mathematics \url{https://www.r-project.org/doc/R-SDLC.pdf}.}. They define R as below.

\begin{quote}
R is an integrated suite of software facilities for data manipulation, calculation and graphical display.\footnote{The R Project. \url{https://www.r-project.org/about.html}.}
\end{quote}

To simplify it, R can be thought of as a fancy calculator. R was designed to do math, specifically statistics. R was designed to be extended to incude further capabilities than just statistics. Indeed it has been. While R is for all intents and purposes a programming language, one should, in theory, feel like they are doing data analysis and not programming\footnote{The S System. John Chambers. \url{https://web.archive.org/web/20181014111802/http://ect.bell-labs.com/sl/S/}.}.

R is unique from other commercial statistical software such as stata and SPSS. Very fundamentally, R is a free project. While it is monetarily free, free refers to ``liberty, not price''\footnote{GNU. \url{https://gnu.org}.}. In order to truly understand the adventure you will be embarking on shortly, I think it is important you familiarize yourself with the four freedoms of free software. These are:

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The freedom to run the program as you wish, for any purpose (freedom 0).
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The freedom to study how the program works, and change it so it does your computing as you wish (freedom 1). Access to the source code is a precondition for this.
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The freedom to redistribute copies so you can help others (freedom 2).
\end{enumerate}
\end{quote}

\begin{quote}
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The freedom to distribute copies of your modified versions to others (freedom 3). By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this. \footnote{GNU. \url{https://gnu.org}.}
\end{enumerate}
\end{quote}

These freedoms are a large part of the success of R as a language. Because of the free nature of R, academics and industry experts from around the globe are contributing to the language. This means that many new statistical techniques are first implemented in R.

The contributions that people make to R are changing the ways in which people perform data analysis. Because of this, we need to start contextualizing the tooling we use as \textbf{part of} the scientific process---not apart from it. When you engage in your analyses and work on contribute to the vast body of scientific literature, remember that without the tools you are using, much of it would not be possible. When you engage in science, think to yourself how you are adhering to the four essential freedoms. Are you enabling others to do with your findings as they wish? Will your research be accessible to the greater community? What will you do to "give the whole community a chance to benefit from your {[}work{]}?

\hypertarget{the-rstudio-ide}{%
\section{The RStudio IDE}\label{the-rstudio-ide}}

When R is downloaded, interacting with it is somewhat of a cumbersome process. While some people love it, it can feel like programming in the matrix.

For this reason, we will use RStudio to program in R. RStudio is an integrated development environment (IDE). This means that most of the features that you will need to develop in R will all be in one place. RStudio gives you a place to write your R code, execute it, view the awesome graphics you produce, and much more.

I like to think of R as typesetting a printing press and using RStudio like using Mircrosoft word. Chester Ismay and Albert Kim's Modern Dive, provide another excellent analogy of R and RStudio. They describe R as the engine of a car, and RStudio as the dashboard\footnote{Modern Dive. \url{https://moderndive.com/1-getting-started.html}.}.

Let's get familiar with RStudio. You need to know where you are when working within RStudio. There are 4 quadrants that we work with (called panes).

\includegraphics{static/rstudio-editor.png}

The above graphic is borrowed from RStudio, PBC's Thomas Mock's Introduction to the Tidyvers\footnote{Intro to RMarkdown. Thomas Mock. \url{https://github.com/jthomasmock/intro-tidyverse/blob/master/intro-to-tidyverse.rmd}.}

\hypertarget{the-editor}{%
\subsection{The Editor}\label{the-editor}}

The editor. The top left pane. This is where you will actually write your code. You will see in the image above that there is tab with the name of the R file being edited, \texttt{mpg-plot.R}. The simplest way in which R code is written, is in documents with the \texttt{.R} extention. Think of the R script as your word document. This is where you put the writing that you want to keep.

There is also a second type of R file called an RMarkdown document, \texttt{.Rmd}s. These are a special type of file that lets us intersperse regular prose with code chunks. Rmd is extremely flexibile and enabled the user to render their content in many different formats such as a pdf, powerpoint, html, and others. For example, this book is written with RMarkdown. But, to keep things simple, we will use R scripts for the vast majority of this book, plus they're my favorite way to interact with R.

\hypertarget{the-console}{%
\subsection{The console}\label{the-console}}

Let us now avert our attention to the bottom left pane. This is known as the console. The console is where your R code is actually execute. When you run a line or chunk of code from your editor, you will see it processed in the console. I often treat my console as my scratch paper. This is a place where I can explore R objects and code without affecting the primary R file. You should become comfortable typing your R code in the editor and see it executed in the console.

\hypertarget{output}{%
\subsection{Output}\label{output}}

Now moving over to the right. This is the most versatile quadrant of RStudio. You will primarily use this quadrant to look at things. There is a pane for navigating your files, looking at help documentation, viewing the charts that you produce, and any interactive applications you may develop.

\hypertarget{installing-r-rstudio}{%
\section{Installing R \& RStudio}\label{installing-r-rstudio}}

Now that you are somewhat familiar with R and RStudio, it is time to install them. I recommend installing R \emph{and then} RStudio.

R can be downloaded from the \textbf{C}entral \textbf{R} \textbf{A}rchival \textbf{N}etwork (CRAN). CRAN is the official location for all things R. CRAN provides access to the R software, license, copyright, and software extensions (called R packages). Go to \href{https://cran.r-project.org/}{CRAN} to download R\footnote{CRAN. \url{https://cran.r-project.org/}.}.

RStudio is provided by RStudio, Public Benefit Corporation (RStudio, PBC). To install RStudio navigate to the \href{https://rstudio.com/products/rstudio/download/\#download}{download page}\footnote{RStudio IDE Download. \url{https://rstudio.com/products/rstudio/download/\#download}.}.
Once both have been installed you can open RStudio to get started. Look for the circular R logo. If you get lost navigating the RStudio IDE, be sure to refer to the \href{https://github.com/rstudio/cheatsheets/raw/master/rstudio-ide.pdf}{cheat sheet}.

\hypertarget{preventative-care}{%
\section{Preventative Care}\label{preventative-care}}

\hypertarget{r-projects}{%
\subsection{R Projects}\label{r-projects}}

Once you open up RStudio you will be able to get rocking and rolling. Though, I want to instill some best practices from the get go. This following section will save you and anyone who you collaborate with an undescribable amount of headaches.
Folks are tempted to open up RStudio and begin doing analysis. That is all well and good though this leads to many problems. We need to contextualize each and every analysis as it's own \textbf{project}. Currently I mean project in the conceptual manner. If there is a common overarching theme, intent, or purpose, that analysis should be delineated as its own project and should be identifiable from others.

You probably already have a notion of projects implemented in your life. Consider your school work. I suspect, and frankly hope, that you have a somewhat organized folder structure where each semester is its own folder, and each course is its own folder within that. An example of what some of my folder organization looks like is below.

\begin{verbatim}
fall/
  big-data-for-cities/
    projects/
  urban-theory/
    literature/
  
spring/
  info-design/
    data/
  intro-data-mining/
\end{verbatim}

In the above case we would consider each course as its own project. The important thing to keep in mind here is that each project is self-contained. By working inside of self-contained folders we can ensure that there are no problems with accessing files. R uses the concept of a working directory. Think of the working directory as ``where do I start when I am looking for things?''

Imagine R is using the working directory \texttt{spring} but you are working on your \texttt{info-design} work. Conceptually, you feel as if you're working from the \texttt{info-design} folder, but R is actually under the impression you are working from \texttt{spring}. So when you try to load some data from the \texttt{data} folder inside of \texttt{info-design} (which looks like \texttt{info-design/data}), you have to tell R how to get there. And the way R would get there is probably different than you think at the time.

To prevent this, we can essentially level set with R by creating a project. An R project gently imposes the standalone structure that we will need to prevent most of the headaches described above.

The way to create a new project is by navigating to \texttt{File\ \textgreater{}\ New\ Project}.

\begin{figure}
\centering
\includegraphics{static/01-project.png}
\caption{Step 1: new Project}
\end{figure}

Click \texttt{New\ Directory}. This will create a new folder for you. Next, RStudio will prompt you to specify what kind of project to create. Today, that will be a generic \texttt{New\ Project}. In the future, I suspect you will end up creating Shiny applications and much more.

\begin{figure}
\centering
\includegraphics{static/02-project.png}
\caption{Step 2: New Project}
\end{figure}

The final step is to specify what the project will be called and where to put it. In the below image I name the new directory \texttt{uitk}, short for \emph{Urban Informatics Toolkit}, and place it in the directory \texttt{R}. Be sure to select which folder you want your project to live in.

A few tips:

\begin{itemize}
\tightlist
\item
  Think about where you will be able to find the project again
\item
  Where would it make the most sense for the project to live?
\item
  \textbf{Do not} put spaces or periods in the directory name. Use \texttt{\_} or \texttt{-} if you feel the need.
\end{itemize}

\begin{figure}
\centering
\includegraphics{static/03-project.png}
\caption{Step 3: Naming the project}
\end{figure}

This will open up a new RStudio session. You will notice in your \texttt{Files} pane that there is now a \texttt{uitk.Rproj} file there. That file is what tells RStudio about the project, so don't delete it! If you would like to open up an RStudio project you can either open the \texttt{.Rproj} file from your file navigator or open it by following \texttt{File\ \textgreater{}\ Open\ Project}.

\hypertarget{the-data}{%
\subsection{The data}\label{the-data}}

In order to complete the exercises throughout this book you will need to have the data accessible to you. You can download the data \href{https://github.com/JosiahParry/urban-informatics-toolkit/raw/master/data.zip}{here}\footnote{Urban Informatics Toolkit Data. \url{https://github.com/JosiahParry/urban-informatics-toolkit/raw/master/data.zip}}. This link will download a file called \texttt{data.zip}. Once downloaded open the file. It will create a folder called \texttt{data}. Move that entire folder into your new project. If you created a project called \texttt{uitk} in the \texttt{R} directory, move the folder to \texttt{\textasciitilde{}/R/uitk}. This will create a folder path of \texttt{\textasciitilde{}/R/uitk/data}.

\hypertarget{your-workspace}{%
\subsection{Your Workspace}\label{your-workspace}}

In another effort to impose good habits and reproducibility standards I will suggest you change one setting in RStudio. Navigate to \texttt{Tools\ \textgreater{}\ Global\ Options} now change the below setting.

\includegraphics{static/save-workspace.png}

This setting makes it so that your analysis is dependent upon the code you write, not the things you create while interactively programming. A general rule of thumb is that your R script should be able to run from top to buttom successfully.

\hypertarget{before-we-embark}{%
\section{Before we embark}\label{before-we-embark}}

Lastly, I want to emphasize that R and RStudio can be used for so much more than statistical analysis. It can be used to make aRt.

It can be used to make beautiful graphics for the BBC.

\includegraphics{static/bbplot_example_plots.png}

R can be found in the infrastructure of our modern world. R is utilized in our global financial institutions, civil rights groups such as the ACLU, investigative journalism, national defense, and so much more. Do not feel that the \emph{only} thing you will get from learning R is how to do some simple statistics.

\hypertarget{getting-help}{%
\section{Getting Help}\label{getting-help}}

Undoubtedly you will run into problems when programming and you're going to want or need help. If you don't know where to go, getting help may feel impossible. There are two main places that I recommend you go to for help when you encounter problems: Stack Overflow and RStudio community.

\href{https://stackoverflow.com}{Stack Overflow} is one piece of the large Stack Exchange network that is dedicated strictly to programming issues\footnote{Stack Overflow. \url{https://stackoverflow.com}.}. There is a good chance that you've encountered Stack Exchange at some point in your various google searches. Stack Exchange is a network that is strictly dedicated to a question and answer format. Each topic has their own subdomain. For those who are unfamiliar, think of Stack Exchange like Reddit where each subreddit is their own own Q\&A page. And for those who are unfamiliar with Reddit, think of Stack Overflow like Yahoo! Answers or Quora. Stack Overflow has a huge database of questions and answers for all of the programming problems that folks have encountered. It is very unlikely to have a question that has not been answered on Stack Overflow before.

The \href{https://community.rstudio.com/}{RStudio Community} page is a community forum created by RStudio\footnote{RStudio Community. \url{https://community.rstudio.com/}.}. This is a location for members of the R community---which you can now count yourself a part of---to ask questions, engage in thoughtful dialogue, and much more. While Stack Overflow is committed to all programming languages, of which R is just one of them, the RStudio Community is maintained entirely by R users.

If you have never asked a technical question before I would recommend spending doing so on RStudio Community as Stack Overflow has a \href{https://meta.stackoverflow.com/questions/262791/the-rudeness-on-stack-overflow-is-too-damn-high}{history} of somewhat rude community\footnote{The Rudeness on Stack Overflow is Too Damn High. \url{https://meta.stackoverflow.com/questions/262791/the-rudeness-on-stack-overflow-is-too-damn-high}.}. Before you do so, be sure to create a \href{https://www.tidyverse.org/help/\#reprex}{\textbf{rep}roducible \textbf{ex}ample} so that the community can best help you\footnote{Reproducible Examples. \url{https://www.tidyverse.org/help/\#reprex}.}.

\hypertarget{reminders}{%
\section{Reminders}\label{reminders}}

Learning to program can be exceptionally difficult and frustrating at times. It can be a roller coaster of emotions. It is expected that you will not understand everything the first go around. Do not get down on yourself. I encourage you to take breaks and not push yourself too hard or even be self-critical. I understand you have deadlines, but sometimes it is better if you take a break, eat a healthy snack, go exercise, sleep, be social, or do whatever makes you happy and then come back. You will be much happier and your work will be even better and that I promise you!

If you ever find yourself in a bout of programming induced frustration try one of the below:

\begin{itemize}
\tightlist
\item
  Drink water! Just do this even if you don't feel like it. Water is always good.
\item
  Get some sleep. Without sleep you wil be running at 60\% or less.
\item
  Eat your greens. You are what you eat!
\item
  Shower. Feeling clean can change your perspective and approach.
\item
  Get your blood flowing! Go for a walk. Do some squats or pushups. Making sure your blood is moving is important.
\item
  And take care of yourself!
\end{itemize}

Now, let's get going.

\hypertarget{r-as-a-calculator}{%
\chapter{R as a calculator}\label{r-as-a-calculator}}

Before we get going, let's find our footing. R is a statistical programming language. That means that R does math and pretty well too. In this chapter you'll learn the basics of using R including:

\begin{itemize}
\tightlist
\item
  arithmetic operators
\item
  creating and assigning variables
\item
  using functions
\end{itemize}

\hypertarget{arithmetic-operators}{%
\section{Arithmetic Operators}\label{arithmetic-operators}}

Do you remember PEMDAS? If not, a quick refresher that PEMDAS specifies the order of operations for a math equation. Do the math inside of the parentheses, then the exponents, and then the multiplication or the division before addition or subtraction. We can't write the math out, so we need to type it out. Below are the basic arithmetic operators

\begin{itemize}
\tightlist
\item
  \texttt{\^{}} : exponentiation (exponents) {[}E{]}
\item
  \texttt{*} : multiplication {[}M{]}
\item
  \texttt{/} : division {[}D{]}
\item
  \texttt{+} : addition {[}A{]}
\item
  \texttt{-} : subtraction {[}S{]}
\end{itemize}

These can be used together in parentheses {[}P{]} \texttt{(\ )} to determine the order of operations (\texttt{PEMDAS})

There are three different ways to \href{https://support.rstudio.com/hc/en-us/articles/200484448-Editing-and-Executing-Code\#executing}{execute code} inside of RStudio\footnote{Executing Code. \url{https://support.rstudio.com/hc/en-us/articles/200484448-Editing-and-Executing-Code\#executing}.}. The easiest way is to have your cursor on the line of code that you would like to execute. To execute hold \texttt{command\ +\ enter} (Mac) or \texttt{control\ +\ enter} (PC). Alternatively you can press the \texttt{Run} button at the top of the source page.

Now, try out some mathematic expressions in the console.

\hypertarget{variable-assignment}{%
\section{Variable assignment}\label{variable-assignment}}

I'm sure you recall some basic algebra questions like \(y = 3x - 24\). In this equation, x and y are variables that represents some value. We will often need to create variables to represent some value or set of values. In R, we refer to variables as \textbf{objects}. Objects can be a single number, a set of words, matrixes, and so many other things.

To create an object we need to assign it to some value. Object assignment is done with the assignment operator which looks like \texttt{\textless{}-}. You can automagically insert the assignment operator with \texttt{opt\ +\ -}.

Let's work with the above example. We will solve for \texttt{y} when \texttt{x} is equal to 5.

First, we need to assign 5 to the variable x.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x \textless{}{-}}\StringTok{ }\DecValTok{5}
\end{Highlighting}
\end{Shaded}

If you want to see the contents of an object, you can print it. To print an object you can type the name of it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 5
\end{verbatim}

We can reference the value that \texttt{x} stores in other mathematic expressions. Now what does \texttt{y} equal? Now solve for y in the above equation!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{y \textless{}{-}}\StringTok{ }\DecValTok{3} \OperatorTok{*}\StringTok{ }\NormalTok{x }\OperatorTok{{-}}\StringTok{ }\DecValTok{24}

\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] -9
\end{verbatim}

\hypertarget{functions}{%
\section{Functions}\label{functions}}

Functions are a special kind of R object. Very simply, a function is an object that performs some action and (usually) produces an output. Functions exist to simplify a task. You can identify a function by the parentheses that are appended to the function name. A function looks like \texttt{function\_name()}.

R has many functions that come built in. The collection of functions that come out of the box with R are called *Base R**.

An example of a simple base R function is \texttt{sum()}. \texttt{sum()} takes any number of inputs and calculates the sum of those inputs.

We can run \texttt{sum()} without providing any inputs.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 0
\end{verbatim}

We can provide more inputs (formally called function arguments) to \texttt{sum()}. For example to find the sum of 10 we write

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 10
\end{verbatim}

The sum of a single number is the number itself. We can provide more arguments to \texttt{sum()}. Additional arguments are specified by separating them with commas---e.g.~\texttt{function(argument\_1,\ argument\_2)}.

To find the sum of 10, 3, and 2 we write \texttt{sum(10,\ 3,\ 2)}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\DecValTok{10}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 15
\end{verbatim}

Much of the analysis we will do is done with functions. You will become much more comfortable with them rather quickly.

If you ever need to know how a function works, you can look at its help page by typing \texttt{?function\_name()} in your console. That will bring up the documentation page in the bottom right pane.

\hypertarget{extensions-to-r}{%
\section{Extensions to R}\label{extensions-to-r}}

While R was created as a statistical programming language, it was designed with the intention of being extended to include even more functionality. Extensions to R are called \emph{packages}. R packages often provide a set of functions to accomplish a specific kind of task.

To analyse, manipulate, and visualize our data, we will use a number of different packages to do so. Throughout this book we will become familiar with a set of packages that together are known as the \href{https://tidyverse.org}{Tidyverse}.

R packages do not come installed out of the box. We will need to install them our selves. Base R includes a function called \texttt{install.packages()}. \texttt{install.packages()} will download a specified package from CRAN and install it for us.

To download packages, we must tell \texttt{install.packages()} which package to download. We will provide the name of the package as the only argument to \texttt{install.packages()}. The name of the package needs to be put into quotations such as \texttt{install.packages("package-name")}.

\begin{quote}
Note: By putting text into quotations we are creating what is called a \textbf{character string}.
\end{quote}

\begin{quote}
Reminder: we create objects with the assignment operator \texttt{\textless{}-}.
\end{quote}

When we don't use quotes (create a character string), R thinks we are referring to an object we have created.

\hypertarget{exercise}{%
\subsection{Exercise}\label{exercise}}

Use your new knowlege of functions and installing packages to install the tidyverse.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{install.packages}\NormalTok{(}\StringTok{"tidyverse"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{loading-packages}{%
\section{Loading Packages}\label{loading-packages}}

Now that you have installed the tidyverse, you are going to need to know how to make it available to you for use. To load a package, we use the function \texttt{library()}. Oddly, though, when specifying which package to load, we do not put that name in quotations.

\begin{quote}
Note: It is best practice to load all of your packages at the top of your R script.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## -- Attaching packages ----------------------------------------------------------------- tidyverse 1.3.0 --
\end{verbatim}

\begin{verbatim}
## v ggplot2 3.3.0     v purrr   0.3.3
## v tibble  3.0.0     v dplyr   0.8.5
## v tidyr   1.0.2     v stringr 1.4.0
## v readr   1.3.1     v forcats 0.5.0
\end{verbatim}

\begin{verbatim}
## -- Conflicts -------------------------------------------------------------------- tidyverse_conflicts() --
## x dplyr::filter() masks stats::filter()
## x dplyr::lag()    masks stats::lag()
\end{verbatim}

Notice the message above. When we load the tidyverse, we are actually loading eight packages at once! These are the packages listed under ``\textbf{Attaching packages}.''

You have now successfully installed and loaded the tidyverse. Next, we will begin to learn how to visually analyze data!

\hypertarget{whats-next}{%
\section{What's next?}\label{whats-next}}

Now with this very rudimentary foundation underneath you we will embark upon a very long journey---the journey to learn how to analyze data with R and RStudio. In the following chapters you will learn how to program in R from creating compelling graphics, manipulating data, and performing statistical tests.

In the next chapter we will dive right into working with data. We will begin by learning how to read data into R. Then, we will work through a first visual analysis after which we will focus on skills used in general data manipulation.

\hypertarget{reading-data}{%
\chapter{Reading data}\label{reading-data}}

Requisite to any data analysis is the data. Making those data available for you to analyse is not always the easiest of tasks. In this chapter we will review how data are imported and some of the formats they may take. Once we complete this chapter we will get going on our very first analaysis!

\hypertarget{background}{%
\section{Background}\label{background}}

There are three general sources where we as social scientists will receive or access data: 1) text files, 2) databases, and 3) application programming interfaces (APIs). Frankly, though this is the age of ``big data,'' we are not always able to interface directly with these sources. But through partnership efforts between the public and private we able to share data. For example, BARI's work with the Boston Police Department provides them with annual access to crime data. But BARIs access is limited. They do not have credentials to log in to the database and perform their own queries. What they are usually presented with is a flat text file(s) that contains the data requisite for analysis. And this is what we will focus in this chapter.

Flat text files will be sufficient for 85\% of all of your data needs Now, what do I mean by \emph{flat text file}? A flat text file is a file that stores data in plain text---I know, this seems somewhat confusing. In otherwords, you can open up a text file and actually read the data with your own eyes or a screenreader. For a long while tech pundits believed---and some still do---that text data will be a thing of the past. Perhaps this may be true in the future, but plain text still persists and there are some good reasons for that. Since plain text is extremely simple it is lightweight and usually does not take up that much memory. Also, because there is no fancy embelishing of the data in a plain text file, they can be easily shared from machine to machine without concern of dependent tools and software. Not to mention that we humans can actually be rather hands on and inspect the source of the data ourselves.

\hypertarget{actually-reading-data}{%
\section{Actually Reading Data}\label{actually-reading-data}}

Within the \texttt{tidyverse} there is a package called \href{https://readr.tidyverse.org}{\texttt{readr}} (pronounced \emph{read-r}) which we use for reading in rectangular data from text files.

I just threw the phrase \emph{rectangular data} at you. It is only fair to actually describe what that means. If you were to look at rectangular data in something like excel it would resemble a rectangle. In fancy speak, rectangular data is a \emph{two-dimensional} data structure with rows and columns. We will learn more about the ``proper'' way to shape rectangular data in the ``tidying data'' chapter. For now, all you need to know is that there are rows and columns in rectangular data.

To get started, let us load the tidyverse. This will load readr for us.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse) }
\end{Highlighting}
\end{Shaded}

You most likely have seen and encountered flat text files in the wild inthe form of a \texttt{csv}. It is important to know what \emph{csv} stands for because it will help you understand what it actually is. it stands for \textbf{c}omma \textbf{s}eparated \textbf{v}alues. \_csv\_s are a flat text data file where the data is rectangular! Each new line of the file indicates that there is a new row. Within each row, each comma indicates a new column. If you opened one up in a text editor like text edit or notepad a csv would look something like below.

\begin{verbatim}
column_a, column_b, column_c,
10, "these are words", .432,
1, "and more words", 1.11
\end{verbatim}

To read a csv we use the \texttt{readr::read\_csv()} function. \texttt{read\_csv()} will read in the csv file and create a \texttt{tibble}. A tibble is type of a data structure that we will be interacting with the most throughout this book. A tibble is a rectangular data structure with rows and columns. Since a csv contains rectangular data, it is natural for it to be stored in a tibble.

\begin{quote}
Note: the syntax above is used for referencing a function from a namespace (package name). The syntax is pkgname::function(). This means the \texttt{read\_csv()} function from the package \texttt{readr}. This is something you will see frequently on websites like \href{https://stackoverflow.com/questions/tagged/r}{StackOverflow}.
\end{quote}

Have a look at the arguments of \texttt{read\_csv()} by entering \texttt{?read\_csv()} into the console. You will notice that there are many arguments that you can set. These are there to give you a lot of control over how R will read your data. For now, and most of the time, we do not need to be concerned about these extra arguments. All we need to do is tell R \emph{where} our data file lives. If you haven't deduced from the help page yet, we will supply only the first argument \texttt{file}. This argument is \emph{either a path to a file, a connection, or literal data (either a single string or a raw vector)}.

\begin{quote}
Note: When you see the word \emph{string}, that means values inside of quotations---i.e.~\emph{``this is a string''}.
\end{quote}

We will read in the dataset we will use in the next chapter. These data are stored in the file named \texttt{acs\_edu.csv}. We can try reading this as the file path.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"acs\_edu.csv"}\NormalTok{)}

\CommentTok{\#\# Error: \textquotesingle{}acs\_edu.csv\textquotesingle{} does not exist in current working directory }
\CommentTok{\#\#   (\textquotesingle{}/Users/Josiah/GitHub/urban{-}commons{-}toolkit\textquotesingle{}).}
\end{Highlighting}
\end{Shaded}

Oops. We've got red text and that is never fun. Except, this is a very important error message that, frankly, you will get \textbf{a lot}.

Again it says:

\begin{quote}
\emph{Error: `acs\_edu.csv' \textbf{does not exist} in current \textbf{working directory}}
\end{quote}

I've bolded two portions of this error message. Take a moment to think through what this error is telling you.

For those of you who weren't able to figure it out or just too impatient (like myself): this error is telling us that R looked for the file we provided \texttt{acs\_edu.csv} but it could not find it. This usually means to me that I've either misspelled the file name, or I have not told R to look in the appropriate folder (a.k.a. directory).

\texttt{acs\_edu.csv} actually lives in a directory called \texttt{data}. To tell R---or any computer system, really---where that file is we write \texttt{data/acs\_edu.csv}. This tells R to first enter the \texttt{data} directory and then look for the \texttt{acs\_edu.csv} file.

Now, read the \texttt{acs\_edu.csv} file!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{read\_csv}\NormalTok{(}\DataTypeTok{file =} \StringTok{"data/acs\_edu.csv"}\NormalTok{) }
\CommentTok{\#\textgreater{} Parsed with column specification:}
\CommentTok{\#\textgreater{} cols(}
\CommentTok{\#\textgreater{}   med\_house\_income = col\_double(),}
\CommentTok{\#\textgreater{}   less\_than\_hs = col\_double(),}
\CommentTok{\#\textgreater{}   hs\_grad = col\_double(),}
\CommentTok{\#\textgreater{}   some\_coll = col\_double(),}
\CommentTok{\#\textgreater{}   bach = col\_double(),}
\CommentTok{\#\textgreater{}   white = col\_double(),}
\CommentTok{\#\textgreater{}   black = col\_double()}
\CommentTok{\#\textgreater{} )}
\CommentTok{\#\textgreater{} \# A tibble: 1,456 x 7}
\CommentTok{\#\textgreater{}    med\_house\_income less\_than\_hs hs\_grad some\_coll  bach white   black}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1           105735       0.0252   0.196     0.221 0.325 0.897 0.0122 }
\CommentTok{\#\textgreater{}  2            69625       0.0577   0.253     0.316 0.262 0.885 0.0171 }
\CommentTok{\#\textgreater{}  3            70679       0.0936   0.173     0.273 0.267 0.733 0.0795 }
\CommentTok{\#\textgreater{}  4            74528       0.0843   0.253     0.353 0.231 0.824 0.0306 }
\CommentTok{\#\textgreater{}  5            52885       0.145    0.310     0.283 0.168 0.737 0.0605 }
\CommentTok{\#\textgreater{}  6            64100       0.0946   0.294     0.317 0.192 0.966 0.00256}
\CommentTok{\#\textgreater{}  7            37093       0.253    0.394     0.235 0.101 0.711 0.0770 }
\CommentTok{\#\textgreater{}  8            87750       0.0768   0.187     0.185 0.272 0.759 0.0310 }
\CommentTok{\#\textgreater{}  9            97417       0.0625   0.254     0.227 0.284 0.969 0.00710}
\CommentTok{\#\textgreater{} 10            43384       0.207    0.362     0.262 0.124 0.460 0.105  }
\CommentTok{\#\textgreater{} \# ... with 1,446 more rows}
\end{Highlighting}
\end{Shaded}

This is really good! Except, all that happened was that the function was ran. The data it imported was not saved anywhere which means we will not be able to interact with it. What we saw was the output of the data. In order to interact with the data we need to assign it to an object.

\begin{quote}
Reminder: we assign object with the assignment operator \texttt{\textless{}-}---i.e.~\texttt{new\_obj\ \textless{}-\ read\_csv("file-path.csv")}. Objects are things that we interact with such as a tibble. Functions such as \texttt{read\_csv()} \emph{usually}, but not always, modify or create objects.
\end{quote}

In order to interact with the data, let us store the output into a tibble object called \texttt{acs}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\DataTypeTok{file =} \StringTok{"data/acs\_edu.csv"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

Notice how now there was no data printed in the console. This is a good sign! It means that R read the data and stored it properly into the \texttt{acs} object. When we don't store the function results, the results are (usually) printed out. To print an object, we can just type it's name into the console.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs}
\CommentTok{\#\textgreater{} \# A tibble: 1,456 x 7}
\CommentTok{\#\textgreater{}    med\_house\_income less\_than\_hs hs\_grad some\_coll  bach white   black}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1           105735       0.0252   0.196     0.221 0.325 0.897 0.0122 }
\CommentTok{\#\textgreater{}  2            69625       0.0577   0.253     0.316 0.262 0.885 0.0171 }
\CommentTok{\#\textgreater{}  3            70679       0.0936   0.173     0.273 0.267 0.733 0.0795 }
\CommentTok{\#\textgreater{}  4            74528       0.0843   0.253     0.353 0.231 0.824 0.0306 }
\CommentTok{\#\textgreater{}  5            52885       0.145    0.310     0.283 0.168 0.737 0.0605 }
\CommentTok{\#\textgreater{}  6            64100       0.0946   0.294     0.317 0.192 0.966 0.00256}
\CommentTok{\#\textgreater{}  7            37093       0.253    0.394     0.235 0.101 0.711 0.0770 }
\CommentTok{\#\textgreater{}  8            87750       0.0768   0.187     0.185 0.272 0.759 0.0310 }
\CommentTok{\#\textgreater{}  9            97417       0.0625   0.254     0.227 0.284 0.969 0.00710}
\CommentTok{\#\textgreater{} 10            43384       0.207    0.362     0.262 0.124 0.460 0.105  }
\CommentTok{\#\textgreater{} \# ... with 1,446 more rows}
\end{Highlighting}
\end{Shaded}

This is sometimes a little overwhelming of a view. For previewing data, the function \texttt{dplyr::glimpse()} (there is the namespace notation again) is a great option. Try using the function \texttt{glimpse()} with the first argument being the \texttt{acs} object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(acs)}
\CommentTok{\#\textgreater{} Rows: 1,456}
\CommentTok{\#\textgreater{} Columns: 7}
\CommentTok{\#\textgreater{} $ med\_house\_income \textless{}dbl\textgreater{} 105735, 69625, 70679, 74528, 52885, 64100, 37093, ...}
\CommentTok{\#\textgreater{} $ less\_than\_hs     \textless{}dbl\textgreater{} 0.02515518, 0.05773956, 0.09364548, 0.08426318, 0....}
\CommentTok{\#\textgreater{} $ hs\_grad          \textless{}dbl\textgreater{} 0.19568768, 0.25307125, 0.17332284, 0.25298192, 0....}
\CommentTok{\#\textgreater{} $ some\_coll        \textless{}dbl\textgreater{} 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.2830...}
\CommentTok{\#\textgreater{} $ bach             \textless{}dbl\textgreater{} 0.32473048, 0.26167076, 0.26677159, 0.23124279, 0....}
\CommentTok{\#\textgreater{} $ white            \textless{}dbl\textgreater{} 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.7371...}
\CommentTok{\#\textgreater{} $ black            \textless{}dbl\textgreater{} 0.012213740, 0.017090069, 0.079514240, 0.030640286...}
\end{Highlighting}
\end{Shaded}

\hypertarget{other-common-data-formats}{%
\section{Other common data formats}\label{other-common-data-formats}}

While csv files are going to be the most ubiquitous, you will invariably run into other data formats. The workflow is almost always the same. If you want to read excel files, you can use the function \texttt{readxl::read\_excel()} from the \href{https://readxl.tidyverse.org/}{\texttt{readxl}} package.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs\_xl \textless{}{-}}\StringTok{ }\NormalTok{readxl}\OperatorTok{::}\KeywordTok{read\_excel}\NormalTok{(}\StringTok{"data/acs\_edu.xlsx"}\NormalTok{)}

\KeywordTok{glimpse}\NormalTok{(acs\_xl)}
\CommentTok{\#\textgreater{} Rows: 1,456}
\CommentTok{\#\textgreater{} Columns: 7}
\CommentTok{\#\textgreater{} $ med\_house\_income \textless{}dbl\textgreater{} 105735, 69625, 70679, 74528, 52885, 64100, 37093, ...}
\CommentTok{\#\textgreater{} $ less\_than\_hs     \textless{}dbl\textgreater{} 0.02515518, 0.05773956, 0.09364548, 0.08426318, 0....}
\CommentTok{\#\textgreater{} $ hs\_grad          \textless{}dbl\textgreater{} 0.19568768, 0.25307125, 0.17332284, 0.25298192, 0....}
\CommentTok{\#\textgreater{} $ some\_coll        \textless{}dbl\textgreater{} 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.2830...}
\CommentTok{\#\textgreater{} $ bach             \textless{}dbl\textgreater{} 0.32473048, 0.26167076, 0.26677159, 0.23124279, 0....}
\CommentTok{\#\textgreater{} $ white            \textless{}dbl\textgreater{} 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.7371...}
\CommentTok{\#\textgreater{} $ black            \textless{}dbl\textgreater{} 0.012213740, 0.017090069, 0.079514240, 0.030640286...}
\end{Highlighting}
\end{Shaded}

Another common format is a \textbf{tsv} which stands for \textbf{t}ab \textbf{s}eparated \textbf{f}ormat. \texttt{readr::read\_tsv()} will be able to assist you here.

If for some reason there are special delimiters like \texttt{\textbar{}}, the \texttt{readr::read\_delim()} function will work best. For example \texttt{readr::read\_delim("file-path",\ delim\ =\ "\textbar{}")} would do the trick!

Additionally, another extremely common data type is \emph{json} which is short for javascript object notation. json is a data type that you will usually not read directly from a text file but interact with from an API. If you do happen to encounter a json flat text file, use the \texttt{jsonlite} package. \texttt{jsonlite::read\_json()}.

With this new skill we are ready for our first analysis. In the next chapter we will perform our very first graphical analysis using the package \href{https://ggplot2.tidyverse.org}{\texttt{ggplot2}} from the tidyverse.

\hypertarget{exploratory-visual-analysis}{%
\chapter{Exploratory Visual Analysis}\label{exploratory-visual-analysis}}

In the next part of this book we will introduce to you visual data exploration through the use of the R package \texttt{ggplot2}. You will ask questions of your data, visualize relationships, and draw inferences from the graphics you develop.

The below image from R for Data Science is renowned for its representation of the data analysis workflow. The concept map encompasses the need to get data (import), clean it up (tidy), explore, and finally communicate insights. The box in blue is a representation of EDA. Within EDA we will find ourselves transforming our data---creating new variables, aggregating, etc.---visualizing it, and creating statistical models.

\includegraphics{static/data-science-explore.png}

This chapter will focus on the visualization step of EDA. We have all heard the trope that ``an image is worth a thousand words.'' I'd take a leap and say that a good visualization is worth ten thousand words. An older statistical manual from the National Institute of Standards and Technology (NIST) beautifully lays out the role that visualization plays in EDA.

\begin{quote}
The reason for the heavy reliance on graphics is that by its very nature the main role of EDA is to open-mindedly explore, and graphics gives the analysts unparalleled power to do so, enticing the data to reveal its structural secrets, and being always ready to gain some new, often unsuspected, insight into the data. In combination with the natural pattern-recognition capabilities that we all possess, graphics provides, of course, unparalleled power to carry this out.\footnote{NIST Handbook. \url{https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm}.}
\end{quote}

\href{}{source}

In the following section, you will become acquainted with the graphical R package \texttt{ggplot2} for visual analysis and the American Community Survey. We will walk through the process building a chart from the ground up and drawing inferences from it along the way.

\hypertarget{the-american-community-survey}{%
\section{The American Community Survey}\label{the-american-community-survey}}

For this first data exploration we will work with data from the American Community Survey (ACS). While the ACS is central to Urban Informatics (UI), it does not exhibit the primary characteristic of that we rely upon in UI---namely being naturally occurring. This is a topic we will explore in more depth later. In order to use the ACS data, we must understand what data we are actually working with. The ACS is one of the most fundamental data sets in American social sciences. The ACS is administered by the US Census Bureau but is done so for much different purposes. Article I Section 2 of the US Constitution legislates a decennial census.

\begin{quote}
. . . {[}an{]} enumeration shall be made within three Years after the first Meeting of the Congress of the United States, and within every subsequent Term of ten Years, in such Manner as they shall by Law direct.
\end{quote}

The above clause requires the US government to conduct a complete counting of every single individual in the United States for the purposes of determining how many congressional representatives each state will have. These censuses provided a detailed image of \emph{how many} people there were in the US, but lacked much information beyond that. The first census asked each household for ``the number of free white males under 16 years'' and of ``16 years and upward'', the ``number of free White females'', ``number of other free persons, and the''number of slaves"\footnote{\url{https://www.census.gov/history/www/through_the_decades/index_of_questions/1790_1.html}} Since then the breadth of questions asked during the census has increased as well as other supplementary sources of information.

The ACS was developed in response to two shortcomings of the decennial census. The first being that the census only occurrs every ten years. There was, and still is, a need for more consistent and current data. Not only are the censuses too infrequent, but they also do not provide the most colorful picture of who it is that lives within the US. Local, state, and federal governments desired more context about who their constituents are.

The ACS was developed and first officially released in 2005\footnote{\url{https://www2.census.gov/programs-surveys/acs/methodology/design_and_methodology/acs_design_methodology_report_2014.pdf}}. The ACS uses a ``series of monthly samples'' to ``produce annual estimates for the same small areas (census tracts and block groups) formerly surveyed via the decennial census long-form sample''\footnote{\url{https://www.census.gov/programs-surveys/acs/methodology.html}}. As Catherine Rampell wrote in the New York times

\begin{quote}
``It tells Americans how poor we are, how rich we are, who is suffering, who is thriving, where people work, what kind of training people need to get jobs, what languages people speak, who uses food stamps, who has access to health care, and so on.''\footnote{\url{https://www.nytimes.com/2012/05/20/sunday-review/the-debate-over-the-american-community-survey.html}}
\end{quote}

The impact of the ACS are wide stretching from funding to social research.

\hypertarget{understanding-acs-estimates}{%
\subsection{Understanding ACS Estimates}\label{understanding-acs-estimates}}

Continuous sampling done by the US Census Bureau occurs at a monthly basis and are used to produce annual estimates\footnote{\url{https://www2.census.gov/programs-surveys/acs/methodology/design_and_methodology/acs_design_methodology_report_2014.pdf}}. There are two different types of estimates one can retrieve from the ACS. These are the 1-year and 5-year estimates. Each kind of estimate serves a different purpose.

When choosing between 1-year and 5-year estimates we are making a tradeoff. 1-year estimates provide us with the most current data possible at the expense of a smaller sample size. This means that the estimates are not as reliable as the 5-year estimates which are collected over a period of 60 months. On the other hand, when we consider 5-year estimates, we benefit from a large sample size and increased reliability, but we lose the ability to make statements about a single year.

In the cases where 5-year estimates are used researchers are analyzing populations and rates derived from five years of data collection. This requires you, the researcher, to qualify this with a statement to the effect of \emph{``the rate of unemployment in 2014-2018 was 4\%''}\footnote{\url{https://www.census.gov/content/dam/Census/library/publications/2018/acs/acs_general_handbook_2018_ch03.pdf}}. Because of this, you are unable to use consecutive 5-year estimates to analyze annual trends. In the case that you need to analyse annual trends 1-year estimates are recommended.

There is also another important tradeoff one must consider when using ACS data and that is of unit of analysis. The census and ACS are conducted at the household level. However, estimates are provided for geographic areas. These geographic areas have a hierarchy going from block groups at the smallest level geography, to census tracts, and to counties. Beyond counties are geographies at the state level and even larger. Urban informatics is inherently focuses on a more micro---arguably meso---unit of analysis.

The following analysis is done using the Massachussetts census indicators published from BARI. The dataset is based on 5-year estimates from the ACS at the tract level.

\hypertarget{a-first-visualization}{%
\section{A first visualization}\label{a-first-visualization}}

For your first introduction to R, we will explore the relationship between education and income in Massachusetts.

\textbf{NOTE}: this needs to be updated once I make \texttt{uitk} a package with R objects

\hypertarget{familiarize-yourself}{%
\subsection{Familiarize yourself}\label{familiarize-yourself}}

There is no one best way to begin an exploratory analysis to guarantee interesting outcomes. But before one begins their EDA, they must know what their data actually contain. We will use the \texttt{read\_csv()} function from the last chapter to read in the \texttt{acs\_edu} dataset. Save it to a variable called \texttt{acs\_edu}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}
\NormalTok{acs\_edu \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/acs\_edu.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\texttt{acs\_edu} contains data demographic information about every census tract in Massachusetts. Print \texttt{acs\_edu} to the console. What do you see?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs\_edu}
\CommentTok{\#\textgreater{} \# A tibble: 1,456 x 7}
\CommentTok{\#\textgreater{}    med\_house\_income less\_than\_hs hs\_grad some\_coll  bach white   black}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1           105735       0.0252   0.196     0.221 0.325 0.897 0.0122 }
\CommentTok{\#\textgreater{}  2            69625       0.0577   0.253     0.316 0.262 0.885 0.0171 }
\CommentTok{\#\textgreater{}  3            70679       0.0936   0.173     0.273 0.267 0.733 0.0795 }
\CommentTok{\#\textgreater{}  4            74528       0.0843   0.253     0.353 0.231 0.824 0.0306 }
\CommentTok{\#\textgreater{}  5            52885       0.145    0.310     0.283 0.168 0.737 0.0605 }
\CommentTok{\#\textgreater{}  6            64100       0.0946   0.294     0.317 0.192 0.966 0.00256}
\CommentTok{\#\textgreater{}  7            37093       0.253    0.394     0.235 0.101 0.711 0.0770 }
\CommentTok{\#\textgreater{}  8            87750       0.0768   0.187     0.185 0.272 0.759 0.0310 }
\CommentTok{\#\textgreater{}  9            97417       0.0625   0.254     0.227 0.284 0.969 0.00710}
\CommentTok{\#\textgreater{} 10            43384       0.207    0.362     0.262 0.124 0.460 0.105  }
\CommentTok{\#\textgreater{} \# ... with 1,446 more rows}
\end{Highlighting}
\end{Shaded}

\texttt{\#\#\ \#\ A\ tibble:\ 1,456\ x\ 7} is printed out at the top followed by column names, their types---e.g.~\texttt{\textless{}dbl\textgreater{}}---their respective values and, to the far left we see the numbers 1 through 10 before each row of values.

Let us dissect \texttt{\#\ A\ tibble:\ 1,456\ x\ 7} a little bit more. This alone is quite informative. It tells us that the type of object we are working with is a \texttt{tibble} with 1,456 rows and 7 columns.

A tibble is a method of representing rectangular data and is very similar to a table one may create within Excel with rows an columns. When working with tibbles we try to adhere to what are called the principles of tidy data\footnote{\url{https://vita.had.co.nz/papers/tidy-data.pdf}}. There are three key principles that we ought to keep in mind when working with rectangular data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Each variable forms a column.
\item
  Each observation forms a row.
\item
  Each value represents a combination of an observation and a variable.
\end{enumerate}

There can often be confusion about what should be a variable and what is to be an observation. In \emph{Tidy Data} Hadley Wickham write that

\begin{quote}
``A variable contains all values that measure the same underlying attribute (like height, temperature, duration) across units. An observation contains all values measured on the same unit (like a person, or a day, or a race) across attributes.''\footnote{\url{https://vita.had.co.nz/papers/tidy-data.pdf}}
\end{quote}

Say we have a tibble of survey respondents. In this case each row should be a respondent and each column should be a variable that is associated with that respondent. This could be something such as age, birth date, or the respondents response to a survey question.

In the case of our \texttt{acs\_edu} tibble, our unit of observation, aka row, is a census tract. Each variable measures a different characteristic of a census tract. For example, the column \texttt{med\_house\_income} is an estimate of the median household income of a given census tract. The other columns indicate what proportion of a population meets some criteria.

How does one know what criteria their columns represent? This brings us to the importance of column names. Column names ought to be descriptors of their corresponding variables. This is a surprsingly difficult task! In \texttt{acs\_edu} we can infer---though we should always have documentation to supplement the data---that the variables measure income, educational attainment rates, and race.

\hypertarget{form-a-question}{%
\subsection{Form a question}\label{form-a-question}}

Once you have familiarized yourself with the data that you will be working with, you can begin to form a question that can be feasibly be explored or answered with the present data. The importance of domain expertise in EDA cannot be understated. Without an understanding of what underlying phenomena your data are measuring it will be extremely difficult to come to meaningful insights.

My background is in sociology. Within sociology, and specifically social stratification, it is believed that more education leads to more social prestiege, economic stability, and is more readily accessible by the white population. Given this background and the data available in \texttt{acs\_edu}, we will explore the relationship between education and income. We will try to answer the question \emph{what is the relationship between education and income?} We will first look at each variable in isolation and then try and identify any relationship that may exist between the two variables.

\hypertarget{building-a-graph}{%
\subsection{Building a graph}\label{building-a-graph}}

To create our visualizations we will use the package \href{https://ggplot2.tidyverse.org/}{\texttt{ggplot2}} from the tidyverse. Before we can begin, we need to make sure that the collection of functions from ggplot2 are available to us with the \texttt{library()} function.

\begin{quote}
Reminder: \texttt{library(pkg\_name)} loads a package into your workspace and makes the functions and objects it exports available to you.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(ggplot2)}
\end{Highlighting}
\end{Shaded}

To begin building a ggplot, we use the function \texttt{ggplot()}. There are two function arguments these being \texttt{data} and the aesthetics \texttt{mapping}. The \texttt{data} is the tibble that we wish to visualize. In this case we want to visualize the data from \texttt{acs\_edu}.

We will begin constructing our first visualization with the \texttt{ggplot()} \textbf{function} using the \texttt{acs\_edu} \textbf{object}.

\begin{quote}
Reminder: Functions are characterised by the parentheses at the end of them. Functions do things whereas objects hold information.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu)}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-5-1.pdf}

Notice that this plot is entirely empty. This is because we have not defined what it is that we want to visualize. ggplot uses what is called a grammar of graphics (this is expanded upon in depth in the \emph{Visualizing Trends and Relationships} chapter) which requires us to sequentially build our graphs by first defining what data and variables will be visualized and then adding layers to the plot.

The next step we need to take is to define which columns we want to visualize. These are called the \emph{aesthetics} and they are defined using the \texttt{aes()} function which is supplied to the \texttt{mapping} argument. The purpose of \texttt{aes()} is to tell ggplot which columns are mapped to what. The most important and fundamental of these are the \texttt{x} and \texttt{y} arguments. These refer to the x and y axes in the chart that we will begin to make.

Before we begin to analyze the relationship between \texttt{med\_house\_income} and \texttt{bach} (bachelor's degree attainment rate), we ought to do our due diligence of looking at the distribution of each of these first. Let us start with the \texttt{med\_house\_income} column. When exploring only a single variable, we want to supply that to the \texttt{x} argument of \texttt{aes()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ med\_house\_income))}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-6-1.pdf}

Alright, we are making progress. We can see that the x axis is now filled out a bit more. The axis breaks have been labeled as has the axis itself. In order to see the data in a graphical representation, we need to determine how we want to see the data and what sort of geometry will be used to visualize it.

To \textbf{add} geometry to our ggplot, we use the plus sign \texttt{+} which signifies that we are adding a layer on top of the basic graph. There are many ways we can visualize univariate data but the histogram has stood the test of time. To create a histogram we \textbf{add} the \texttt{geom\_histogram()} layer to our existing ggplot code.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ med\_house\_income)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-7-1.pdf}

\begin{quote}
Note: To ensure that our code is legible we add each new layer on a line. R will manage the indentation for you. Code readibility is very important and you will thank yourself later for instilling good practices from the start.
\end{quote}

This histogram illustrates the distribution of median household income in the state of Massachusetts. The median value seems to sit somewhere around \$75k with a few outliers near \$250k as well demonstrating a right skew.

\begin{quote}
Reminder: The skew is where there are few {[}observations{]}.
\end{quote}

Usually when we look at distributions of wealth they are extremely right skewed meaning there are a few people who make an outrageous amount of money. What is interesting is that this histogram is rather normally distributed almost challenging intuition. This is because the ACS does something called top-coding. Top-coding is the practice of creating a ceiling value. For example, if there is a tract has a median household income of \$1m, that will be reduced to the top-coded value---what appears to be \$250k. This creates what are called censored data.

\begin{quote}
\textbf{Censored data}: data ``in which the value of a measurement or observation is only partially known.'' \footnote{\url{https://en.wikipedia.org/wiki/Censoring_(statistics)}}\footnote{\url{https://stats.stackexchange.com/questions/49443/how-to-model-this-odd-shaped-distribution-almost-a-reverse-j}}
\end{quote}

Let us now create a histogram of our second variable of interest, \texttt{bach}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bach)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-8-1.pdf}

This histogram illustrates the distribution of the bachelor degree attainment rate (the proportion of people with a bachelor's degree) across census tracts in Massachusetts. Because we did our homework ahead of time, we know that the national attainment rate in 2018 for people over 25 was \textasciitilde35\%\footnote{\url{https://www.census.gov/data/tables/2018/demo/education-attainment/cps-detailed-tables.html}}. Our histogram shows that within MA there is a lot of variation in the attainment rate from a low of about 0\% to a high of over 60\%. There is not a steep peak in the distribution which tells us that there is a fair amount of variation in the distribution.

Now that there is an intuition of the distribution and characteristics of both \texttt{med\_house\_income} and \texttt{bach}, we can begin to try and answer the question \emph{what is the effect of education on median household income?} The phrasing of our question will determine how we visualize our data.

When stating research questions we often phrase it as \emph{what is the effect of x \textbf{on} y}? In this formulation we are determining that \texttt{bach}, our independent variable, will be plotted on the x axis and \texttt{med\_house\_income} will be plotted on the y axis. To visualize this bivariate relationship we will create a scatter plot.

\begin{quote}
This structure and phrasing is useful for continuity in verbal communication, graphical representation, and hypothesis testing.
\end{quote}

We can visualize this relationship by adding additional mapped aesthetics. In this case, we will map both the x and y arguments of the \texttt{aes()} function. Rather than adding histogram layer, we will need to create a scatter plot. Scatter plots are created by plotting points for each (x, y) pair. To get such an effect we will use the \texttt{geom\_point()} layer.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bach, }\DataTypeTok{y =}\NormalTok{ med\_house\_income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-9-1.pdf}

The above scatter plot provides a lot of information. We see that there is a positive linear trend---that is that when the \texttt{bach} value increases so does the \texttt{med\_house\_income} variable. When looking at a scatter plot we are looking to see if there is a consistent pattern that can be sussed out.

In this scatterplot we can see that there is a linear pattern. When the points on the scatter plot are closer to eachother and demonstrate less spread, that means there is a stronger relationship between the two variables. Imagine if we drew a line through the middle of the points, we would want each point to be as close to that line as possible. The further the point is away from that line, the more variation there is. In these cases we often create linear regression models to estimate the relationship.

Using ggplot, we can plot the estimated linear regression line on top of our scatterplot. This is done with a \texttt{geom\_smooth()} layer. By default, \texttt{geom\_smooth()} does not plot the linear relationship. To do that, we need to specify what kind of smoothing we would like. To plot the estimated linear model, we set \texttt{method\ =\ "lm"}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bach, }\DataTypeTok{y =}\NormalTok{ med\_house\_income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-10-1.pdf}

Wonderful! To finish up this graphic, we should add informative labels. Labels live in their own layer which is created with \texttt{labs()}. Each argument maps to an aesthetic---e.g.~\texttt{x} and \texttt{y}. By default ggplot uses the column names for axis labels, but these labels are usually uninformative.

Let's give the plot a title and better labels for its axes. We will set the following arguments to \texttt{labs()}

\begin{itemize}
\tightlist
\item
  \texttt{x\ =\ "\%\ of\ population\ with\ a\ Bachelor\textquotesingle{}s\ Degree"}
\item
  \texttt{y\ =\ "Median\ Household\ Income"}
\item
  \texttt{title\ =\ "Relationship\ between\ Education\ and\ Income"}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bach, }\DataTypeTok{y =}\NormalTok{ med\_house\_income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"\% of population with a Bachelor\textquotesingle{}s Degree"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Median Household Income"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Relationship between Education and Income"}\NormalTok{)}
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-11-1.pdf}

\begin{quote}
Note that each argument is placed on a new line. Again, this is to improve readability.
\end{quote}

What can we determine from this graph? Take a few minutes and write down what you see and what you can infer from that.

\begin{quote}
Consider asking these questions:

\begin{itemize}
\tightlist
\item
  Is there a relationship between our variables?
\item
  Is the relationship linear?
\item
  Which direction does the trend go?
\item
  How spread out are the value pairs?
\item
  Are there any outliers?
\end{itemize}
\end{quote}

This chart ilicits further lines of inquiry. For example, in the sociological literature there is a well documented achievement gap. The achievement gap can be traced along racial lines---though it is not inherently \emph{caused} by race but rather intertwined with it. Can this be seen in the tracts of Massachusetts?

We can visualize a third variable on our chart by mapping another \texttt{aes}thetic. When we add a third variable to the visualization we are generally trying to illustrate group membership or size / magnitude of the third variable. Due to the large number of points on our chart already, we may benefit more from mapping color rather than size---imagine 1,000 points overlapping even more than they already do.

We can map the proportion of the population that is white to the color of our points. We do this by setting the \texttt{color} aesthetic to \texttt{white}. While we're at it, let us include a subtitle which is informative to the viewer.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_edu, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{x =}\NormalTok{ bach, }\DataTypeTok{y =}\NormalTok{ med\_house\_income, }\DataTypeTok{color =}\NormalTok{ white)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.4}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_smooth}\NormalTok{(}\DataTypeTok{method =} \StringTok{"lm"}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{x =} \StringTok{"\% of population with a Bachelor\textquotesingle{}s Degree"}\NormalTok{,}
       \DataTypeTok{y =} \StringTok{"Median Household Income"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Relationship between Education and Income"}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"Colored by whiteness"}\NormalTok{) }
\CommentTok{\#\textgreater{} \textasciigrave{}geom\_smooth()\textasciigrave{} using formula \textquotesingle{}y \textasciitilde{} x\textquotesingle{}}
\end{Highlighting}
\end{Shaded}

\includegraphics{01e-visual-analysis_files/figure-latex/unnamed-chunk-12-1.pdf}

What can we conclude now? Does the addition of the third variable increase or decrease the utility of our scatter plot? Does the trend seem to mediated by race? I'll leave those questions to you to answer.

You've now completed your first visual analysis. You've learned how to create publication ready histograms and scatter plots using ggplot2. This is no small feat!

This chapter provided you with data that was used in our visualization exercises. You're going to want to be able to visualize and analyze your own data. The next chapter introduces you reading data and some of the most common file formats you may encounter.

\hypertarget{general-data-manipulation}{%
\chapter{General data manipulation}\label{general-data-manipulation}}

We spent the last chapter performing our first exploratory visual analysis. From our visualizations we were able to inductively conlcude that as both median household income and the proportion of the population with a bachelors degree increases, so does the share of the population is white.

While we were able to make wonderful visualizations, we did skip a number of steps in the exploratory analysis process! Arguably the most important is that we skipped the curating of our dataset. The ACS dataset was already cleaned and curated for you. This almost always will not be the case. As such, we're going to spend this chapter learning about ways of selecting subsets of our data.

Now that you have the ability to read in data, it is important that you get comfortable handling it. Some people call the process of rearranging, cleaning, and reshaping data massaging, plumbing, engineering, and myriad other names. Here, we will refer to this as data manipulation. This is a preferable catch-all term that does not illicit images of toilets or Phoebe Buffay (she was a masseuse!).

You may have heard of the 80/20 rule, or at least one of the many 80/20 rules. The 80/20 rule I'm referring to, is the idea that data scientists will spend 80\% or more of their time cleaning and manipulating their data. The other 20\% is the analysis part---creating statistics and models. I mention this because working with data is \emph{mostly} data manipulation and only \emph{some} statistics. Be prepared to get your hands dirty with data---euphemisms like this is probably why the phrase \emph{data plumbing} ever came about.

In this chapter we will learn how to preview data, select columns, arrange rows, and filter rows. This is where we get hands on with our data. This is just about as physical as we will be able to get unless you want to open up the raw csv and edit the data there!

\begin{quote}
Note: I do not recommend editing any csvs directly. That statement was in jest.
\end{quote}

This is all to say that you will find yourself with messy, unsanitized, gross, not fun to look at data most of the time. Because of this, it is really important that we have the skills to clean our data. Right now we're going to go over the foundational skills we will learn how to select columns and rows, filter data, and create new columns, and arrange our data. To do this, we will be using the \href{https://dplyr.tidyverse.org}{\texttt{dplyr}} package from the tidyverse.

The data we read in in the last chapter was only a select few variables from the annual census data release that the team at the Boston Area Research Initiative (BARI) provides. These census indicators are used to provide a picture into the changing landscape of Boston and Massachusetts more generally. In this chapter we will work through a rather real life scenario that you may very well encounter using the BARI data.

\hypertarget{scenario}{%
\section{Scenario}\label{scenario}}

A local non-profit is interested in the commuting behavior of Greater Boston residents. Your adviser suggested that you assist the non-profit in their work. You've just had coffee with the project manager to learn more about what their specific research question is. It seems that the extension of the Green Line is of great interest to them. She spoke at length about the history of the Big Dig and its impact on commuters working in the city. This poured over into a conversation about the spatial and social stratification of a city. She looks at her watch and realizes she's about to miss the commuter train home. You shake their hand, thank them for their time (and the free coffee because you're a grad student), and affirm that you will email them in a week or so with some data for them to work with.

You're back at your apartment, french press next your laptop---though not too close---notes open, and ready to begin. You pore over your notes and realize while you now have a rather good understanding of \emph{what} the Green Line Extensions is and the impact that the Big Dig had, you really have no idea what about commuting behavior in Greater Boston they are interested in. You realize you did not even confirm what constitutes the Greater Boston area. You push down the coffee grinds and pour your first cup of coffee. This will take at least two cups of coffee.

The above scenario sounds like something out of a stress dream. This is scenario that I have found myself in many times and I am sure that you will find yourself in at one point as well. The more comfortable you get with data analysis and asking good questions, the more guided and directed you can make these seemingly vague objectives.

\begin{quote}
At the end of this chapter, we will expound upon ways to prevent this in the future.
\end{quote}

\hypertarget{getting-physical-with-the-data}{%
\section{Getting physical with the data}\label{getting-physical-with-the-data}}

The data we used in both chapters one and two were curated from the \href{https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/XZXAUP}{annual census indicator release} from BARI{[}\^{}indicators{]}. This is the dataset from which \texttt{acs\_edu} was created. We will use these data to provide relevant data relating to commuting in the Greater Boston Area. The first thing you'll notice is that these data are large and somewhat unforgiving to work with. What a better way to get started than with big data?

We will be using the tidyverse to read in and manipulate our data (as we did last chapter). Recall that we will load the tidyverse using \texttt{library(tidyverse)}.

\begin{quote}
Refresher: the tidyverse is a collection of packages used for data analysis. When you load the tidyverse it loads \texttt{readr}, \texttt{ggplot2}, and \texttt{dplyr} for us, among other packages. For now, though, these are the only relevant packages.
\end{quote}

\textbf{Try it:}

\begin{itemize}
\tightlist
\item
  Load the tidyverse
\item
  Read in the file \texttt{ACS\_1317\_TRACT.csv} located in the \texttt{data} directory, store it in an object called \texttt{acs\_raw}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{acs\_raw \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/ACS\_1317\_TRACT.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Wonderful! You've got the hang of reading data in which is truly no small feat. Once we have the data accessible from R, it is important to get familiar with what the data are. This means we need to know \emph{which} variables are available to us and get a feel for what the values in those variables represent.

Try printing out the \texttt{acs\_raw} object in your console.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs\_raw}
\end{Highlighting}
\end{Shaded}

Oof, yikes. It's a bit messy! Not to mention that R did not even print out all of the columns. That's because it ran out of room. When we're working with wide data (many columns), it's generally best to view only a preview of the data. The function \texttt{dplyr::glimpse()} can help us do just that. Provide \texttt{acs\_raw} as the only argument to \texttt{glimpse()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(acs\_raw)}
\end{Highlighting}
\end{Shaded}

Much better, right? It is frankly still a lot of text, but the way it is presented is rather useful. Each variable is written followed by its data type, i.e.~\texttt{\textless{}dbl\textgreater{}}, and then a preview of values in that column. If the \texttt{\textless{}dbl\textgreater{}} does not make sense yet, do not worry. We will go over data types in depth later. Data types are not the most fun and I think it is important we have fun!

\texttt{acs\_raw} is the dataset from which \texttt{acs\_edu} was created. As you can see, there are many, many, \emph{many} different variables that the ACS data provide us with. These are only the tip of the iceberg.
\textbf{Pause}

Now have a think.

Looking at the preview of these data, which columns do you think will be most useful to the non-profit for understanding commuter behavior?

\begin{quote}
Note: ``all of them'' is not always the best answer. By providing too much data one may be moved to inaction because they now must determine what variables are the most useful and how to use them.
\end{quote}

If you spotted the columns commute\_less10, commute1030, commute3060, commute6090, and commute\_over90, your eyes and intuition have served you well! These variables tell us about what proportion of the sampled population in a given census tract have commute times that fall within the indicated duration range, i.e.~30-60.

\hypertarget{selecting}{%
\subsection{\texorpdfstring{\texttt{select()}ing}{select()ing}}\label{selecting}}

So now we have an intuition of the most important variables, but the next problem soon arises: how do we isolate just these variables? Whenever you find yourself needing to select or deselect columns from a tibble \texttt{dplyr::select()} will be the main function that you will go to.

\textbf{What does it do?}: \texttt{select()} selects variables from a tibble and returns another tibble.

Before we work through how to use \texttt{select()}, refer to the help documentation and see if you can get somewhat of an intuition by typing \texttt{?select()} into the console. Once you press enter the documentation page should pop up in RStudio.

There are a few reasons why I am directing you towards the function documentation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  To get you comfortable with navigating the RStudio IDE
\item
  Expose you to the R vocabulary
\item
  Soon you'll be too advanced for this book and will have to figure out the way functions work on your own!
\end{enumerate}

Perhaps the help documentation was a little overwhelming and absolutely confusing. That's okay. It's just an exposure! With each exposure things will make more sense. Let's tackle these arguments one by one.

\begin{quote}
\texttt{.data}: A tbl. All main verbs are S3 generics and provide methods for tbl\_df(), dtplyr::tbl\_dt() and dbplyr::tbl\_dbi().
\end{quote}

What I want you to take away from this argument definition is \texttt{a\ tbl}. Whenever you read \texttt{tbl} think to your self ``oh, that is just a \texttt{tibble}.'' If you recall, when we read rectangular data with \texttt{readr::read\_csv()} or any other \texttt{readr::read\_*()} function we will end up with a tibble. To verify that this is the case, we can double check our objects using the function \texttt{is.tbl()}. This function takes an object and returns a logical value (\texttt{TRUE} or \texttt{FALSE}) if the statement is true. Let's double check that \texttt{acs\_raw} is in fact a tbl.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.tbl}\NormalTok{(acs\_raw)}
\CommentTok{\#\textgreater{} [1] TRUE}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Aside: Each object type usually has a function that let's you test if objects are that type that follow the structure \texttt{is.obj\_type()} or the occasional \texttt{is\_obj\_type()}. We will go over object types more later.
\end{quote}

We can read the above as if we are asking R the question ``is this object a \texttt{tbl}?'' The resultant output of \texttt{is.tbl(acs\_raw)} is \texttt{TRUE}. Now we can be doubly confident that this object can be used with \texttt{select()}.

The second argument to \texttt{select()} is a little bit more difficult to grasp, so don't feel discouraged if this isn't clicking right away. There is \emph{a lot} written in this argument definition and I feel that not all of it is necessary to understand from the get go.

\begin{quote}
\texttt{...}: One or more unquoted expressions separated by commas. You can treat variable names like they are positions, so you can use expressions like x:y to select ranges of variables.
\end{quote}

\texttt{...}, referred to as ``dots'' means that we can pass any number of arguments to the function. Translating ``one or more unquoted expressions separated by commas'' into regular person speak reiterates that there can be multiple other arguments passed into \texttt{select()}. ``Unquoted expressions'' means that if we want to select a column we do not put that column name in quotes.

``You can treat variable names like they are positions'' translates to ``if you want the first column you can write the number \texttt{1} etc.'' and because of this, if you want the first through tenth variable you can pass \texttt{1:10} as an argument to \texttt{...}.

The most important thing about \texttt{...} is that we \textbf{do not} assign \texttt{...} as an argument, for example . \texttt{...\ =\ column\_a} is \textbf{not} the correct notation. We provide \texttt{column\_a} alone.

As always, this makes more sense once we see it in practice. We will now go over the many ways in which we can select columns using \texttt{select()}. Once we have gotten the hang of selecting columns we will return back to assisting our non-profit.

We will go over:

\begin{itemize}
\tightlist
\item
  selecting by name
\item
  selecting by position
\item
  select helpers
\end{itemize}

\hypertarget{selecting-exercises}{%
\subsection{\texorpdfstring{\texttt{select()}ing exercises}{select()ing exercises}}\label{selecting-exercises}}

\texttt{select()} enables us to choose columns from a tibble based on their names. But remember that these will be unquoted column names.

Try it:

\begin{itemize}
\tightlist
\item
  select the column \texttt{name} from \texttt{acs\_raw}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, name)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 1}
\CommentTok{\#\textgreater{}    name                                                 }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                                                }
\CommentTok{\#\textgreater{}  1 Census Tract 7281, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  2 Census Tract 7292, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  3 Census Tract 7307, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  4 Census Tract 7442, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  5 Census Tract 7097.01, Worcester County, Massachusetts}
\CommentTok{\#\textgreater{}  6 Census Tract 7351, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  7 Census Tract 7543, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  8 Census Tract 7308.02, Worcester County, Massachusetts}
\CommentTok{\#\textgreater{}  9 Census Tract 7171, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{} 10 Census Tract 7326, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

The column \texttt{name} was passed to \texttt{...}. Recall that dots allows us to pass ``one ore more unquoted expressions separated by commas.'' To test this statement out, select \texttt{town} in addition to \texttt{name} from \texttt{acs\_raw}

Try it:

\begin{itemize}
\tightlist
\item
  select \texttt{name} and \texttt{town} from \texttt{acs\_raw}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, name, town)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 2}
\CommentTok{\#\textgreater{}    name                                                  town         }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                                                 \textless{}chr\textgreater{}        }
\CommentTok{\#\textgreater{}  1 Census Tract 7281, Worcester County, Massachusetts    HOLDEN       }
\CommentTok{\#\textgreater{}  2 Census Tract 7292, Worcester County, Massachusetts    WEST BOYLSTON}
\CommentTok{\#\textgreater{}  3 Census Tract 7307, Worcester County, Massachusetts    WORCESTER    }
\CommentTok{\#\textgreater{}  4 Census Tract 7442, Worcester County, Massachusetts    MILFORD      }
\CommentTok{\#\textgreater{}  5 Census Tract 7097.01, Worcester County, Massachusetts LEOMINSTER   }
\CommentTok{\#\textgreater{}  6 Census Tract 7351, Worcester County, Massachusetts    LEICESTER    }
\CommentTok{\#\textgreater{}  7 Census Tract 7543, Worcester County, Massachusetts    WEBSTER      }
\CommentTok{\#\textgreater{}  8 Census Tract 7308.02, Worcester County, Massachusetts WORCESTER    }
\CommentTok{\#\textgreater{}  9 Census Tract 7171, Worcester County, Massachusetts    BERLIN       }
\CommentTok{\#\textgreater{} 10 Census Tract 7326, Worcester County, Massachusetts    WORCESTER    }
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

Great, you're getting the hang of it.

Now, in addition to to selecting columns solely based on their names, we can also select a range of columns using the format \texttt{col\_from:col\_to}. In writing this \texttt{select()} will register that you want every column from and including \texttt{col\_from} up until and including \texttt{col\_to}.

Let's refresh ourselves with what our data look like:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(acs\_raw)}
\end{Highlighting}
\end{Shaded}

Try it:

\begin{itemize}
\tightlist
\item
  select the columns \texttt{age\_u18} through \texttt{age\_o65}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, age\_u18}\OperatorTok{:}\NormalTok{age\_o65)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 4}
\CommentTok{\#\textgreater{}    age\_u18 age1834 age3564 age\_o65}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1   0.234   0.202   0.398   0.166}
\CommentTok{\#\textgreater{}  2   0.181   0.151   0.461   0.207}
\CommentTok{\#\textgreater{}  3   0.171   0.214   0.437   0.178}
\CommentTok{\#\textgreater{}  4   0.203   0.227   0.436   0.133}
\CommentTok{\#\textgreater{}  5   0.177   0.203   0.430   0.190}
\CommentTok{\#\textgreater{}  6   0.163   0.237   0.439   0.162}
\CommentTok{\#\textgreater{}  7   0.191   0.326   0.380   0.102}
\CommentTok{\#\textgreater{}  8   0.202   0.183   0.466   0.148}
\CommentTok{\#\textgreater{}  9   0.188   0.150   0.462   0.200}
\CommentTok{\#\textgreater{} 10   0.244   0.286   0.342   0.128}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

Now to really throw you off! You can even reverse the order of these ranges.

Try it:

\begin{itemize}
\tightlist
\item
  select columns from \texttt{age\_o65} to \texttt{age\_u18}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, age\_o65}\OperatorTok{:}\NormalTok{age\_u18)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 4}
\CommentTok{\#\textgreater{}    age\_o65 age3564 age1834 age\_u18}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1   0.166   0.398   0.202   0.234}
\CommentTok{\#\textgreater{}  2   0.207   0.461   0.151   0.181}
\CommentTok{\#\textgreater{}  3   0.178   0.437   0.214   0.171}
\CommentTok{\#\textgreater{}  4   0.133   0.436   0.227   0.203}
\CommentTok{\#\textgreater{}  5   0.190   0.430   0.203   0.177}
\CommentTok{\#\textgreater{}  6   0.162   0.439   0.237   0.163}
\CommentTok{\#\textgreater{}  7   0.102   0.380   0.326   0.191}
\CommentTok{\#\textgreater{}  8   0.148   0.466   0.183   0.202}
\CommentTok{\#\textgreater{}  9   0.200   0.462   0.150   0.188}
\CommentTok{\#\textgreater{} 10   0.128   0.342   0.286   0.244}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

Alright, so now we have gotten the hang of selecting columns based on their names. But equally important is the ability to select columns based on their position. Consider the situation in which you regularly receive georeferenced data from a research partner and the structure of the dataset is rather consistent \emph{except} that they frequently change the name of the coordinate columns. Sometimes the columns are \texttt{x} and \texttt{y}. Sometimes they are capitalized \texttt{X} and \texttt{Y}, \texttt{lon} and \texttt{lat}, or even \texttt{long} and \texttt{lat}. It eats you up inside! But you know that while the names may change, their positiions never do---they're always the last two columns. You decide to program a solution rather than having a conversation with your research partner---though, I recommend you both level set on reproducibility standards.

\begin{quote}
``\ldots You can treat variable names like they are positions\ldots{}''
\end{quote}

The above was taken from the argument definition of dots \texttt{...}. Like providing the name of the column, we can also provide their positions (also referred to as an index). In our previous example, we selected the \texttt{name} column. We can select this column by it's position too. \texttt{name} is the second column in our tibble. We select it by position like so:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\DecValTok{2}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 1}
\CommentTok{\#\textgreater{}    name                                                 }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}                                                }
\CommentTok{\#\textgreater{}  1 Census Tract 7281, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  2 Census Tract 7292, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  3 Census Tract 7307, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  4 Census Tract 7442, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  5 Census Tract 7097.01, Worcester County, Massachusetts}
\CommentTok{\#\textgreater{}  6 Census Tract 7351, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  7 Census Tract 7543, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{}  8 Census Tract 7308.02, Worcester County, Massachusetts}
\CommentTok{\#\textgreater{}  9 Census Tract 7171, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{} 10 Census Tract 7326, Worcester County, Massachusetts   }
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

Try it:

\begin{itemize}
\tightlist
\item
  select \texttt{age\_u18} and \texttt{age\_o65} by their position
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\DecValTok{6}\NormalTok{, }\DecValTok{9}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 2}
\CommentTok{\#\textgreater{}    age\_u18 age\_o65}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1   0.234   0.166}
\CommentTok{\#\textgreater{}  2   0.181   0.207}
\CommentTok{\#\textgreater{}  3   0.171   0.178}
\CommentTok{\#\textgreater{}  4   0.203   0.133}
\CommentTok{\#\textgreater{}  5   0.177   0.190}
\CommentTok{\#\textgreater{}  6   0.163   0.162}
\CommentTok{\#\textgreater{}  7   0.191   0.102}
\CommentTok{\#\textgreater{}  8   0.202   0.148}
\CommentTok{\#\textgreater{}  9   0.188   0.200}
\CommentTok{\#\textgreater{} 10   0.244   0.128}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

You may see where I am going with this. Just like column names, we can select a range of columns using the same method \texttt{index\_from:index\_to}.

Try it:

\begin{itemize}
\tightlist
\item
  select the columns from \texttt{age\_u18} to \texttt{age\_o65} using \texttt{:} and the column position
\item
  select the columns in reverse order by their indexes
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\DecValTok{6}\OperatorTok{:}\DecValTok{9}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 4}
\CommentTok{\#\textgreater{}    age\_u18 age1834 age3564 age\_o65}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1   0.234   0.202   0.398   0.166}
\CommentTok{\#\textgreater{}  2   0.181   0.151   0.461   0.207}
\CommentTok{\#\textgreater{}  3   0.171   0.214   0.437   0.178}
\CommentTok{\#\textgreater{}  4   0.203   0.227   0.436   0.133}
\CommentTok{\#\textgreater{}  5   0.177   0.203   0.430   0.190}
\CommentTok{\#\textgreater{}  6   0.163   0.237   0.439   0.162}
\CommentTok{\#\textgreater{}  7   0.191   0.326   0.380   0.102}
\CommentTok{\#\textgreater{}  8   0.202   0.183   0.466   0.148}
\CommentTok{\#\textgreater{}  9   0.188   0.150   0.462   0.200}
\CommentTok{\#\textgreater{} 10   0.244   0.286   0.342   0.128}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\DecValTok{9}\OperatorTok{:}\DecValTok{6}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 4}
\CommentTok{\#\textgreater{}    age\_o65 age3564 age1834 age\_u18}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1   0.166   0.398   0.202   0.234}
\CommentTok{\#\textgreater{}  2   0.207   0.461   0.151   0.181}
\CommentTok{\#\textgreater{}  3   0.178   0.437   0.214   0.171}
\CommentTok{\#\textgreater{}  4   0.133   0.436   0.227   0.203}
\CommentTok{\#\textgreater{}  5   0.190   0.430   0.203   0.177}
\CommentTok{\#\textgreater{}  6   0.162   0.439   0.237   0.163}
\CommentTok{\#\textgreater{}  7   0.102   0.380   0.326   0.191}
\CommentTok{\#\textgreater{}  8   0.148   0.466   0.183   0.202}
\CommentTok{\#\textgreater{}  9   0.200   0.462   0.150   0.188}
\CommentTok{\#\textgreater{} 10   0.128   0.342   0.286   0.244}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Base R Side Bar: To help build your intuition, I want to point out some base R functionality. Using the colon \texttt{:} with integers (whole numbers) is actually not a \texttt{select()} specific functionality. This is something that is rather handy and built directly into R. Using the colon operator, we can create ranges of numbers in the same exact way as we did above. If we want create the range of numbers from 1 to 10, we write \texttt{1:10}. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10.
\end{quote}

In our scenario, we want to select the last two columns. We may not know their names \emph{or} their position. Luckily, there's a function for that.

\texttt{last\_col()} is a handy function that enables us to select the last column. There is also an option to get an offset from the last column. An offset would allow us to grab the second to last column by setting the offset to 1. By setting the offset, \texttt{last\_col()} will from the \texttt{offset\ +\ 1} from the last column. So if the offset is set to 1, we would be grabbing the second to last column.

Let's give it a shot:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{last\_col}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 1}
\CommentTok{\#\textgreater{}    m\_atown      }
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}        }
\CommentTok{\#\textgreater{}  1 HOLDEN       }
\CommentTok{\#\textgreater{}  2 WEST BOYLSTON}
\CommentTok{\#\textgreater{}  3 WORCESTER    }
\CommentTok{\#\textgreater{}  4 MILFORD      }
\CommentTok{\#\textgreater{}  5 LEOMINSTER   }
\CommentTok{\#\textgreater{}  6 LEICESTER    }
\CommentTok{\#\textgreater{}  7 WEBSTER      }
\CommentTok{\#\textgreater{}  8 WORCESTER    }
\CommentTok{\#\textgreater{}  9 BERLIN       }
\CommentTok{\#\textgreater{} 10 WORCESTER    }
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{last\_col}\NormalTok{(}\DataTypeTok{offset =} \DecValTok{1}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 1}
\CommentTok{\#\textgreater{}    area\_acr\_1}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1     23242.}
\CommentTok{\#\textgreater{}  2      8868.}
\CommentTok{\#\textgreater{}  3     24610.}
\CommentTok{\#\textgreater{}  4      9616.}
\CommentTok{\#\textgreater{}  5     18993.}
\CommentTok{\#\textgreater{}  6     15763.}
\CommentTok{\#\textgreater{}  7      9347.}
\CommentTok{\#\textgreater{}  8     24610.}
\CommentTok{\#\textgreater{}  9      8431.}
\CommentTok{\#\textgreater{} 10     24610.}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{last\_col}\NormalTok{(}\DataTypeTok{offset =} \DecValTok{1}\NormalTok{)}\OperatorTok{:}\KeywordTok{last\_col}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 2}
\CommentTok{\#\textgreater{}    area\_acr\_1 m\_atown      }
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{} \textless{}chr\textgreater{}        }
\CommentTok{\#\textgreater{}  1     23242. HOLDEN       }
\CommentTok{\#\textgreater{}  2      8868. WEST BOYLSTON}
\CommentTok{\#\textgreater{}  3     24610. WORCESTER    }
\CommentTok{\#\textgreater{}  4      9616. MILFORD      }
\CommentTok{\#\textgreater{}  5     18993. LEOMINSTER   }
\CommentTok{\#\textgreater{}  6     15763. LEICESTER    }
\CommentTok{\#\textgreater{}  7      9347. WEBSTER      }
\CommentTok{\#\textgreater{}  8     24610. WORCESTER    }
\CommentTok{\#\textgreater{}  9      8431. BERLIN       }
\CommentTok{\#\textgreater{} 10     24610. WORCESTER    }
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

\texttt{last\_col()} comes from another packages called \texttt{tidyselect} which is imported with \texttt{dplyr}. This package contains a number of helper functions. There are 9 total helpers and you've already learned one of them. We will briefly review four more of these. I'm sure you are able to deduce how the functions work solely based on their names. The functions are:

\begin{itemize}
\tightlist
\item
  \texttt{starts\_with()}: a string to search that columns start with
\item
  \texttt{ends\_with()}: a string to search that columns end with
\item
  \texttt{contains()}: a string to search for in the column names at any position
\item
  \texttt{everything()}: selects the remaining columns
\end{itemize}

Each of these function take a character string and searches the column headers for them.

Try it out:

\begin{itemize}
\tightlist
\item
  find all columns that start with \texttt{"med"}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{starts\_with}\NormalTok{(}\StringTok{"med"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 7}
\CommentTok{\#\textgreater{}    med\_house\_income med\_gross\_rent med\_home\_val med\_yr\_built\_raw med\_yr\_built}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{}          \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{} \textless{}chr\textgreater{}       }
\CommentTok{\#\textgreater{}  1           105735           1640       349000             1988 1980 to 1989}
\CommentTok{\#\textgreater{}  2            69625            894       230200             1955 1950 to 1959}
\CommentTok{\#\textgreater{}  3            70679           1454       207200             1959 1950 to 1959}
\CommentTok{\#\textgreater{}  4            74528            954       268400             1973 1970 to 1979}
\CommentTok{\#\textgreater{}  5            52885           1018       223200             1964 1960 to 1969}
\CommentTok{\#\textgreater{}  6            64100            867       232700             1966 1960 to 1969}
\CommentTok{\#\textgreater{}  7            37093            910       170900             1939 Prior to 19\textasciitilde{}}
\CommentTok{\#\textgreater{}  8            87750           1088       270100             1939 Prior to 19\textasciitilde{}}
\CommentTok{\#\textgreater{}  9            97417           1037       379600             1981 1980 to 1989}
\CommentTok{\#\textgreater{} 10            43384           1017       156500             1939 Prior to 19\textasciitilde{}}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows, and 2 more variables: med\_yr\_moved\_inraw \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_yr\_rent\_moved\_in \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  select columns that end with \texttt{"per"}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{ends\_with}\NormalTok{(}\StringTok{"per"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 8}
\CommentTok{\#\textgreater{}    fam\_pov\_per fam\_house\_per fem\_head\_per same\_sex\_coup\_p\textasciitilde{} grand\_head\_per}
\CommentTok{\#\textgreater{}          \textless{}dbl\textgreater{}         \textless{}dbl\textgreater{}        \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{}          \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1      0.0475         0.797       0.0899            0            0      }
\CommentTok{\#\textgreater{}  2      0.0652         0.698       0.120             0            0.00583}
\CommentTok{\#\textgreater{}  3      0.0584         0.659       0.114             0            0      }
\CommentTok{\#\textgreater{}  4      0.0249         0.657       0.121             0            0      }
\CommentTok{\#\textgreater{}  5      0.198          0.531       0.158             0            0.00946}
\CommentTok{\#\textgreater{}  6      0.0428         0.665       0.0603            0            0.0353 }
\CommentTok{\#\textgreater{}  7      0.0762         0.632       0.227             0            0.00643}
\CommentTok{\#\textgreater{}  8      0.101          0.636       0.0582            0.297        0.0260 }
\CommentTok{\#\textgreater{}  9      0.0149         0.758       0.0721            0            0.00434}
\CommentTok{\#\textgreater{} 10      0.0954         0.460       0.225             0            0.0279 }
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows, and 3 more variables: vacant\_unit\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   renters\_per \textless{}dbl\textgreater{}, home\_own\_per \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  find any column that contains the string \texttt{"yr"}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{contains}\NormalTok{(}\StringTok{"yr"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 4}
\CommentTok{\#\textgreater{}    med\_yr\_built\_raw med\_yr\_built  med\_yr\_moved\_inraw med\_yr\_rent\_moved\_in}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{} \textless{}chr\textgreater{}                      \textless{}dbl\textgreater{}                \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1             1988 1980 to 1989                2004                 2012}
\CommentTok{\#\textgreater{}  2             1955 1950 to 1959                2003                 2010}
\CommentTok{\#\textgreater{}  3             1959 1950 to 1959                2007                 2012}
\CommentTok{\#\textgreater{}  4             1973 1970 to 1979                2006                 2011}
\CommentTok{\#\textgreater{}  5             1964 1960 to 1969                2006                 2011}
\CommentTok{\#\textgreater{}  6             1966 1960 to 1969                2000                 2009}
\CommentTok{\#\textgreater{}  7             1939 Prior to 1940               2011                 2012}
\CommentTok{\#\textgreater{}  8             1939 Prior to 1940               2006                 2012}
\CommentTok{\#\textgreater{}  9             1981 1980 to 1989                2004                 2012}
\CommentTok{\#\textgreater{} 10             1939 Prior to 1940               2011                   NA}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows}
\end{Highlighting}
\end{Shaded}

\begin{itemize}
\tightlist
\item
  select columns that start with \texttt{med} then select everything else
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(acs\_raw, }\KeywordTok{contains}\NormalTok{(}\StringTok{"yr"}\NormalTok{), }\KeywordTok{everything}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 1,478 x 59}
\CommentTok{\#\textgreater{}    med\_yr\_built\_raw med\_yr\_built med\_yr\_moved\_in\textasciitilde{} med\_yr\_rent\_mov\textasciitilde{} ct\_id\_10}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{} \textless{}chr\textgreater{}                   \textless{}dbl\textgreater{}            \textless{}dbl\textgreater{}    \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1             1988 1980 to 1989             2004             2012  2.50e10}
\CommentTok{\#\textgreater{}  2             1955 1950 to 1959             2003             2010  2.50e10}
\CommentTok{\#\textgreater{}  3             1959 1950 to 1959             2007             2012  2.50e10}
\CommentTok{\#\textgreater{}  4             1973 1970 to 1979             2006             2011  2.50e10}
\CommentTok{\#\textgreater{}  5             1964 1960 to 1969             2006             2011  2.50e10}
\CommentTok{\#\textgreater{}  6             1966 1960 to 1969             2000             2009  2.50e10}
\CommentTok{\#\textgreater{}  7             1939 Prior to 19\textasciitilde{}             2011             2012  2.50e10}
\CommentTok{\#\textgreater{}  8             1939 Prior to 19\textasciitilde{}             2006             2012  2.50e10}
\CommentTok{\#\textgreater{}  9             1981 1980 to 1989             2004             2012  2.50e10}
\CommentTok{\#\textgreater{} 10             1939 Prior to 19\textasciitilde{}             2011               NA  2.50e10}
\CommentTok{\#\textgreater{} \# ... with 1,468 more rows, and 54 more variables: name \textless{}chr\textgreater{}, total\_pop \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   pop\_den \textless{}dbl\textgreater{}, sex\_ratio \textless{}dbl\textgreater{}, age\_u18 \textless{}dbl\textgreater{}, age1834 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   age3564 \textless{}dbl\textgreater{}, age\_o65 \textless{}dbl\textgreater{}, for\_born \textless{}dbl\textgreater{}, white \textless{}dbl\textgreater{}, black \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   asian \textless{}dbl\textgreater{}, hispanic \textless{}dbl\textgreater{}, two\_or\_more \textless{}dbl\textgreater{}, eth\_het \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_house\_income \textless{}dbl\textgreater{}, pub\_assist \textless{}dbl\textgreater{}, gini \textless{}dbl\textgreater{}, fam\_pov\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   unemp\_rate \textless{}dbl\textgreater{}, total\_house\_h \textless{}dbl\textgreater{}, fam\_house\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   fem\_head\_per \textless{}dbl\textgreater{}, same\_sex\_coup\_per \textless{}dbl\textgreater{}, grand\_head\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   less\_than\_hs \textless{}dbl\textgreater{}, hs\_grad \textless{}dbl\textgreater{}, some\_coll \textless{}dbl\textgreater{}, bach \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   master \textless{}dbl\textgreater{}, prof \textless{}dbl\textgreater{}, doc \textless{}dbl\textgreater{}, commute\_less10 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute1030 \textless{}dbl\textgreater{}, commute3060 \textless{}dbl\textgreater{}, commute6090 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute\_over90 \textless{}dbl\textgreater{}, by\_auto \textless{}dbl\textgreater{}, by\_pub\_trans \textless{}dbl\textgreater{}, by\_bike \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   by\_walk \textless{}dbl\textgreater{}, total\_house\_units \textless{}dbl\textgreater{}, vacant\_unit\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   renters\_per \textless{}dbl\textgreater{}, home\_own\_per \textless{}dbl\textgreater{}, med\_gross\_rent \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_home\_val \textless{}dbl\textgreater{}, area\_acres \textless{}dbl\textgreater{}, town\_id \textless{}dbl\textgreater{}, town \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   fips\_stco \textless{}dbl\textgreater{}, county \textless{}chr\textgreater{}, area\_acr\_1 \textless{}dbl\textgreater{}, m\_atown \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{selecting-rows}{%
\section{Selecting Rows}\label{selecting-rows}}

Though a somewhat infrequent event, it will be handy to know how to select rows. There are two ways in which we can select our rows. The first is by specifying exactly which rows by their position. The other way is to filter down our data based on a condition---i.e.~median household income within a range. The functions to do this are \texttt{slice()} and \texttt{filter()} respectively. The remainder of this chapter will introduce you to \texttt{slice()}. We will learn how to filter in the next chapter.

Like \texttt{select()} we can also select rows. But rows do not have names, so we must select the rows based on their position. Given your familiarity with selecting by column position this should be a cake walk for you.

Similar to \texttt{last\_col()} we have the function \texttt{n()}. \texttt{n()} is a rather handy little function which tells us how many observations there are in a tibble. This allows to specify the last row of a tibble.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{slice}\NormalTok{(acs\_raw, }\KeywordTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 59}
\CommentTok{\#\textgreater{}   ct\_id\_10 name  total\_pop pop\_den sex\_ratio age\_u18 age1834 age3564 age\_o65}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1  2.50e10 Cens\textasciitilde{}      5821   2760.     0.885   0.181   0.204   0.435   0.180}
\CommentTok{\#\textgreater{} \# ... with 50 more variables: for\_born \textless{}dbl\textgreater{}, white \textless{}dbl\textgreater{}, black \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   asian \textless{}dbl\textgreater{}, hispanic \textless{}dbl\textgreater{}, two\_or\_more \textless{}dbl\textgreater{}, eth\_het \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_house\_income \textless{}dbl\textgreater{}, pub\_assist \textless{}dbl\textgreater{}, gini \textless{}dbl\textgreater{}, fam\_pov\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   unemp\_rate \textless{}dbl\textgreater{}, total\_house\_h \textless{}dbl\textgreater{}, fam\_house\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   fem\_head\_per \textless{}dbl\textgreater{}, same\_sex\_coup\_per \textless{}dbl\textgreater{}, grand\_head\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   less\_than\_hs \textless{}dbl\textgreater{}, hs\_grad \textless{}dbl\textgreater{}, some\_coll \textless{}dbl\textgreater{}, bach \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   master \textless{}dbl\textgreater{}, prof \textless{}dbl\textgreater{}, doc \textless{}dbl\textgreater{}, commute\_less10 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute1030 \textless{}dbl\textgreater{}, commute3060 \textless{}dbl\textgreater{}, commute6090 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute\_over90 \textless{}dbl\textgreater{}, by\_auto \textless{}dbl\textgreater{}, by\_pub\_trans \textless{}dbl\textgreater{}, by\_bike \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   by\_walk \textless{}dbl\textgreater{}, total\_house\_units \textless{}dbl\textgreater{}, vacant\_unit\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   renters\_per \textless{}dbl\textgreater{}, home\_own\_per \textless{}dbl\textgreater{}, med\_gross\_rent \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_home\_val \textless{}dbl\textgreater{}, med\_yr\_built\_raw \textless{}dbl\textgreater{}, med\_yr\_built \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_yr\_moved\_inraw \textless{}dbl\textgreater{}, med\_yr\_rent\_moved\_in \textless{}dbl\textgreater{}, area\_acres \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   town\_id \textless{}dbl\textgreater{}, town \textless{}chr\textgreater{}, fips\_stco \textless{}dbl\textgreater{}, county \textless{}chr\textgreater{}, area\_acr\_1 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   m\_atown \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

Unlike \texttt{last\_col()}, \texttt{n()} provides us with a number. Instead of specifying an offset we can instead subtract directly from the output of \texttt{n()}. To grab the last three rows we can write \texttt{(n()\ -\ 3):n()}. We put \texttt{n()-3} inside of parantheses so R knows to process \texttt{n()\ -\ 3} first.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{slice}\NormalTok{(acs\_raw, (}\KeywordTok{n}\NormalTok{() }\OperatorTok{{-}}\StringTok{ }\DecValTok{3}\NormalTok{)}\OperatorTok{:}\KeywordTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 59}
\CommentTok{\#\textgreater{}   ct\_id\_10 name  total\_pop pop\_den sex\_ratio age\_u18 age1834 age3564 age\_o65}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1  2.50e10 Cens\textasciitilde{}      2519   3083.     0.806   0.202   0.268   0.397   0.132}
\CommentTok{\#\textgreater{} 2  2.50e10 Cens\textasciitilde{}      3500   5392.     1.05    0.205   0.277   0.395   0.122}
\CommentTok{\#\textgreater{} 3  2.50e10 Cens\textasciitilde{}      5816   2677.     1.20    0.191   0.233   0.458   0.118}
\CommentTok{\#\textgreater{} 4  2.50e10 Cens\textasciitilde{}      5821   2760.     0.885   0.181   0.204   0.435   0.180}
\CommentTok{\#\textgreater{} \# ... with 50 more variables: for\_born \textless{}dbl\textgreater{}, white \textless{}dbl\textgreater{}, black \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   asian \textless{}dbl\textgreater{}, hispanic \textless{}dbl\textgreater{}, two\_or\_more \textless{}dbl\textgreater{}, eth\_het \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_house\_income \textless{}dbl\textgreater{}, pub\_assist \textless{}dbl\textgreater{}, gini \textless{}dbl\textgreater{}, fam\_pov\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   unemp\_rate \textless{}dbl\textgreater{}, total\_house\_h \textless{}dbl\textgreater{}, fam\_house\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   fem\_head\_per \textless{}dbl\textgreater{}, same\_sex\_coup\_per \textless{}dbl\textgreater{}, grand\_head\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   less\_than\_hs \textless{}dbl\textgreater{}, hs\_grad \textless{}dbl\textgreater{}, some\_coll \textless{}dbl\textgreater{}, bach \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   master \textless{}dbl\textgreater{}, prof \textless{}dbl\textgreater{}, doc \textless{}dbl\textgreater{}, commute\_less10 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute1030 \textless{}dbl\textgreater{}, commute3060 \textless{}dbl\textgreater{}, commute6090 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute\_over90 \textless{}dbl\textgreater{}, by\_auto \textless{}dbl\textgreater{}, by\_pub\_trans \textless{}dbl\textgreater{}, by\_bike \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   by\_walk \textless{}dbl\textgreater{}, total\_house\_units \textless{}dbl\textgreater{}, vacant\_unit\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   renters\_per \textless{}dbl\textgreater{}, home\_own\_per \textless{}dbl\textgreater{}, med\_gross\_rent \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_home\_val \textless{}dbl\textgreater{}, med\_yr\_built\_raw \textless{}dbl\textgreater{}, med\_yr\_built \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_yr\_moved\_inraw \textless{}dbl\textgreater{}, med\_yr\_rent\_moved\_in \textless{}dbl\textgreater{}, area\_acres \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   town\_id \textless{}dbl\textgreater{}, town \textless{}chr\textgreater{}, fips\_stco \textless{}dbl\textgreater{}, county \textless{}chr\textgreater{}, area\_acr\_1 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   m\_atown \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

Try it:

\begin{itemize}
\tightlist
\item
  select the first row, rows 100-105, and the last row
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{slice}\NormalTok{(acs\_raw, }\DecValTok{1}\NormalTok{, }\DecValTok{100}\OperatorTok{:}\DecValTok{105}\NormalTok{, }\KeywordTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 8 x 59}
\CommentTok{\#\textgreater{}   ct\_id\_10 name  total\_pop pop\_den sex\_ratio age\_u18 age1834 age3564 age\_o65}
\CommentTok{\#\textgreater{}      \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}     \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}   \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1  2.50e10 Cens\textasciitilde{}      4585    333.     1.13    0.234  0.202    0.398  0.166 }
\CommentTok{\#\textgreater{} 2  2.50e10 Cens\textasciitilde{}      5223   2402.     1.21    0.183  0.171    0.450  0.197 }
\CommentTok{\#\textgreater{} 3  2.50e10 Cens\textasciitilde{}      5586    592.     1.09    0.278  0.116    0.413  0.193 }
\CommentTok{\#\textgreater{} 4  2.50e10 Cens\textasciitilde{}      4474   1119.     0.962   0.282  0.0847   0.427  0.206 }
\CommentTok{\#\textgreater{} 5  2.50e10 Cens\textasciitilde{}      6713    674.     0.928   0.223  0.216    0.423  0.139 }
\CommentTok{\#\textgreater{} 6  2.50e10 Cens\textasciitilde{}      6676   3541.     0.999   0.249  0.266    0.395  0.0902}
\CommentTok{\#\textgreater{} 7  2.50e10 Cens\textasciitilde{}      8141    820.     1.25    0.258  0.169    0.410  0.164 }
\CommentTok{\#\textgreater{} 8  2.50e10 Cens\textasciitilde{}      5821   2760.     0.885   0.181  0.204    0.435  0.180 }
\CommentTok{\#\textgreater{} \# ... with 50 more variables: for\_born \textless{}dbl\textgreater{}, white \textless{}dbl\textgreater{}, black \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   asian \textless{}dbl\textgreater{}, hispanic \textless{}dbl\textgreater{}, two\_or\_more \textless{}dbl\textgreater{}, eth\_het \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_house\_income \textless{}dbl\textgreater{}, pub\_assist \textless{}dbl\textgreater{}, gini \textless{}dbl\textgreater{}, fam\_pov\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   unemp\_rate \textless{}dbl\textgreater{}, total\_house\_h \textless{}dbl\textgreater{}, fam\_house\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   fem\_head\_per \textless{}dbl\textgreater{}, same\_sex\_coup\_per \textless{}dbl\textgreater{}, grand\_head\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   less\_than\_hs \textless{}dbl\textgreater{}, hs\_grad \textless{}dbl\textgreater{}, some\_coll \textless{}dbl\textgreater{}, bach \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   master \textless{}dbl\textgreater{}, prof \textless{}dbl\textgreater{}, doc \textless{}dbl\textgreater{}, commute\_less10 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute1030 \textless{}dbl\textgreater{}, commute3060 \textless{}dbl\textgreater{}, commute6090 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute\_over90 \textless{}dbl\textgreater{}, by\_auto \textless{}dbl\textgreater{}, by\_pub\_trans \textless{}dbl\textgreater{}, by\_bike \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   by\_walk \textless{}dbl\textgreater{}, total\_house\_units \textless{}dbl\textgreater{}, vacant\_unit\_per \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   renters\_per \textless{}dbl\textgreater{}, home\_own\_per \textless{}dbl\textgreater{}, med\_gross\_rent \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_home\_val \textless{}dbl\textgreater{}, med\_yr\_built\_raw \textless{}dbl\textgreater{}, med\_yr\_built \textless{}chr\textgreater{},}
\CommentTok{\#\textgreater{} \#   med\_yr\_moved\_inraw \textless{}dbl\textgreater{}, med\_yr\_rent\_moved\_in \textless{}dbl\textgreater{}, area\_acres \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   town\_id \textless{}dbl\textgreater{}, town \textless{}chr\textgreater{}, fips\_stco \textless{}dbl\textgreater{}, county \textless{}chr\textgreater{}, area\_acr\_1 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   m\_atown \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

\hypertarget{revisiting-commmuting}{%
\section{Revisiting commmuting}\label{revisiting-commmuting}}

We've just spent a fair amount of time learning how to work with our data. It's now time to return to the problem at hand. We still haven't addressed what data will be of use to our partner at the non-profit. While urban informatics is largely technical, it is still mostly intellectual. We have to think through problems and be methodical with our data selection and curation. We have to think about what our data tells us and why it is important.

During these exercises, I hope you were looking at the data and thinking about what may be helpful to the non-profit. Again, the goal is to provide them with what is useful, but not more than they need.

\hypertarget{exercise-1}{%
\subsection{Exercise}\label{exercise-1}}

It is now incumbent upon you to curate the data BARI Census Indicator dataset for the non-profit. Refamiliarize yourself with the data. Select a subset of columns that you believe will provide the best insight into commuting behavior in the Greater Boston Area while also providing demographic insight into the the area.

When making decisions like this, I like to think of a quote from The Master of Disguise:

\begin{quote}
``Answer these questions for yourself: who? Why? Where? How?''
\end{quote}

Save the resultant tibble to an object named \texttt{commute} or something else informative.

Below is one approach to this question. For this, I have selected all columns pertaining to commute time (columns that start with \texttt{commute}), the method by which people commute (begin with \texttt{by}), medisan household income, and the name of the census tract. The name of the census tract will be helpful for identifying ``where''.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{select}\NormalTok{(acs\_raw,}
\NormalTok{                  county,}
\NormalTok{                  hs\_grad, bach, master,}
                  \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"commute"}\NormalTok{),}
                  \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"by"}\NormalTok{),}
\NormalTok{                  med\_house\_income)}
\end{Highlighting}
\end{Shaded}

{[}\^{}indicators{]}

\hypertarget{thats-too-much-data}{%
\chapter{\texorpdfstring{That's \emph{too much} data}{That's too much data}}\label{thats-too-much-data}}

Let us continue with the scenario developed in the last chapter. There is a non-profit who is seeking graduate student assistance to provide a curated dataset that provides insight into the commuting behavior of the Greater Boston Area. Using BARI's Massachussett's Census Indicators dataset, we were able to reduce the 52 initial columns down to 11. However these data are for the entire state not just the Greater Boston Area. This leaves us with two tasks: 1) definine the Greater Boston Area and 2) create a subset of our data that fit our criteria defined in 1.

Execute the below code to recreate to \texttt{commute} tibble. This will be the \emph{second to last} time we write this chunk of code.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{acs\_raw \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/ACS\_1317\_TRACT.csv"}\NormalTok{)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{select}\NormalTok{(acs\_raw,}
\NormalTok{                  county,}
\NormalTok{                  hs\_grad, bach, master,}
                  \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"commute"}\NormalTok{),}
                  \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"by"}\NormalTok{),}
\NormalTok{                  med\_house\_income)}
\end{Highlighting}
\end{Shaded}

\hypertarget{filtering}{%
\section{\texorpdfstring{\texttt{filter()}ing}{filter()ing}}\label{filtering}}

Previously we looked at ways of selecting columns. Here, we will focus on creating subsets of our data. We will rely on the function \texttt{dplyr::filter()} for this. \texttt{filter()} differs from \texttt{slice()} in that filter will check to see if data fit a specified criteria whereas slice is only concerned with the position of a row.

\begin{quote}
Explore the help documentation of \texttt{filter()} by running the command \texttt{?filter()} in your console.
\end{quote}

The first argument of \texttt{filter()} is of course the tibble which you would like to subset. Secondly, we again see \texttt{...}. In the case of \texttt{filter()}, we provide what are called logical expressions to \texttt{...}. \texttt{filter()} then only returns the observations when that logical expression, or condition, is true.

Almost every time you purchase something online whether that is from Amazon or Etsy you are filtering your data using some logic. Whether that is checking the Prime tick box, specifying a price range on Etsy, or a restaurant rating on Yelp. These are all \emph{conditions} that you are providing to your search.

We can create these types of filters on our own data and to do so, we will need to understand how logical expressions work. A logical expression is any expression that can be boiled down to true or false.

For example, using our \texttt{commute} dataset, we can check to see which Census Tracts have more than 75\% of commuters traveling by auto.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{auto\_commuters \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute, by\_auto }\OperatorTok{\textgreater{}}\StringTok{ }\FloatTok{0.75}\NormalTok{)}

\KeywordTok{select}\NormalTok{(auto\_commuters, by\_auto)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1,012 x 1
##    by_auto
##      <dbl>
##  1   0.927
##  2   0.970
##  3   0.925
##  4   0.904
##  5   0.899
##  6   0.911
##  7   0.810
##  8   0.851
##  9   0.900
## 10   0.787
## # ... with 1,002 more rows
\end{verbatim}

This above line checks every single value of \texttt{by\_auto} and asks ``is this value above 0.75?'' and when it is filter will include that row in the output. Another way to say this is that when \texttt{by\_auto} is above 0.75 \emph{the condition is met}.

In R, as with most other programming languages, there are a number of \emph{logical operators} that are used to check for conditions. We will learn these in the following section.

\hypertarget{logical-operators}{%
\section{Logical operators}\label{logical-operators}}

R has myriad ways to create logical expressions. Here we will focus on the six main \emph{relational} logical operators. These are called relational because we are checking to see how one value relates to another. In general we tend to ask questions like ``are these the same?'', ``are they not the same?'', and ``is one value larger or smaller than another?''. While you are getting the hang of it, I encourage you to try and verbalize logical expressions.

\begin{itemize}
\tightlist
\item
  \texttt{\textless{}} : less than
\item
  \texttt{\textgreater{}} : greater than
\item
  \texttt{\textless{}=} : less than or equal to
\item
  \texttt{\textgreater{}=} : greater than or equal to
\item
  \texttt{==} : exactly equal (I like to think of it as \emph{``are these the same thing?''})
\item
  \texttt{!=} : not equal ( \emph{``are these things not the same''})
\item
  \texttt{!}: Negation. This returns the opposite value.
\end{itemize}

Let's bring it back to early algebra and work with the variable x and y. Let's say \texttt{x\ =\ 3} and \texttt{y\ =\ 5}.

Is \texttt{x} less than \texttt{y}?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# set variables.}
\NormalTok{x \textless{}{-}}\StringTok{ }\DecValTok{3}
\NormalTok{y \textless{}{-}}\StringTok{ }\DecValTok{5}

\NormalTok{x }\OperatorTok{\textless{}}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

Yes it is. Here R returns a logical value which are represented as \texttt{TRUE} and \texttt{FALSE}.

Is \texttt{x} greater than \texttt{y}?

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# greater than?}
\NormalTok{x }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

And for the sake of illustration:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# less than or equal}
\NormalTok{x }\OperatorTok{\textless{}=}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# greater or equal}
\NormalTok{x }\OperatorTok{\textgreater{}=}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# exactly equal?}
\NormalTok{x }\OperatorTok{==}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# not equal}
\NormalTok{x }\OperatorTok{!=}\StringTok{ }\NormalTok{y}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

The power of logical operators isn't necessarily in the ability to compare one value against another, but the ability to compare many values to one value or even many values to multiple other values. This is what \texttt{filter()} helps us do.

Using \texttt{filter()} we can compare one column to another. When we do this, the values of those columns at each row are compared. For example, we can identify all of the Census Tracts where people walk more than they drive.

\begin{quote}
Note: I am saving the results to an object called \texttt{walk} to then select only a few columns.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{walking \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute, by\_walk }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{by\_auto)}

\KeywordTok{select}\NormalTok{(walking, county, by\_walk, by\_auto)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 54 x 3
##    county    by_walk by_auto
##    <chr>       <dbl>   <dbl>
##  1 HAMPSHIRE   0.652  0.0993
##  2 HAMPSHIRE   0.632  0.0985
##  3 HAMPSHIRE   0.580  0.189 
##  4 HAMPSHIRE   0.613  0.143 
##  5 HAMPSHIRE   0.625  0.0916
##  6 MIDDLESEX   0.427  0.213 
##  7 MIDDLESEX   0.545  0.142 
##  8 MIDDLESEX   0.372  0.287 
##  9 MIDDLESEX   0.299  0.240 
## 10 SUFFOLK     0.548  0.105 
## # ... with 44 more rows
\end{verbatim}

We can also use filter to see where the walking rates and the driving rates are the same. As shown above we use \texttt{==} to test if things are the same.

\begin{quote}
\textbf{An important distinction}: \texttt{\textless{}-} is different from \texttt{=} and \texttt{=} is different from \texttt{==}. If you are ever confused about which operator to use ask yourself what your goal is. If your goal is to assign an object use \texttt{\textless{}-}. If your goal is to assign an argument value use \texttt{=}. And if you are trying to compare two things use \texttt{==}.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{walk\_auto \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute, by\_walk }\OperatorTok{==}\StringTok{ }\NormalTok{by\_auto)}

\KeywordTok{select}\NormalTok{(walk\_auto, county, by\_walk, by\_auto)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 3
##   county  by_walk by_auto
##   <chr>     <dbl>   <dbl>
## 1 <NA>          0       0
## 2 SUFFOLK       0       0
\end{verbatim}

So far we have only been checking one condition and in most cases this actually will not suffice. You may want to check multiple conditions at one time. When we use filter we can add a logical expression as another argument. In doing so, filter will check to see if both conditions are met and, when they are, that row is returned. This is called an ``and'' statement. Meaning condition one \textbf{and} condition two need to be \texttt{TRUE}.

Building upon the \texttt{walking} example, we can further narrow down the observations by adding a second condition which returns only the observations that have median household incomes below \$40,000.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{low\_inc\_walk \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute, }
\NormalTok{                       by\_walk }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{by\_auto,}
\NormalTok{                       med\_house\_income }\OperatorTok{\textless{}}\StringTok{ }\DecValTok{40000}\NormalTok{)}

\KeywordTok{select}\NormalTok{(low\_inc\_walk, by\_walk, by\_auto, med\_house\_income)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 11 x 3
##    by_walk by_auto med_house_income
##      <dbl>   <dbl>            <dbl>
##  1   0.580   0.189             2499
##  2   0.581   0.214            21773
##  3   0.647   0.108            36250
##  4   0.407   0.170            34677
##  5   0.381   0.159            30500
##  6   0.465   0.192            28618
##  7   0.340   0.243            16094
##  8   0.536   0.112            19267
##  9   0.436   0.161            22930
## 10   0.451   0.170            36500
## 11   0.677   0.107            31218
\end{verbatim}

\hypertarget{statements}{%
\subsection{\texorpdfstring{\texttt{\&} statements:}{\& statements:}}\label{statements}}

Now I want to introduce two more logical operators, \textbf{and} (\texttt{\&}), which was implicitly used in the above filter statement, and \textbf{or} (\texttt{\textbar{}}). \texttt{\&} compares two conditions and will return \texttt{TRUE} only if they are both \texttt{TRUE}. \texttt{\textbar{}} will return \texttt{TRUE} when one of the conditions is \texttt{TRUE}.

Now for an illustrative example of \texttt{\&} statements :

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# We have TRUE and TRUE, this should be false because they aren\textquotesingle{}t both TRUE}
\OtherTok{TRUE} \OperatorTok{\&}\StringTok{ }\OtherTok{FALSE} 
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# both a TRUE, we expect TRUE}
\OtherTok{TRUE} \OperatorTok{\&}\StringTok{ }\OtherTok{TRUE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The first statement is TRUE, but the second is not TRUE, expect FALSE}
\NormalTok{(}\DecValTok{1} \OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{\&}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{\textless{}}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The first statement is TRUE and the second is TRUE, expect TRUE}
\NormalTok{(}\DecValTok{1} \OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{\&}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{\textless{}=}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\hypertarget{statements-1}{%
\subsection{\texorpdfstring{\texttt{\textbar{}} statements:}{\textbar{} statements:}}\label{statements-1}}

Or statements are used to evaluate whether or not something meets \textbf{at least} one of the two conditions. This means that the only time that an \emph{or} statement evaluates to \texttt{FALSE} is when both expressions result in \texttt{FALSE}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# True is present, so we expect TRUE}
\OtherTok{TRUE} \OperatorTok{|}\StringTok{ }\OtherTok{TRUE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# True is present, so we expect TRUE}
\OtherTok{TRUE} \OperatorTok{|}\StringTok{ }\OtherTok{FALSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# }
\OtherTok{FALSE} \OperatorTok{|}\StringTok{ }\OtherTok{FALSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

We can alter the previous \texttt{filter()} statement to show us places that walk more than they drive \textbf{or} have a low median household income.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{low\_inc\_or\_walk \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute,}
\NormalTok{                          by\_walk }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{by\_auto }\OperatorTok{|}\StringTok{ }\NormalTok{med\_house\_income }\OperatorTok{\textless{}}\StringTok{ }\DecValTok{40000}\NormalTok{)}

\KeywordTok{select}\NormalTok{(low\_inc\_or\_walk, by\_walk, by\_auto, med\_house\_income)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 224 x 3
##    by_walk by_auto med_house_income
##      <dbl>   <dbl>            <dbl>
##  1 0.0959    0.810            37093
##  2 0.102     0.800            31465
##  3 0.0478    0.813            14604
##  4 0.144     0.647            34940
##  5 0.0541    0.831            26615
##  6 0.0345    0.931            37935
##  7 0.0430    0.750            16400
##  8 0.0543    0.785            19548
##  9 0.00469   0.944            34821
## 10 0         0.955            34697
## # ... with 214 more rows
\end{verbatim}

\hypertarget{negation}{%
\subsection{Negation}\label{negation}}

Many times it will be easier to create a logical statement and say you want the \emph{opposite} of those results. In this case we will use the bang operator or the exclamation mark, \texttt{!}. To negate a logical value or logical statement put the bang \textbf{in front} of the statement or value.

For example we can make \texttt{FALSE} true by negating it.

\begin{Shaded}
\begin{Highlighting}[]
\OperatorTok{!}\OtherTok{FALSE}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

We can take a previous example

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# The first statement is TRUE and the second is TRUE, expect TRUE}
\NormalTok{(}\DecValTok{1} \OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{\&}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{\textless{}=}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# negate it}
\OperatorTok{!}\NormalTok{(}\DecValTok{1} \OperatorTok{==}\StringTok{ }\DecValTok{1}\NormalTok{) }\OperatorTok{\&}\StringTok{ }\NormalTok{(}\DecValTok{1} \OperatorTok{\textless{}=}\StringTok{ }\DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] FALSE
\end{verbatim}

Keep this in your pocket for later.

\hypertarget{defining-the-greater-boston-area}{%
\section{Defining the Greater Boston Area}\label{defining-the-greater-boston-area}}

You now have developed the requisite skills to subset the commuting data to just the Greater Boston Area. But we still haven't completely decided what constitutes it. We will take the naïve approach and say that Suffolk, Norfolk, and Middlesex counties are the Greater Boston Area. We can now filter our data to just these counties!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gba\_commute \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute, county }\OperatorTok{==}\StringTok{ "SUFFOLK"} \OperatorTok{|}\StringTok{ }\NormalTok{county }\OperatorTok{==}\StringTok{ "NORFOLK"} \OperatorTok{|}\StringTok{ }\NormalTok{county }\OperatorTok{==}\StringTok{ "MIDDLESEX"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The above code is actually rather redundant as we have written \texttt{county\ ==} three different times. When we using the same equality comparison we can actually use the sepcial \texttt{\%in\%} operator. This lets us look for a value \textbf{in} a vector of values (we'll learn more about vectors very shortly).

For example:

\begin{Shaded}
\begin{Highlighting}[]
\DecValTok{1} \OperatorTok{\%in\%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] TRUE
\end{verbatim}

This looks to see if the value on the left hand side is any of the three values in the vector---the thing that looks like \texttt{c(val1,\ val2,\ ...)}. Using this we can rewrite \texttt{gba\_commute} as:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gba\_commute \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(commute, county }\OperatorTok{\%in\%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"SUFFOLK"}\NormalTok{, }\StringTok{"NORFOLK"}\NormalTok{, }\StringTok{"MIDDLESEX"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{writing-data}{%
\subsection{Writing Data}\label{writing-data}}

You have created the proper subset of data that is needed. However, there is one more hurdle of jump---sending the data. To do this we need to get the tibble out of R and into a data format that can be used---probably a csv. \texttt{readr} provides funcitonality to do this as well.

While we used \texttt{read\_csv()} earlier, to write a csv we will use \texttt{write\_csv()}. The functionality is beautifully simple. The first argument here will be the tibble that you're going to write, \texttt{gba\_commute} in this case. And the second is the path to where you will write the data.

In general I recommend that your project has two folders. One titled \texttt{data-raw} where you will keep the scripts and raw data that you used to process the data. Then I suggest having a \texttt{data} folder as well. This is where you will keep your tidy, or finalized, data files.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{write\_csv}\NormalTok{(gba\_commute, }\StringTok{"data/gba\_commute.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Now you have a csv file that can be shared!

\hypertarget{the-pipe}{%
\chapter{\texorpdfstring{The pipe \texttt{\%\textgreater{}\%}}{The pipe \%\textgreater\%}}\label{the-pipe}}

Until now we have been using one function at a time. This can feel like it is rather limiting at times. The approach that we have been taking has been to perform some action, save the resultant object, and then perform another action. This leads to either overwriting the same existing object multiple times with an assignment \texttt{\textless{}-} or creating multiple other objects. The former solution does not have a great story for reproducibility. At any point within a script may refr to many different objects with the same name. The second solution can clutter your working environment and lead to an excess usage of memory.

Well then, ``what do we do instead?'' you may be asking. And my answer is ``use the forward pipe operator, of course.'' The forward pipe operator looks like \texttt{\%\textgreater{}\%}. This is a special function which allows the user to ``pipe an object forward into a function or . . . expression'' (Milton and Wickham, 2019). This is where the true power of the tidyverse comes from from.

\textbf{What it does}: The pipe operator takes the object or the output an expression on it's left hand side \texttt{lhs} and provides that as the first argument in the function of the right hand side. Additionally, it exposes the \texttt{lhs} as a temporary variable \texttt{.}. It is documented as \texttt{lhs\ \%\textgreater{}\%\ rhs}.

The creator of \href{https://github.com/rfordatascience/tidytuesday}{\#TidyTuesday} and RStudio employee, Thomas Mock has created a very illustrative example of how the pipe can simplify complex R function calls\footnote{Tidy Tuesday is a weekly online community event in which useRs across the world analyse the same dataset and share their visualizations online. Get involved on twitter with \texttt{\#TidyTuesday}.}.

The first example illustrates the creation of intermediate variables.

\begin{verbatim}
did_something <- do_something(data)

did_another_thing <- do_another_thing(did_something)

final_thing <- do_last_thing(did_another_thing)
\end{verbatim}

The second demonstrates nesting function calls inside of other function calls.

\begin{verbatim}
final_thing <- do_last_thing(
  do_another_thing(
    do_something(
      data
    )
  )
)
\end{verbatim}

Nested function calls are often difficult to debug and the user may get caught up in the mintuae of properly places parentheses.

\begin{quote}
Note: Debugging is the process of taking misbehaving code and fixing it.
\end{quote}

Using the pipe this chain of functions can be rewritten in the order that it happens.

\begin{verbatim}
final_thing <- data %>% 
  do_something() %>% 
  do_another_thing() %>% 
  do_last_thing()
\end{verbatim}

By using the pipe we are able to align our thinking and code writing. Additionally, each function call is separated on its own line which makes debugging a less daunting task.

\hypertarget{applying-the-pipe}{%
\section{Applying the pipe}\label{applying-the-pipe}}

Remember how I pointed out that the first argument for almost every function is the data? This is where that comes in handy. This allows us to use the pipe to chain together functions and ``makes it more intuitive to both read and write'' (magrittr vignette).

The tidyverse was designed with this in mind. This is why \texttt{select()}, \texttt{filter()}, and \texttt{mutate()} among many others are data first functions. Moreover, the output of each function is always a data frame which allows the user to provide that output as input into the next function.

As always, the most helpful way to wrap your head around this is to see it in action. Let's take one of the lines of code we used above and adapt it to use a pipe. We will select the name column of our data again. Previously we may have written

\begin{verbatim}
my_tbl <- select(data_frame, ...)
smaller_tbl <- filter(my_tbl, ...)
new_col_tbl <- mutate(smaller_tbl, ...)
\end{verbatim}

But now we are able to write more complex function chains such as

\begin{verbatim}
data %>% 
  filter() %>% 
  mutate() %>% 
  select()
\end{verbatim}

In the chapter on filtering data we began by reading in the data, selecting columns, and then filtered data. Here we will recreate the \texttt{low\_inc\_or\_walk} object which identified Census Tracts that have a higher rate of commuters who walk than drive or have a median household income below \$40,000.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{low\_inc\_or\_walk \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/ACS\_1317\_TRACT.csv"}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}
\NormalTok{    county,}
    \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"commute"}\NormalTok{),}
    \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"by"}\NormalTok{),}
\NormalTok{    med\_house\_income}
\NormalTok{  ) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}
\NormalTok{    by\_walk }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{by\_auto,}
\NormalTok{    med\_house\_income }\OperatorTok{\textless{}}\StringTok{ }\DecValTok{40000}
\NormalTok{  )}
\KeywordTok{glimpse}\NormalTok{(low\_inc\_or\_walk)}
\CommentTok{\#\textgreater{} Rows: 11}
\CommentTok{\#\textgreater{} Columns: 11}
\CommentTok{\#\textgreater{} $ county           \textless{}chr\textgreater{} "HAMPSHIRE", "SUFFOLK", "SUFFOLK", "SUFFOLK", "SUF...}
\CommentTok{\#\textgreater{} $ commute\_less10   \textless{}dbl\textgreater{} 0.40234261, 0.34490741, 0.41918528, 0.09421755, 0....}
\CommentTok{\#\textgreater{} $ commute1030      \textless{}dbl\textgreater{} 0.5077599, 0.3726852, 0.4042926, 0.5200162, 0.5528...}
\CommentTok{\#\textgreater{} $ commute3060      \textless{}dbl\textgreater{} 0.07027818, 0.22777778, 0.14673675, 0.35624747, 0....}
\CommentTok{\#\textgreater{} $ commute6090      \textless{}dbl\textgreater{} 0.01288433, 0.04953704, 0.02058695, 0.02951880, 0....}
\CommentTok{\#\textgreater{} $ commute\_over90   \textless{}dbl\textgreater{} 0.006734993, 0.005092593, 0.009198423, 0.000000000...}
\CommentTok{\#\textgreater{} $ by\_auto          \textless{}dbl\textgreater{} 0.1889132, 0.2140026, 0.1082621, 0.1704500, 0.1592...}
\CommentTok{\#\textgreater{} $ by\_pub\_trans     \textless{}dbl\textgreater{} 0.04570873, 0.11933069, 0.15384615, 0.29191557, 0....}
\CommentTok{\#\textgreater{} $ by\_bike          \textless{}dbl\textgreater{} 0.014344761, 0.019815059, 0.008547009, 0.071286340...}
\CommentTok{\#\textgreater{} $ by\_walk          \textless{}dbl\textgreater{} 0.5801118, 0.5808014, 0.6471306, 0.4066109, 0.3805...}
\CommentTok{\#\textgreater{} $ med\_house\_income \textless{}dbl\textgreater{} 2499, 21773, 36250, 34677, 30500, 28618, 16094, 19...}
\end{Highlighting}
\end{Shaded}

The reason this pipe works is because the output of each function call is yet another tibble and the pipe operator is passing that resultant tibble as the first argument to the next function.

Not only does the pipe aid in the manipulation of data, it also has a lot of utility in crafting ggplots. By piping your tibble into a ggplot call, this allows you to quickly iterate on the input data from either filtering down your data to creating new variables for visualization purposes.

The following sections will use the pipe operator in favor of the above listed alternatives.

\hypertarget{revisiting-our-scenario}{%
\section{Revisiting our scenario}\label{revisiting-our-scenario}}

Now that we have the pipe operator at our fingertips, we ought to think about how we can incorporate it into our previous work. In our earlier scenario we

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs\_raw \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/ACS\_1317\_TRACT.csv"}\NormalTok{)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{select}\NormalTok{(acs\_raw,}
\NormalTok{       county,}
\NormalTok{       hs\_grad, bach, master,}
       \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"commute"}\NormalTok{),}
       \KeywordTok{starts\_with}\NormalTok{(}\StringTok{"by"}\NormalTok{),}
\NormalTok{       med\_house\_income) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(county }\OperatorTok{\%in\%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"SUFFOLK"}\NormalTok{, }\StringTok{"NORFOLK"}\NormalTok{, }\StringTok{"MIDDLESEX"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\hypertarget{creating-new-measures}{%
\chapter{Creating new measures}\label{creating-new-measures}}

It's been a week now and the non-profit has finally emailed you back. They were ecstatic with what you provided but it begat even more questions for them. They indicated that while the median household income data was very intruiging, that would be difficult for them to report on. As such, they would like you to report on the income quintiles as well. Moreover, they also would like to see the rate of Bachelor's and Master's degrees combined into one general educational attainment variable.

This poses some challenges for you. You know \emph{what} is being asked, just not necessarily \emph{how} to achieve that from R. To accomplish this we're going to have to learn how to use the \texttt{dplyr::mutate()} function. For the sake of example, let's select only the columns that we're going to need and make a tibble called \texttt{df} just to work with.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/gba\_commute.csv"}\NormalTok{)}
\NormalTok{df \textless{}{-}}\StringTok{ }\KeywordTok{select}\NormalTok{(commute, med\_house\_income, bach, master)}

\NormalTok{df}
\CommentTok{\#\textgreater{} \# A tibble: 648 x 3}
\CommentTok{\#\textgreater{}    med\_house\_income  bach master}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1            75085 0.188 0.100 }
\CommentTok{\#\textgreater{}  2           132727 0.400 0.130 }
\CommentTok{\#\textgreater{}  3           110694 0.317 0.139 }
\CommentTok{\#\textgreater{}  4           109125 0.322 0.144 }
\CommentTok{\#\textgreater{}  5            76746 0.177 0.0742}
\CommentTok{\#\textgreater{}  6           138700 0.310 0.207 }
\CommentTok{\#\textgreater{}  7           104673 0.247 0.149 }
\CommentTok{\#\textgreater{}  8            73191 0.300 0.126 }
\CommentTok{\#\textgreater{}  9           121488 0.198 0.140 }
\CommentTok{\#\textgreater{} 10            99358 0.348 0.151 }
\CommentTok{\#\textgreater{} \# ... with 638 more rows}
\end{Highlighting}
\end{Shaded}

\texttt{mutate()} is a function that let's us create or modify variables. The arguments for \texttt{mutate()} are the same as those for \texttt{select()}---\texttt{.data} and \texttt{...}. In the case of \texttt{mutate()} dots works a littble bit differently. After indicating our data, we create columns by specifying a name-value pair. More simply the names of our arguments will be the name of the columns that we are creating. The value is any expression. For example we could use \texttt{mutate(df,\ one\ =\ 1)} to create a column called \texttt{one} with the value of 1. When using mutate, however, the result from the expression needs to be either only \emph{one} value, or as many values as there are rows.

If we take our \texttt{df}, we can add the columns \texttt{bach} and \texttt{master} together to create a new column called \texttt{edu\_attain}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mutate}\NormalTok{(df, }\DataTypeTok{edu\_attain =}\NormalTok{ bach }\OperatorTok{+}\StringTok{ }\NormalTok{master)}
\CommentTok{\#\textgreater{} \# A tibble: 648 x 4}
\CommentTok{\#\textgreater{}    med\_house\_income  bach master edu\_attain}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1            75085 0.188 0.100       0.288}
\CommentTok{\#\textgreater{}  2           132727 0.400 0.130       0.531}
\CommentTok{\#\textgreater{}  3           110694 0.317 0.139       0.456}
\CommentTok{\#\textgreater{}  4           109125 0.322 0.144       0.466}
\CommentTok{\#\textgreater{}  5            76746 0.177 0.0742      0.251}
\CommentTok{\#\textgreater{}  6           138700 0.310 0.207       0.516}
\CommentTok{\#\textgreater{}  7           104673 0.247 0.149       0.396}
\CommentTok{\#\textgreater{}  8            73191 0.300 0.126       0.426}
\CommentTok{\#\textgreater{}  9           121488 0.198 0.140       0.338}
\CommentTok{\#\textgreater{} 10            99358 0.348 0.151       0.499}
\CommentTok{\#\textgreater{} \# ... with 638 more rows}
\end{Highlighting}
\end{Shaded}

We could even think about ways that we can check if some observations are above some specified income threshold.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mutate}\NormalTok{(df, }\DataTypeTok{above\_70k\_inc =}\NormalTok{ med\_house\_income }\OperatorTok{\textgreater{}}\StringTok{ }\DecValTok{80000}\NormalTok{) }
\CommentTok{\#\textgreater{} \# A tibble: 648 x 4}
\CommentTok{\#\textgreater{}    med\_house\_income  bach master above\_70k\_inc}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{} \textless{}lgl\textgreater{}        }
\CommentTok{\#\textgreater{}  1            75085 0.188 0.100  FALSE        }
\CommentTok{\#\textgreater{}  2           132727 0.400 0.130  TRUE         }
\CommentTok{\#\textgreater{}  3           110694 0.317 0.139  TRUE         }
\CommentTok{\#\textgreater{}  4           109125 0.322 0.144  TRUE         }
\CommentTok{\#\textgreater{}  5            76746 0.177 0.0742 FALSE        }
\CommentTok{\#\textgreater{}  6           138700 0.310 0.207  TRUE         }
\CommentTok{\#\textgreater{}  7           104673 0.247 0.149  TRUE         }
\CommentTok{\#\textgreater{}  8            73191 0.300 0.126  FALSE        }
\CommentTok{\#\textgreater{}  9           121488 0.198 0.140  TRUE         }
\CommentTok{\#\textgreater{} 10            99358 0.348 0.151  TRUE         }
\CommentTok{\#\textgreater{} \# ... with 638 more rows}
\end{Highlighting}
\end{Shaded}

This function is immensly useful and can be combined with almost any expression to create new data for us. Furthermore there are a number of handy functions built into dplyr that help us create new columns. Some of these are \texttt{case\_when()}, \texttt{min\_rank()}, and \texttt{ntile()} among others. You can always explore these with \texttt{?function\_name()}. For our purposes, we will look at the use of \texttt{ntile()}.

\texttt{ntile()} is a function that will calculate percentiles for us. Given a column of data, \texttt{x}, and a number of buckets, \texttt{n}, we can create a new column of ranks. In our case, we are interested in calculating the quintile of \texttt{med\_house\_income}. This means we can provide \texttt{med\_house\_income} and \texttt{n\ =\ 5} as arguments to \texttt{ntile()} to group our observations by quintile.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mutate}\NormalTok{(df, }\DataTypeTok{inc\_quintile =} \KeywordTok{ntile}\NormalTok{(med\_house\_income, }\DecValTok{5}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 648 x 4}
\CommentTok{\#\textgreater{}    med\_house\_income  bach master inc\_quintile}
\CommentTok{\#\textgreater{}               \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}        \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1            75085 0.188 0.100             2}
\CommentTok{\#\textgreater{}  2           132727 0.400 0.130             5}
\CommentTok{\#\textgreater{}  3           110694 0.317 0.139             4}
\CommentTok{\#\textgreater{}  4           109125 0.322 0.144             4}
\CommentTok{\#\textgreater{}  5            76746 0.177 0.0742            2}
\CommentTok{\#\textgreater{}  6           138700 0.310 0.207             5}
\CommentTok{\#\textgreater{}  7           104673 0.247 0.149             4}
\CommentTok{\#\textgreater{}  8            73191 0.300 0.126             2}
\CommentTok{\#\textgreater{}  9           121488 0.198 0.140             5}
\CommentTok{\#\textgreater{} 10            99358 0.348 0.151             4}
\CommentTok{\#\textgreater{} \# ... with 638 more rows}
\end{Highlighting}
\end{Shaded}

Now we can put everything together into one mutate call to create the new variables that were requested!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{updated\_commute \textless{}{-}}\StringTok{ }\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{edu\_attain =}\NormalTok{ bach }\OperatorTok{+}\StringTok{ }\NormalTok{master, }
         \DataTypeTok{inc\_quintile =} \KeywordTok{ntile}\NormalTok{(med\_house\_income, }\DecValTok{5}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

As you have made these changes you can now write the data again to csv and share it. As this process becomes more and more iterative, it's good to put \emph{some} structure to the data so you have an idea of the history. One general practice that is good to get into is dating your files. So in this case I would label the file \texttt{yyyy-mm-dd-commute.csv}.

\hypertarget{data-structures}{%
\chapter{Data Structures}\label{data-structures}}

There is a topic I have been skirting around for some time now and I think it is time that we have to have a rather important conversation. It's one that is almost never fun but is quite necessary because without it, there may be many painful lessons learned in the future. We're going to spend this next chapter talking about data structures---but not all of them! We'll only cover the three most common and, by the end of this, it is my hope you will have a much stronger idea of \emph{what} you are working with and \emph{why} it behaves the way it does.

We will cover vectors, data frames rather briefly, and lists. We'll talk about some of their defining characteristics and how we can interact with them. Often the theory behind these object types are omitted, but I am of the mind that learning this early on will pay off in dividends. Take a deep breath before we dive in and remind yourself that it ain't nothin' but a thing.

This section is undoubtedly the most theoretically dense from a software perspective of this entire book. These concepts may be a little bit difficult to grasp at the first go around particularly if you do not have a programming background. But do not be discouraged! This is tough and there is no way to around it, so might as well go through it. If you can grasp this chapter programming in R will become so much easier. You will develop an intuition of why certain things happen to your data and how to interact with other data structures.

\hypertarget{atomic-vectors}{%
\section{Atomic Vectors}\label{atomic-vectors}}

I like to think of the atomic vector much like the atom---that is as the building block of any R object. You've actually been working with atomic vectors this entire time. But we haven't been very explicit about this yet. Up until this point we have been working mainly with tibbles. And here is the secret: each column of a tibble is \emph{actually} an atomic vector.

What makes a vector a atomic is that it can only be \textbf{a single data type} and that they are \emph{one-dimensional}---opposed to tibbles which are two-dimensional\footnote{Data Types and Structures. Software Carpentries. \url{https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/}}. You may have noticed that every value of a column is of the same data type. This means that they are rather strict to work with and for good reason. Imagine you wanted to multiple a column by 10, what would happen if a few of the values in the column were actually written out as text? Let's try exploring this idea.

The most common way to create a vector in R is to use the \texttt{c()} function. This stands for \emph{combine}. We can \texttt{c}ombine as many elements as we want into a single vector using \texttt{c()}. Each element of the vector is it's own argument (separated by a comma).

For example if we wanted to create a vector of \href{https://data.bls.gov/timeseries/LAUMT257165000000003?amp\%253bdata_tool=XGtable\&output_view=data\&include_graphs=true}{Boston's unemployment rate} rate for each month in 2019 that we have data for (until October as of this writing on Dec.~18th, 2019) we could write the below. We will save it in a a vector called \texttt{unemp}\footnote{Boston Unemployment \url{https://data.bls.gov/timeseries/LAUMT257165000000003?amp\%253bdata_tool=XGtable\&output_view=data\&include_graphs=true}.}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{3.2}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{2.4}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{2.9}\NormalTok{, }\FloatTok{2.7}\NormalTok{, }\FloatTok{2.6}\NormalTok{, }\FloatTok{2.7}\NormalTok{, }\FloatTok{2.3}\NormalTok{)}
\NormalTok{unemp}
\CommentTok{\#\textgreater{}  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3}
\end{Highlighting}
\end{Shaded}

What is really great about vectors is that we can perform any number of operations on them---i.e.~find the sum of all the values, the average, add a value to each element, etc.

If we wanted to find the average unemployment rate for Boston for Jan - Oct.~2019, we can supply the vector to the function \texttt{mean()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(unemp)}
\CommentTok{\#\textgreater{} [1] 2.72}
\end{Highlighting}
\end{Shaded}

However, you may be thinking ``there are 12 months in a year not 10 and that should be represented'' and if you are, I totally agree with you. Since the data for November and December are missing, we should denote that and update \texttt{unemp} accordingly. R uses \texttt{NA} to represent missing data. To represent this we can append two \texttt{NA}s to the vector we have. There are two ways we can do this. We can either combine \texttt{unemp} with two \texttt{NA}s, or rewrite the above vector.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# combining existing with 2 NAs}
\KeywordTok{c}\NormalTok{(unemp, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\CommentTok{\#\textgreater{}  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA}
\end{Highlighting}
\end{Shaded}

This works, but since we will be saving this to \texttt{unemp} again it is not best practices to use the variable you are changing in that objects assignment.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# for example}
\NormalTok{unemp \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(unemp, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The above is rather unclear and might confuse someone that will have to read your code at a later time---that person may even be you. For this reason we will redefine it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\FloatTok{3.2}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{2.4}\NormalTok{, }\FloatTok{2.8}\NormalTok{, }\FloatTok{2.9}\NormalTok{, }\FloatTok{2.7}\NormalTok{, }\FloatTok{2.6}\NormalTok{, }\FloatTok{2.7}\NormalTok{, }\FloatTok{2.3}\NormalTok{, }\OtherTok{NA}\NormalTok{, }\OtherTok{NA}\NormalTok{)}
\NormalTok{unemp}
\CommentTok{\#\textgreater{}  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA}
\end{Highlighting}
\end{Shaded}

We know that there are 12 elements in this vector, but sometimes it is quite nice to sanity check oneself. We can always find out how long (or how many elements are in) a vector is by supplying the vector to the \texttt{length()} function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# how many observations are in \textasciigrave{}unemp\textasciigrave{}?}
\KeywordTok{length}\NormalTok{(unemp)}
\CommentTok{\#\textgreater{} [1] 12}
\end{Highlighting}
\end{Shaded}

There are a total of six types of vectors. Fortunately, only four of these really matter to us. These are \texttt{integer}, \texttt{double}, \texttt{character}, \texttt{logical}.

Integers represent whole numbers. To specify an integer we append an \texttt{L} after the number such as \texttt{20L}. Doubles are any number that requires any precision aka decimal places. You can specify doubles in a number of formats such as scientific notation. Generally the easiest way to do this, though, is using a decimal. Together integers and doubles are lumped into the category of numeric. This is because, well, they are numbers.

As you learned previously, character vectors are created with the use of quotation marks; either \texttt{"} or \texttt{\textquotesingle{}}.

We've already created a vector of type double, \texttt{unemp}. You can check what type of vector \texttt{unemp} is with \texttt{typeof()}

\begin{quote}
Note: \texttt{typeof()} is used only for internal R object such as lists and vectors. In most cases you will want to use \texttt{class()} to return the class of an object.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{typeof}\NormalTok{(unemp)}
\CommentTok{\#\textgreater{} [1] "double"}
\end{Highlighting}
\end{Shaded}

Say we create another vector called \texttt{month} with the numbers 1 through 12.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{month \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{3}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{5}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{7}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{11}\NormalTok{, }\DecValTok{12}\NormalTok{)}

\KeywordTok{typeof}\NormalTok{(month)}
\CommentTok{\#\textgreater{} [1] "double"}
\end{Highlighting}
\end{Shaded}

Notice that since we didn't specify the \texttt{L} after the numbers R defaulted to treating \texttt{month} as a double. When possible it is good to make the distinction between integer and numeric.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{month \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L)}

\KeywordTok{typeof}\NormalTok{(month)}
\CommentTok{\#\textgreater{} [1] "integer"}
\end{Highlighting}
\end{Shaded}

R has a number of vectors that are built in these being the letters of the alphabet (\texttt{letters} and \texttt{LETTERS} respectively), as well as \texttt{month.abb}, \texttt{month.name}, and \texttt{pi}. \texttt{month.name} is already available to us so let's not recreate it.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{month.name}
\CommentTok{\#\textgreater{}  [1] "January"   "February"  "March"     "April"     "May"       "June"     }
\CommentTok{\#\textgreater{}  [7] "July"      "August"    "September" "October"   "November"  "December"}

\KeywordTok{typeof}\NormalTok{(month.name)}
\CommentTok{\#\textgreater{} [1] "character"}
\end{Highlighting}
\end{Shaded}

Notice the quotes around each vector element. This is how we identify character vectors.

Logical vectors are the last kind of vector we need to go over. Logical vectors are represented as the values \texttt{TRUE} and \texttt{FALSE}. Simple enough. Onward!

Recall that vectors are atomic meaning that there can only be one type per vector and we cannot mix and match. When a character is in the presence of another element of a different type, that value is \emph{\textbf{coerced}} into a character. Coersion is the process of implicitly or contextually changing an object from one type to another. For example:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"a"}\NormalTok{, }\DecValTok{1}\NormalTok{)}

\KeywordTok{typeof}\NormalTok{(x)}
\CommentTok{\#\textgreater{} [1] "character"}
\end{Highlighting}
\end{Shaded}

Something similar happens when a logical value is in the presence of a numeric value

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\DecValTok{1}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 1 1 0}
\end{Highlighting}
\end{Shaded}

In the presence of a numeric value \texttt{TRUE} becomes equal to \texttt{1L} and \texttt{FALSE} equal to \texttt{0L}.This behavior exists whenever a logical value is presented where a numeric is expected such as the function call below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{sum}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 1}
\end{Highlighting}
\end{Shaded}

While coersion occurs from other processes like combining values in a vector, \emph{\textbf{casting}} is the process of intentionally changing an object's class. There are a number of casting functions whice generaly take the shape of \texttt{as.class()} or \texttt{as\_class()}. Each of the vector types covered have their own casting functions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as.integer}\NormalTok{(}\OtherTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 1}
\KeywordTok{as.character}\NormalTok{(}\DecValTok{123}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "123"}
\KeywordTok{as.double}\NormalTok{(}\StringTok{"2.331"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 2.331}
\KeywordTok{as.logical}\NormalTok{(}\DecValTok{0}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] FALSE}
\end{Highlighting}
\end{Shaded}

As you progress in your R journey you will find scenarios in which you need to cast objects from one class to another and these functions are the trick.

You now have a strong understanding of the underbellies of R vectors. One thing that is missing is an understanding of how we can select subsets from vectors. To extract a value from vectors we append square brackets at the end of the vector \texttt{vec{[}{]}}. We supply an index value to the square brackets to receive the value at that position

To select the month of January from the \texttt{unemp} vector, the first element, we provide the value of \texttt{1} to the brackets.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp[}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 3.2}
\end{Highlighting}
\end{Shaded}

To extract more than one value, we provide a vector of the row indexes we desire.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp[}\KeywordTok{c}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{3}\NormalTok{)]}
\CommentTok{\#\textgreater{} [1] 3.2 2.8}
\end{Highlighting}
\end{Shaded}

There is yet another way to extract values from these vectors. We can provide a logical vector to our square brackets. For example, we can identify every value of \texttt{unemp} that is above the average rate.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# find average removing missing values}
\NormalTok{avg\_unemp \textless{}{-}}\StringTok{ }\KeywordTok{mean}\NormalTok{(unemp, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}

\CommentTok{\# identify which values are above average}
\NormalTok{index \textless{}{-}}\StringTok{ }\NormalTok{unemp }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{avg\_unemp}

\NormalTok{index}
\CommentTok{\#\textgreater{}  [1]  TRUE  TRUE  TRUE FALSE  TRUE  TRUE FALSE FALSE FALSE FALSE    NA    NA}
\end{Highlighting}
\end{Shaded}

Notice that the \texttt{NA}s stayed \texttt{NA}? They can be pesky. Hadley writes in Advanced R ``missing values tend to be infectious: most computations involving a missing value will return another missing value.''\footnote{Vectors. Advanced R. \url{https://adv-r.hadley.nz/vectors-chap.html}.}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp[index]}
\CommentTok{\#\textgreater{} [1] 3.2 2.8 2.8 2.8 2.9  NA  NA}
\end{Highlighting}
\end{Shaded}

How annoying those NAs can be! To prevent these NAs from showing upwe can add another condition to our \texttt{index} line to remove NAs. Like there are \texttt{as.*()} functions for casting, there are also \texttt{is.*()} functions for testing. \texttt{is.*()} returns a logical vector of the same length as the provided vector.

\begin{quote}
Note: the \texttt{*} is called a wildcard. The wildcard character comes from SQL and when present means that any string can follow. \texttt{is.*()} is intended to indicate any possible testing function such as \texttt{is.numeric()}, \texttt{is\_tibble()}, etc.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{is.na}\NormalTok{(unemp)}
\CommentTok{\#\textgreater{}  [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE  TRUE  TRUE}
\end{Highlighting}
\end{Shaded}

As we learned, we can negate logical vectors with an \texttt{!}. We can negate the test results and include an an \texttt{\&} condition to only identify unemployment values above average \emph{and} aren't missing.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index \textless{}{-}}\StringTok{ }\NormalTok{unemp }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{avg\_unemp }\OperatorTok{\&}\StringTok{ }\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(unemp)}

\NormalTok{unemp[index]}
\CommentTok{\#\textgreater{} [1] 3.2 2.8 2.8 2.8 2.9}
\end{Highlighting}
\end{Shaded}

There is one last thing to keep in mind and with subsetting vectors using a logical vector that is of a different length. When you use a logical vector to subset and they are of differing length, the logical vector will be recycled for the remaining values of the vector being subset. As always, an example will be the best.

Say we have an object called \texttt{x} which are the values from 0 to 10 and an \texttt{index} to subset with. If we subset it with \texttt{index} and \texttt{index} is a logical vector of length two with the values of \texttt{TRUE} and \texttt{FALSE}, every other observation will be returned. This is because come the third value in \texttt{x}, R has ran out of values in \texttt{index} to use so it goes back to the beginning

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x \textless{}{-}}\StringTok{ }\DecValTok{0}\OperatorTok{:}\DecValTok{10}
\NormalTok{x}
\CommentTok{\#\textgreater{}  [1]  0  1  2  3  4  5  6  7  8  9 10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{index \textless{}{-}}\StringTok{ }\KeywordTok{c}\NormalTok{(}\OtherTok{TRUE}\NormalTok{, }\OtherTok{FALSE}\NormalTok{)}

\NormalTok{x[index]}
\CommentTok{\#\textgreater{} [1]  0  2  4  6  8 10}
\end{Highlighting}
\end{Shaded}

And what happens when the only value is a single logical value?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OtherTok{TRUE}\NormalTok{]}
\CommentTok{\#\textgreater{}  [1]  0  1  2  3  4  5  6  7  8  9 10}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x[}\OtherTok{FALSE}\NormalTok{]}
\CommentTok{\#\textgreater{} integer(0)}
\end{Highlighting}
\end{Shaded}

In this latter case see how the output says \texttt{integer(0)}. This is informing you that the vector contains 0 elements.

\hypertarget{data-frames}{%
\section{Data Frames}\label{data-frames}}

The entirety of the work in this book so far has been with \texttt{tibbles}. Tibbles are actually a special type of data frame. Data frames are R's native way for storing rectangular data. Rectangles are two-dimensional, so are data frames.

Data frames are secretly just a bunch of vectors squished together. The important thing is that all vectors are of the same length. This ensures that each observation (row) has one value from each vector. Because of the nature of a data frame, each column must adhere to the rules of vectors.

Let's create a tibble using the \texttt{unemp} vector and the \texttt{tibble()} function. \texttt{tibble()} works in a somewhat similar manner as \texttt{mutate()} where the arguments we provide are name value pairs. In the case of tibble, the argument take the form of \texttt{col\_name\ =\ vector}.

We create a tibble with the unemployment rate below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(dplyr)}

\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{unemp\_rate =}\NormalTok{ unemp}
\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 1}
\CommentTok{\#\textgreater{}    unemp\_rate}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1        3.2}
\CommentTok{\#\textgreater{}  2        2.8}
\CommentTok{\#\textgreater{}  3        2.8}
\CommentTok{\#\textgreater{}  4        2.4}
\CommentTok{\#\textgreater{}  5        2.8}
\CommentTok{\#\textgreater{}  6        2.9}
\CommentTok{\#\textgreater{}  7        2.7}
\CommentTok{\#\textgreater{}  8        2.6}
\CommentTok{\#\textgreater{}  9        2.7}
\CommentTok{\#\textgreater{} 10        2.3}
\CommentTok{\#\textgreater{} 11       NA  }
\CommentTok{\#\textgreater{} 12       NA}
\end{Highlighting}
\end{Shaded}

We can add the month name and create a new column to indicate if that month has a higher than average unemployment rate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl \textless{}{-}}\StringTok{ }\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{unemp\_rate =}\NormalTok{ unemp, }
  \DataTypeTok{month =}\NormalTok{ month.name}
\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{above\_avg =}\NormalTok{ unemp\_rate }\OperatorTok{\textgreater{}}\StringTok{ }\NormalTok{avg\_unemp)}

\NormalTok{unemp\_tbl}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    unemp\_rate month     above\_avg}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}lgl\textgreater{}    }
\CommentTok{\#\textgreater{}  1        3.2 January   TRUE     }
\CommentTok{\#\textgreater{}  2        2.8 February  TRUE     }
\CommentTok{\#\textgreater{}  3        2.8 March     TRUE     }
\CommentTok{\#\textgreater{}  4        2.4 April     FALSE    }
\CommentTok{\#\textgreater{}  5        2.8 May       TRUE     }
\CommentTok{\#\textgreater{}  6        2.9 June      TRUE     }
\CommentTok{\#\textgreater{}  7        2.7 July      FALSE    }
\CommentTok{\#\textgreater{}  8        2.6 August    FALSE    }
\CommentTok{\#\textgreater{}  9        2.7 September FALSE    }
\CommentTok{\#\textgreater{} 10        2.3 October   FALSE    }
\CommentTok{\#\textgreater{} 11       NA   November  NA       }
\CommentTok{\#\textgreater{} 12       NA   December  NA}
\end{Highlighting}
\end{Shaded}

To interact with the underlying vector of a data frame we can use the dollar sign \texttt{\$} operator. This takes the form of \texttt{tbl\$col\_name}.

For example, extracting the \texttt{unemp\_rate} column looks like:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl}\OperatorTok{$}\NormalTok{unemp\_rate}
\CommentTok{\#\textgreater{}  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA}
\end{Highlighting}
\end{Shaded}

Note the difference between \texttt{select(tbl,\ col)} and \texttt{tbl\$col}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{select}\NormalTok{(unemp\_tbl, unemp\_rate)}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 1}
\CommentTok{\#\textgreater{}    unemp\_rate}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1        3.2}
\CommentTok{\#\textgreater{}  2        2.8}
\CommentTok{\#\textgreater{}  3        2.8}
\CommentTok{\#\textgreater{}  4        2.4}
\CommentTok{\#\textgreater{}  5        2.8}
\CommentTok{\#\textgreater{}  6        2.9}
\CommentTok{\#\textgreater{}  7        2.7}
\CommentTok{\#\textgreater{}  8        2.6}
\CommentTok{\#\textgreater{}  9        2.7}
\CommentTok{\#\textgreater{} 10        2.3}
\CommentTok{\#\textgreater{} 11       NA  }
\CommentTok{\#\textgreater{} 12       NA}
\end{Highlighting}
\end{Shaded}

The difference is that \texttt{\$} returns the underlying vector whereas \texttt{select()} will always return another data frame. You now have the ability to both filter data and grab a subset of a vector. But we have yet to visit how to grab a single value from a data frame.

You could try something like

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   unemp\_rate}
\CommentTok{\#\textgreater{}        \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1        2.3}
\end{Highlighting}
\end{Shaded}

To grab the 10th value of the first column. But again, you still have a tibble and you are not able to use that directly like a standalone number.

We can again use brackets to subset the our R object. But data frames are two dimensional, so we need to specify the indexes in two dimensions. If you have made a hand drawn graph used a cartesian plane, which I assume you all have, this will is the same idea. With a cartesian plane we can identify any point with a combination of two values: x and y. x refers to the horizontal axis and y the vertical axis. When we put the cartesian plane in the same frame of reference as the rectangular data frame we envision our rows as the x and our columns as the y.

In specifying our index, we are able to select all rows or all columns by leaving the x or y spot empty respectively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl[,}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 1}
\CommentTok{\#\textgreater{}    unemp\_rate}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1        3.2}
\CommentTok{\#\textgreater{}  2        2.8}
\CommentTok{\#\textgreater{}  3        2.8}
\CommentTok{\#\textgreater{}  4        2.4}
\CommentTok{\#\textgreater{}  5        2.8}
\CommentTok{\#\textgreater{}  6        2.9}
\CommentTok{\#\textgreater{}  7        2.7}
\CommentTok{\#\textgreater{}  8        2.6}
\CommentTok{\#\textgreater{}  9        2.7}
\CommentTok{\#\textgreater{} 10        2.3}
\CommentTok{\#\textgreater{} 11       NA  }
\CommentTok{\#\textgreater{} 12       NA}
\NormalTok{unemp\_tbl[}\DecValTok{10}\NormalTok{,]}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 3}
\CommentTok{\#\textgreater{}   unemp\_rate month   above\_avg}
\CommentTok{\#\textgreater{}        \textless{}dbl\textgreater{} \textless{}chr\textgreater{}   \textless{}lgl\textgreater{}    }
\CommentTok{\#\textgreater{} 1        2.3 October FALSE}
\end{Highlighting}
\end{Shaded}

To replicate the above tidyverse example we would provide the indexes 10 and 1 respectively.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl[}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{]}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}   unemp\_rate}
\CommentTok{\#\textgreater{}        \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1        2.3}
\end{Highlighting}
\end{Shaded}

This is great, we've rewritten our tidyverse code in base R. But, just like the tidyverse code, we maintain the tibble data structure. This is because when we use a single bracket, it maintains the data structure of the object we are selecting from. If we wrap our brackets in another set of bracket, we are returned the an object of the same class as the underlying object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl[[}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{]]}
\CommentTok{\#\textgreater{} [1] 2.3}
\end{Highlighting}
\end{Shaded}

What that code is doing is narrowing the tibble down to a single column with a single row index and then extracting the underlying vector (the second bracket). To extract the underlying vector using the tidyverse, we can use the function \texttt{dplyr::pull()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{unemp\_tbl }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(}\DecValTok{1}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{10}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{pull}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] 2.3}
\end{Highlighting}
\end{Shaded}

Now this brings us to the second-most fundamental structure in R: the list. Yes, second-most fundamental. I've been keeping a secret from you. Data frames are actually just lists in disguise. To prove it, I will remove the class from \texttt{unemp\_tbl} and return the class of that unclassed object.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{unclass}\NormalTok{(unemp\_tbl) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{class}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "list"}
\end{Highlighting}
\end{Shaded}

That is right, data frames are actually just lists disguised as rectangles.

\hypertarget{lists}{%
\section{Lists}\label{lists}}

There is a good chance that you will not have to interact with them too often That doesn't mean you shouldn't know how to when that time comes.

Lists are generally the most flexible object type in R. Unlike vectors and data frames lists do not impose any structure on the storage of our data.

The most simple lists may resemble something like a vector.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{list}\NormalTok{(}\StringTok{"Jan"}\NormalTok{, }\StringTok{"Feb"}\NormalTok{, }\StringTok{"Mar"}\NormalTok{)}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{} [1] "Jan"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[2]]}
\CommentTok{\#\textgreater{} [1] "Feb"}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} [[3]]}
\CommentTok{\#\textgreater{} [1] "Mar"}
\end{Highlighting}
\end{Shaded}

Notice how this prints differently than

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\StringTok{"Jan"}\NormalTok{, }\StringTok{"Feb"}\NormalTok{, }\StringTok{"Mar"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "Jan" "Feb" "Mar"}
\end{Highlighting}
\end{Shaded}

Each element of a list is self-contained. I think of lists somewhat like shipping containers where each element is its own container and all components of each element are together. We can include any type of R object in a list. For example, we can include the \texttt{unemp\_tbl} and associated vectors.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l \textless{}{-}}\StringTok{ }\KeywordTok{list}\NormalTok{(unemp\_tbl, unemp, month.name)}
\end{Highlighting}
\end{Shaded}

We can view the structure of the list to get an idea of what is actually contained by that list.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{str}\NormalTok{(l)}
\CommentTok{\#\textgreater{} List of 3}
\CommentTok{\#\textgreater{}  $ : tibble [12 x 3] (S3: tbl\_df/tbl/data.frame)}
\CommentTok{\#\textgreater{}   ..$ unemp\_rate: num [1:12] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 ...}
\CommentTok{\#\textgreater{}   ..$ month     : chr [1:12] "January" "February" "March" "April" ...}
\CommentTok{\#\textgreater{}   ..$ above\_avg : logi [1:12] TRUE TRUE TRUE FALSE TRUE TRUE ...}
\CommentTok{\#\textgreater{}  $ : num [1:12] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 ...}
\CommentTok{\#\textgreater{}  $ : chr [1:12] "January" "February" "March" "April" ...}
\end{Highlighting}
\end{Shaded}

The structure of \texttt{l} shows us that the first element is a tibble (has class \texttt{tbl\_df}), and the other elements are numeric and character vectors respectively.

Because of this flexibility there are not predetermined dimensions that we can specify to our brackets. Like extracting the underlying vector value from a data frame we have to use \texttt{{[}{[}} for indexing. I like to think of \texttt{{[}} as walking up to the storage container and \texttt{{[}{[}} as actually opening it up and going inside. To get a sense of the difference lets look at the \texttt{unemp} vector.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[}\DecValTok{2}\NormalTok{]}
\CommentTok{\#\textgreater{} [[1]]}
\CommentTok{\#\textgreater{}  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA}
\KeywordTok{class}\NormalTok{(l[}\DecValTok{2}\NormalTok{])}
\CommentTok{\#\textgreater{} [1] "list"}
\end{Highlighting}
\end{Shaded}

When using the single bracket we are just selecting the first element of the list which is why we are returned another list.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[[}\DecValTok{2}\NormalTok{]]}
\CommentTok{\#\textgreater{}  [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3  NA  NA}
\KeywordTok{class}\NormalTok{(l[[}\DecValTok{2}\NormalTok{]])}
\CommentTok{\#\textgreater{} [1] "numeric"}
\end{Highlighting}
\end{Shaded}

When we use the double bracket we are going inside of the container and actually plucking that element out of the list. Once you have plucked out that element, we can again use another set of brackets to subset that item. To grab the tenth row and first column of the \texttt{unemp\_tbl} inside of \texttt{l} we can write.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{l[[}\DecValTok{1}\NormalTok{]][[}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{]]}
\CommentTok{\#\textgreater{} [1] 2.3}
\end{Highlighting}
\end{Shaded}

Now that we know that data frames are lists we can actually extract the underlying vectors using \texttt{{[}{[}} as well as \texttt{\$}. We can get the tenth row and first column a number of ways.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# subsetting the data frame}
\NormalTok{l[[}\DecValTok{1}\NormalTok{]][[}\DecValTok{10}\NormalTok{,}\DecValTok{1}\NormalTok{]]}
\CommentTok{\#\textgreater{} [1] 2.3}

\CommentTok{\# grabbing the first vector then position}
\NormalTok{l[[}\DecValTok{1}\NormalTok{]][[}\DecValTok{1}\NormalTok{]][}\DecValTok{10}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 2.3}

\CommentTok{\# grabbing the vector by name then position}
\NormalTok{l[[}\DecValTok{1}\NormalTok{]]}\OperatorTok{$}\NormalTok{unemp\_rate[}\DecValTok{10}\NormalTok{]}
\CommentTok{\#\textgreater{} [1] 2.3}
\end{Highlighting}
\end{Shaded}

Frankly all of these brackets can get a little messy. The tidyverse package \texttt{purrr} has a super handy function called \texttt{pluck()} which handles all of these brackets for us. \texttt{purrr::pluck()} is meant for flexible indexing into data structures (documentation).

\texttt{pluck()} works by first providing the object that you'd like to index---again, notice the data first emphasis---and then providing the position of the element you would like to pluck out of the object. Generally, I will use \texttt{pluck()} when possible. By doing so the code becomes more readable and adheres to a single style more thoroughly.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{purrr}\OperatorTok{::}\KeywordTok{pluck}\NormalTok{(l, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{10}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 2.3}
\end{Highlighting}
\end{Shaded}

Congratulations! You made it to the end of this exceptionally dense chapter. You may feel a little overwhlemed and that is to be expected. Nonetheless you should be proud! I have a few more asks of you before you move on.

\hypertarget{exercises}{%
\subsection{Exercises}\label{exercises}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Drink some water
\item
  Move around a bit and shake it out
\item
  Create a list with the vectors \texttt{unemp}, \texttt{month.name}, and \texttt{avg\_unemp}.
\item
  Recreate the \texttt{unemp\_tbl} but referencing the list elements
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(purrr)}

\NormalTok{unemp\_l \textless{}{-}}\StringTok{ }\KeywordTok{list}\NormalTok{(unemp, month.name, avg\_unemp)}

\KeywordTok{tibble}\NormalTok{(}
  \DataTypeTok{unemp\_rate =} \KeywordTok{pluck}\NormalTok{(unemp\_l, }\DecValTok{1}\NormalTok{),}
  \DataTypeTok{month =} \KeywordTok{pluck}\NormalTok{(unemp\_l, }\DecValTok{2}\NormalTok{)}
\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{above\_avg =}\NormalTok{ unemp\_rate }\OperatorTok{\textgreater{}}\StringTok{ }\KeywordTok{pluck}\NormalTok{(unemp\_l, }\DecValTok{3}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 12 x 3}
\CommentTok{\#\textgreater{}    unemp\_rate month     above\_avg}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{} \textless{}chr\textgreater{}     \textless{}lgl\textgreater{}    }
\CommentTok{\#\textgreater{}  1        3.2 January   TRUE     }
\CommentTok{\#\textgreater{}  2        2.8 February  TRUE     }
\CommentTok{\#\textgreater{}  3        2.8 March     TRUE     }
\CommentTok{\#\textgreater{}  4        2.4 April     FALSE    }
\CommentTok{\#\textgreater{}  5        2.8 May       TRUE     }
\CommentTok{\#\textgreater{}  6        2.9 June      TRUE     }
\CommentTok{\#\textgreater{}  7        2.7 July      FALSE    }
\CommentTok{\#\textgreater{}  8        2.6 August    FALSE    }
\CommentTok{\#\textgreater{}  9        2.7 September FALSE    }
\CommentTok{\#\textgreater{} 10        2.3 October   FALSE    }
\CommentTok{\#\textgreater{} 11       NA   November  NA       }
\CommentTok{\#\textgreater{} 12       NA   December  NA}
\end{Highlighting}
\end{Shaded}

\hypertarget{summary-statistics}{%
\chapter{Summary statistics}\label{summary-statistics}}

The last chapter we focused on the underlying data structures that we interact with in R. Most importantly we covered the atomic vector data structure and learned that the columns of a tibble are vectors. When we have been using \texttt{mutate()} to create new columns, we have actually been creating other vectors. When we have filtered data, we have checked the values of the underlying vectors to see if they have matched our specified conditions. Moving forward, we will begin to think about ways of summarizing our data. To do so we will be working with vectors more often. Being able to understand the role a vector plays in data frame operations will make this learning process even easier.

Let us start by asking the question \emph{``what is a statistic?''} Very simply a statistic is a single number that is used to characterize a sample of data. Most often we see statistics used to describe central tendancy and spread---measures like the mean and standard deviation. However, the ways in which we can characterize a sample of data are not restricted to traditional frequentists statistics. We want to be more creative in the ways that we look at our data. In addition to evaluating central tendancy and spread we may find ourselves looking at the average word counts of tweets, or distances from the Boston Common, and so much more.

When we begin to summarize data, we are taking all observations or maybe a subset of observations and calculating one value to represent that sample. This is very important framing to have. Whenever we want to create an aggregate measure of our data there must be only one observation per-subset. Meaning, if you have a data frame with 150 rows and 3 groups within that, there ought to be only three resultant observations---though there may be many more variables.

Let us revisit the \texttt{commute}, and specifically the rate of commuters who travel between 30 and 60 minutes.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/gba\_commute.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

It is of course of interest to identify the average rate of 30-60 minute commuting, as well as the standard deviation, and median. What does this look like and how is it done? Prior to measuring central tendency and spread, we begin by visualizing our data. The purpose of visualizing your data before hand is to give you an intuition of how it may behave and its shape.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(commute3060)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{20}\NormalTok{)}
\CommentTok{\#\textgreater{} Warning: Removed 6 rows containing non{-}finite values (stat\_bin).}
\end{Highlighting}
\end{Shaded}

\includegraphics{03e-summary-stats_files/figure-latex/unnamed-chunk-3-1.pdf}

In the above histogram we can intuit a number of things. The distribution looks fairly normally distributed meaning that both the mean and median are likely to be close to eachother and are equally sound measures of center, most likely somewhere around 0.3. Secondly, due to the distribution's rather round shape (or ``fat tails''), it can be expected to have a rather large standard deviation. Once the intuition has been developed, one should calculate the relevant statistics to quantify these characteristics.

Let's calculate the mean, median, standard deviation, and range of this single variable. When calculating statistics like the mean and standard deviation, we are calculating univariate statistics and as such, working with only one column (variable) at a time---this tends to be the case almost always.

We will first extract the \texttt{commute3060} column as a vector using \texttt{dplyr::pull()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ commute\_rate \textless{}{-}}\StringTok{ }\KeywordTok{pull}\NormalTok{(commute, commute3060)}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Exercise: Read the help documentation for the functions \texttt{mean()}, \texttt{median()}, and \texttt{sd()} to get a sense of how these functions work. Calculate the mean, median, and standard deviation of \texttt{commute\_rate}.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(commute\_rate)}
\CommentTok{\#\textgreater{} [1] NA}

\KeywordTok{median}\NormalTok{(commute\_rate)}
\CommentTok{\#\textgreater{} [1] NA}

\KeywordTok{sd}\NormalTok{(commute\_rate)}
\CommentTok{\#\textgreater{} [1] NA}
\end{Highlighting}
\end{Shaded}

The results of these functions bring good and bad news. The good news is that their output is a single value. The bad news is that the output is \texttt{NA}. To reiterate a previous point, \texttt{NA} will infect your analyses. The only way to get around this is to perform these calculations without them.

Note that each of the functions used above have an argument called \texttt{na.rm}. \texttt{na.rm} tells R to remove the NAs prior to calculation. We can recalculate these statistics with the \texttt{na.rm} argument set to \texttt{TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{mean}\NormalTok{(commute\_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.3896481}

\KeywordTok{median}\NormalTok{(commute\_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.3921988}

\KeywordTok{sd}\NormalTok{(commute\_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.08624757}
\end{Highlighting}
\end{Shaded}

Let us look at one last example: identifying the range. The \texttt{range()} function returns the minimum and the maximum values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(commute\_range \textless{}{-}}\StringTok{ }\KeywordTok{range}\NormalTok{(commute\_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{))}
\CommentTok{\#\textgreater{} [1] 0.0000000 0.6327961}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: by wrapping an assignment in parentheses, the resultant object will be printed to the console.
\end{quote}

\texttt{range()} returned two values. This can be verified with the \texttt{length()} function.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{length}\NormalTok{(commute\_range)}
\CommentTok{\#\textgreater{} [1] 2}
\end{Highlighting}
\end{Shaded}

A length two vector does not adhere to providing just out value. We will see why this is a problem illustrated in the next chapter. To recreate this, use the \texttt{min()} and \texttt{max()} functions.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{min}\NormalTok{(commute\_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0}
\KeywordTok{max}\NormalTok{(commute\_rate, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] 0.6327961}
\end{Highlighting}
\end{Shaded}

Each of these statistics---mean, median, standard deviation, etc---are a way to characterize a larger sample of data. The lesson to take away here is that we will always need a single value when summarising data. Often we will be taking a column (vector) and calculating a single metric from that.

In the following chapter we will learn how to calculate summary statistics using the tidyverse.

\hypertarget{summarizing-the-tidy-way}{%
\chapter{Summarizing the tidy way}\label{summarizing-the-tidy-way}}

Now that we have a basic understanding of how to manipulate our dataset, summarising the dataset into a few useful metrics is important. When we have massive datasets with many subgroups, summary statistics will be very important for distilling all of that information into something consumable. Aggregation will also be very important for visualization purposes.

We have already reviewed what constitutes a summary statistic and how to create them working with vectors. But we have not done so within the context of the tidyverse. We have figured out how to select, filter, mutate and all within a chain of functions. But we have not followed this to its natural next step, the \texttt{group\_by()} and \texttt{summarise()} functions.

dplyr incluides a wonderful helper function called \texttt{count()}. It does just what it says it does. It counts the number of observations in a tibble. Let's recreate the \texttt{commute} tibble and see it for ourselves.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/gba\_commute.csv"}\NormalTok{) }

\KeywordTok{count}\NormalTok{(commute)}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}       n}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1   648}
\end{Highlighting}
\end{Shaded}

We can also count by groups in a data set. For example, we can count how many observations there are per county.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(commute, county)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   county        n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 MIDDLESEX   318}
\CommentTok{\#\textgreater{} 2 NORFOLK     130}
\CommentTok{\#\textgreater{} 3 SUFFOLK     200}
\end{Highlighting}
\end{Shaded}

\texttt{count()} is actually a wrapper around the function \texttt{summarise()} which is a much more flexible function. \texttt{summarise()} is the aggregate analog to \texttt{mutate()}. The difference between \texttt{mutate()} and \texttt{summarise()} is that the result of an expression in \texttt{mutate()} must have the same number of values as there are rows---unless of course you are specifying a scalar value like \texttt{TRUE}---whereas \texttt{summarise()} requires the result to be one an element of length one.

\begin{quote}
Notes:
- A wrapper is function that executes another function.
- A scalar is a vector of length one.
\end{quote}

We can recreate the first above \texttt{count()} call with \texttt{summarise()} and the handy \texttt{n()} function we learned a while ago. Here we follow the same pattern of assigning column names to expressions as we do with.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summarise}\NormalTok{(commute, }\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 1}
\CommentTok{\#\textgreater{}       n}
\CommentTok{\#\textgreater{}   \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1   648}
\end{Highlighting}
\end{Shaded}

Like \texttt{mutate()} there is no restriction on the number of new columns we can create. Previously we calculated the min, max, mean, and standard deviation of the \texttt{commute3060} variable. This is done rather neatly with \texttt{summarise()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{min\_commute =} \KeywordTok{min}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{max\_commute =} \KeywordTok{max}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{avg\_commute =} \KeywordTok{mean}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{sd\_commute  =} \KeywordTok{sd}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{    )}
\CommentTok{\#\textgreater{} \# A tibble: 1 x 4}
\CommentTok{\#\textgreater{}   min\_commute max\_commute avg\_commute sd\_commute}
\CommentTok{\#\textgreater{}         \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1           0       0.633       0.390     0.0862}
\end{Highlighting}
\end{Shaded}

Frankly this alone is somewhat unimpressive. The power of \texttt{summarise()} comes from incorporating \texttt{group\_by()} into the function chain. \texttt{group\_by()} allows us to explicitly identify groups within a tibble as defined by a given variable. The resulting tibble from a \texttt{group\_by()} call is seemingly unchanged.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county)}
\CommentTok{\#\textgreater{} \# A tibble: 648 x 14}
\CommentTok{\#\textgreater{} \# Groups:   county [3]}
\CommentTok{\#\textgreater{}    county hs\_grad  bach master commute\_less10 commute1030 commute3060}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}    \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}  \textless{}dbl\textgreater{}          \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 MIDDL\textasciitilde{}   0.389 0.188 0.100          0.0916       0.357       0.375}
\CommentTok{\#\textgreater{}  2 MIDDL\textasciitilde{}   0.167 0.400 0.130          0.0948       0.445       0.344}
\CommentTok{\#\textgreater{}  3 MIDDL\textasciitilde{}   0.184 0.317 0.139          0.0720       0.404       0.382}
\CommentTok{\#\textgreater{}  4 MIDDL\textasciitilde{}   0.258 0.322 0.144          0.0983       0.390       0.379}
\CommentTok{\#\textgreater{}  5 MIDDL\textasciitilde{}   0.301 0.177 0.0742         0.0670       0.379       0.365}
\CommentTok{\#\textgreater{}  6 MIDDL\textasciitilde{}   0.159 0.310 0.207          0.0573       0.453       0.352}
\CommentTok{\#\textgreater{}  7 MIDDL\textasciitilde{}   0.268 0.247 0.149          0.0791       0.475       0.368}
\CommentTok{\#\textgreater{}  8 MIDDL\textasciitilde{}   0.261 0.300 0.126          0.137        0.450       0.337}
\CommentTok{\#\textgreater{}  9 MIDDL\textasciitilde{}   0.315 0.198 0.140          0.0752       0.478       0.329}
\CommentTok{\#\textgreater{} 10 MIDDL\textasciitilde{}   0.151 0.348 0.151          0.0830       0.474       0.322}
\CommentTok{\#\textgreater{} \# ... with 638 more rows, and 7 more variables: commute6090 \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   commute\_over90 \textless{}dbl\textgreater{}, by\_auto \textless{}dbl\textgreater{}, by\_pub\_trans \textless{}dbl\textgreater{}, by\_bike \textless{}dbl\textgreater{},}
\CommentTok{\#\textgreater{} \#   by\_walk \textless{}dbl\textgreater{}, med\_house\_income \textless{}dbl\textgreater{}}
\end{Highlighting}
\end{Shaded}

However, if we look at comments above the tibble, we see something new: \texttt{\#\ Groups:\ county\ {[}3{]}}. This tells us a couple of things. First that the groups were created using the \texttt{county} column, that there are fifteen groups, and that the data frame is now grouped implying that any future \texttt{mutate()} or \texttt{summarise()} calls will be performed on the specified groups. If we then look at the class of that grouped tibble we see that there is a new class introduced which is \texttt{grouped\_df}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{class}\NormalTok{()}
\CommentTok{\#\textgreater{} [1] "grouped\_df" "tbl\_df"     "tbl"        "data.frame"}
\end{Highlighting}
\end{Shaded}

\begin{quote}
Note: a tibble has the classes \texttt{tbl} and \texttt{tbl\_df} on top of the Base R class \texttt{data.frame}.
\end{quote}

When a tibble has this object class, dplyr knows that operations should be grouped. For example if you were to calculate the mean, this would be the mean for the specified groups rather than the mean for the entire dataset.
One function that is extremely useful is the \texttt{n()} function to identify how many observations there are per group inside of a mutate call.

\begin{quote}
I am including the \texttt{commute3060} column to illustrate that the new \texttt{n} column will be the same for each group value.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(county, commute3060, n) }
\CommentTok{\#\textgreater{} \# A tibble: 648 x 3}
\CommentTok{\#\textgreater{} \# Groups:   county [3]}
\CommentTok{\#\textgreater{}    county    commute3060     n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}           \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 MIDDLESEX       0.375   318}
\CommentTok{\#\textgreater{}  2 MIDDLESEX       0.344   318}
\CommentTok{\#\textgreater{}  3 MIDDLESEX       0.382   318}
\CommentTok{\#\textgreater{}  4 MIDDLESEX       0.379   318}
\CommentTok{\#\textgreater{}  5 MIDDLESEX       0.365   318}
\CommentTok{\#\textgreater{}  6 MIDDLESEX       0.352   318}
\CommentTok{\#\textgreater{}  7 MIDDLESEX       0.368   318}
\CommentTok{\#\textgreater{}  8 MIDDLESEX       0.337   318}
\CommentTok{\#\textgreater{}  9 MIDDLESEX       0.329   318}
\CommentTok{\#\textgreater{} 10 MIDDLESEX       0.322   318}
\CommentTok{\#\textgreater{} \# ... with 638 more rows}
\end{Highlighting}
\end{Shaded}

Here each group only has one unique value for \texttt{n}. As discussed previously, when we want to calculate aggregate measures, there ought to only value per-group. This ability to perform grouped calculation within \texttt{mutate()}can be extremely powerful, but does not create a proper aggregated dataset. For this, we can again use \texttt{summarise()}

Let's recreate the grouped count from before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   county        n}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1 MIDDLESEX   318}
\CommentTok{\#\textgreater{} 2 NORFOLK     130}
\CommentTok{\#\textgreater{} 3 SUFFOLK     200}
\end{Highlighting}
\end{Shaded}

We can also include the summary statistic calculations from before.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}
    \DataTypeTok{n =} \KeywordTok{n}\NormalTok{(),}
    \DataTypeTok{min\_commute =} \KeywordTok{min}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{max\_commute =} \KeywordTok{max}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{avg\_commute =} \KeywordTok{mean}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{sd\_commute  =} \KeywordTok{sd}\NormalTok{(commute3060, }\DataTypeTok{na.rm =} \OtherTok{TRUE}\NormalTok{)}
\NormalTok{    )}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 6}
\CommentTok{\#\textgreater{}   county        n min\_commute max\_commute avg\_commute sd\_commute}
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}     \textless{}int\textgreater{}       \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}       \textless{}dbl\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 MIDDLESEX   318       0.104       0.612       0.383     0.0825}
\CommentTok{\#\textgreater{} 2 NORFOLK     130       0.186       0.613       0.392     0.0761}
\CommentTok{\#\textgreater{} 3 SUFFOLK     200       0           0.633       0.400     0.0972}
\end{Highlighting}
\end{Shaded}

\hypertarget{toolkit-review}{%
\chapter{Toolkit review}\label{toolkit-review}}

We've now come to the end of the first technical section of the Urban Informatics Toolkit. And you have officially covered \emph{\textbf{a lot}} of ground. You've installed both R and RStudio. You've learned the basics of R operations and data structures. You've read and manipulated a large dataset, selected columns, created new ones, and even created a few visualization. You've learned to chain multiple functions together and have even created your own sets of summary statistics. These are all very important useful skills which will serve the foundation of everything else you will do in R.

The next section of this book will focus entirely on data visualization. We will begin by learning about the Grammar of Graphics. Next we will learn how to apply that grammar in R with ggplot2. Following, we will create a \emph{ton} of graphics and build intuition about when and how to create different types of graphics.

Are you ready?

Are you hydrated?

Take a deep breath and let's get after it!

\hypertarget{part-vizualization-strategies}{%
\part{Vizualization Strategies}\label{part-vizualization-strategies}}

\hypertarget{layered-i}{%
\chapter{Grammar of layered graphics I}\label{layered-i}}

You've made it quite far through this book. Now, I want to bring us back to the very beginning. In the first chapter we created a few visualizations with \texttt{ggplot2}. I want to unpack ggplot2 a bit more and also address some of the more philosophical underpinnings of visualization.

This chapter introduces you to the idea of the grammar of graphics, discusses when which visualizations are appropriate, and some fundamental design principles follow.

\hypertarget{the-grammar-of-layered-graphics}{%
\section{The Grammar of Layered Graphics}\label{the-grammar-of-layered-graphics}}

The \texttt{gg} in \texttt{ggplot2} refers to the grammar of graphics (and the \texttt{2} is because it's the second iteration). \emph{The Grammar of Graphics} (Wilkinson, 1999) is a seminal book in data visualization for the sciences in which, Wilkinson defines a complete system (grammar) for creating visualizations that go beyond the standard domain of ``named graphics''---e.g.~histogram, barchart, etc. \footnote{A Layered Grammar of Graphics. Hadley Wickham. \url{https://vita.had.co.nz/papers/layered-grammar.pdf}}\footnote{The Grammar of Graphics. \url{https://www.springer.com/gp/book/9780387245447}.}

ggplot2 is ``an open source implementation of the grammar of graphics for \textbf{R}.''\footnote{A Layered Grammar of Graphics. Hadley Wickham. \url{https://vita.had.co.nz/papers/layered-grammar.pdf}} Once we can internalize the grammar of graphics, creating plots will be an intuitive and artistic process rather than a mechanical one.

There are five high level components of the layered grammar\footnote{A Layered Grammar of Graphics. Hadley Wickham. \url{https://vita.had.co.nz/papers/layered-grammar.pdf}}. We will only cover the first two in this chapter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Defaults}:

  \begin{itemize}
  \tightlist
  \item
    Data
  \item
    Mapping
  \end{itemize}
\item
  \textbf{Layers}:

  \begin{itemize}
  \tightlist
  \item
    Data*
  \item
    Mapping*
  \item
    Geom
  \item
    Stat
  \item
    Position
  \end{itemize}
\item
  Scales
\item
  Coordinates
\item
  Facets
\end{enumerate}

\hypertarget{layers-and-defaults}{%
\section{Layers and defaults}\label{layers-and-defaults}}

Let's first get some data into our environment. We will use the \texttt{commute} dataset again.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/gba\_commute.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the first chapter of this section we explored these principles but did not put a name to them. Recall that we can use \texttt{ggplot()} by itself and it returns a chart of nothing.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04a-grammar-I_files/figure-latex/unnamed-chunk-3-1.pdf}

This is because we have not specified any of the defaults. In order for us to plot anything at all, we need to specify what (the data object) will be visualized, which features (the aesthetic mappings), and how (the geoms). When we begin to specify our x and y aesthetics the scales are interpreted.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(med\_house\_income, by\_auto))}
\end{Highlighting}
\end{Shaded}

\includegraphics{04a-grammar-I_files/figure-latex/unnamed-chunk-4-1.pdf}

The final step is to add the geom layer which will inherit the data, aesthetic mappings, scale, and position while the \texttt{geom\_*()} layer dictates the geometry.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(bach, med\_house\_income))}\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04a-grammar-I_files/figure-latex/unnamed-chunk-5-1.pdf}

While this is the most common way you might define a ggplot, you should also be aware of the fact that each layer can stand on its own without you defining any of the defaults in the \texttt{ggplot()} call. Each geom inherits the defaults from \texttt{ggplot()}, but each \texttt{geom\_*()} also has arguments for \texttt{data}, and \texttt{mapping}, providing you with increased flexibility.

\begin{quote}
Note: the \texttt{geom\_*()}s have the data as the second argument so either put the data there or name the argument explicitly. The choice is yours. Choose wisely!
\end{quote}

What happens if we provide all of this information to \texttt{geom\_point()} and entirely omit \texttt{ggplot()}?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{geom\_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(med\_house\_income, by\_auto), commute)}
\CommentTok{\#\textgreater{} mapping: x = \textasciitilde{}med\_house\_income, y = \textasciitilde{}by\_auto }
\CommentTok{\#\textgreater{} geom\_point: na.rm = FALSE}
\CommentTok{\#\textgreater{} stat\_identity: na.rm = FALSE}
\CommentTok{\#\textgreater{} position\_identity}
\end{Highlighting}
\end{Shaded}

We see that we do not have the plot, but we do have all of the information required of a layer is printed out to the console. If we add an empty ggplot call ahead of the layer, we will be able to create the plot.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\KeywordTok{aes}\NormalTok{(med\_house\_income, by\_auto), commute)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04a-grammar-I_files/figure-latex/unnamed-chunk-7-1.pdf}

Being able to specify different data objects within each layer will provie to be extraordinarily helpful when we begin to work with spatial data, or plotting two different data frames with the same axes, or any other creative problem you wish to solve.

\hypertarget{visualizing-trends-and-relationships}{%
\chapter{Visualizing Trends and Relationships}\label{visualizing-trends-and-relationships}}

We now have a language for creating graphics. Next we must build the intuition of which plots to build and when. We will cover the most basic of visualizations starting with univariate followed by bivariate plots. We will then discuss ways to extend visualizations beyond two variables and some simple principles of design.

In most cases a data analysis will start with a visualization. And that visualization will be dictated by the characteristics of the data available to us. In intro to statistics you probably learned about the four types of data which are: nominal and ordinal, together referred to as \emph{categorical}; interval and ratio, together referred to as \emph{numerical} We're going to contextualize these in R terms where \emph{categorical} is \texttt{character} and \emph{numerical} is \texttt{numeric}.

Categorical and numeric have different are treated differently and thus lead to different kinds of visualizations. When we refer to categorical or character, we are often thinking of groups or a label. In the case where we don't have a quantifiable numeric value, we often count those variables.

Throughout this chapter we will use another ACS data with variables focused towards housing. This file lives at \texttt{data/acs-housing.csv}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{acs \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/acs{-}housing.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\hypertarget{univariate-visualizations}{%
\section{Univariate visualizations}\label{univariate-visualizations}}

There is always a strong urge to begin creating epic graphs with facets, shapes, colors, and hell, maybe even three dimensions. But we must resist that urge! We \emph{must} understand the distributions of our data before we start visualizing them and drawing conlcusions. Who knows, we may find anomalies or errors in the data cleaning process or even collection process. We should always begin by studying the indivual variable characteristics with univariate visualizations.

\begin{quote}
Note that univariate visualizations are for numeric variables
\end{quote}

There a couple of things that we are looking for in a numeric univariate visualization. In the broadest sense, we're looking at characterizations of central tendency, and spread. When we create these visualizations we're trying to answer the following questions:

\begin{itemize}
\tightlist
\item
  Where is the middle?
\item
  Is there more than one middle?
\item
  How close together are our points?
\item
  Are there any points very far from the middle?
\item
  Is the distribution flat? Is it steep?
\end{itemize}

In exploring these questions we will rely on three types of plots:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Histogram
\item
  Density
\item
  Box plot
\end{enumerate}

Each type of plot above serves a different purpose.

\hypertarget{histogram}{%
\subsection{Histogram}\label{histogram}}

We have already created a number of histograms already but it is always good to revisit the subject. Histograms puts our data into \texttt{n} buckets (or bins, or groups, or whatever your stats professor called them), counts the number of values that fall into each bucket, and use that frequency count as the height of each bar.

The true benefit of the histogram is that it is the easiest chart to consume by the layperson. But the downside is that merely by changing the number of bins, the distribution can be rather distorted and it is on you, the researcher and analyst, to ensure that there is no miscommunication of data.

When we wish to create a histogram, we use the \texttt{geom\_histogram(bins\ =\ n)} geom layer. Since it is a univariate visualization, we only specify one aesthetic mapping---in this case it is \texttt{x}.

Let's look at the distribution of the \texttt{med\_yr\_moved\_inraw} column for an example.

\begin{itemize}
\tightlist
\item
  Create a histogram of \texttt{med\_yr\_moved\_inraw} with 10 bins.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(med\_yr\_moved\_inraw)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{10}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-3-1.pdf}

This histogram is rather informative! We can see that shortly after 2000, there was a steep increase in people moving in. Right after 2005 we can see that number tapering off---presumably due to the housing crises that begat the Great Recession.

Now, if we do not specify the number of bins, we get a very different histogram.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(med\_yr\_moved\_inraw)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_histogram}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-4-1.pdf}

The above histogram shows gaps in between buckets of the histogram. On a first glance, we would assume that there may be missing data or some phenemonon in the data recording process that led to some sort of missingness. But that isn't the case! If we count the number of observations per year manually, the story becomes apparent.

\begin{quote}
Note: I am using the base R function \texttt{table()} to produce counts. This produces a class \texttt{table} object which is less friendly to work with. Using \texttt{table()} rather than count serves two purposes: 1) you get to learn another function and 2) the printing method is more friendly for a bookdown document.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(moved\_counts \textless{}{-}}\StringTok{ }\KeywordTok{table}\NormalTok{(acs}\OperatorTok{$}\NormalTok{med\_yr\_moved\_inraw))}
\CommentTok{\#\textgreater{} }
\CommentTok{\#\textgreater{} 1991 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 }
\CommentTok{\#\textgreater{}    1    2    5    7   14   31   56   77  108  121  141  109  113   84   73   67 }
\CommentTok{\#\textgreater{} 2010 2011 2012 2013 }
\CommentTok{\#\textgreater{}  125  140   29    8}

\NormalTok{glue}\OperatorTok{::}\KeywordTok{glue}\NormalTok{(}\StringTok{"There are \{length(moved\_counts)\} unique values"}\NormalTok{)}
\CommentTok{\#\textgreater{} There are 20 unique values}
\end{Highlighting}
\end{Shaded}

\begin{quote}
The glue function provides a way to create strings by combining R expressions and plain text. More in the appendix.
\end{quote}

This above tells us something really important and explains why our histogram is all wonky. Our histogram looks the way it does because we have specified more bins than there are unique values! The moral of the story is that when creating a histogram, be thoughtful and considerate of the number of bins your are using---it changes the whole story.

\hypertarget{density-function-plot}{%
\subsection{Density Function plot}\label{density-function-plot}}

Histograms are a fairly straight-forward chart that provides illustrates the distribution of a sample space. The histogram does not provide a fine grain picture of what the underlying distribution looks like. When we are concerned with understanding the underlying shape of a distribution we should use a \textbf{kernel density plot} (aka density plot).

The density plot represents a variable over a continuous space and by doing so creates a much better visual representation of the underlying distribution shape with all of its curves.

Like a histogram, we only provide a single variable to the aesthetic mappings. The geom layer for a density distribution is \texttt{geom\_density()}.

\begin{itemize}
\tightlist
\item
  Create a density plot of \texttt{med\_yr\_moved\_inraw}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(med\_yr\_moved\_inraw)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_density}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-6-1.pdf}

Now compare the histogram to the density plot.

\begin{itemize}
\tightlist
\item
  Which do you feel does a better job illustrating the shape of the distribution?
\item
  Which do you think is more interpretable?
\end{itemize}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-7-1.pdf}

\hypertarget{boxplots}{%
\subsection{Boxplots}\label{boxplots}}

The boxplot is the third univariate visualization we will cover. Unlike histograms and density plot, the box plot's power comes from being able to effectively illustrate outliers and the general spread of a variable.

There are five elements that make the boxplot:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Minimum
\item
  First quartile (25th percentile)
\item
  Median (50th percentile)
\item
  Third quartile (75th percentile)
\item
  Maximum
\end{enumerate}

When creating a boxplot, the definition of minimum and maximum change a little bit. We are defining the minimums and maximums \emph{without} the outliers. And in the context of a boxplot the outliers are determined by the \textbf{IQR} (inner quartile range). The IQR is different between the third and first quartile. We then take the IQR and \emph{add} it to the third quartile to find the upper bound and then subtract the IQR from the first quartile to find the lower bound.

\(IQR = Q3 - Q1\)

\(Minimum = Q1 - IQR\)

\(Maximum = Q3 + IQR\)

\begin{quote}
Note that this is a naive approach to defining an outlier. This is not a hard and fast rule of what is considered an outlier. There are many considerations that should go into defining an outlier other than arbitrary statistical heuristics. Be sure to have a deep think before calling anything an outlier.
\end{quote}

Any points that fall outside of that the minimum and maximum are plotted individually to give you an idea of any \emph{potential} outliers.

To create a boxplot we use the \texttt{geom\_boxplot()} function.

\begin{itemize}
\tightlist
\item
  Create a boxplot of \texttt{med\_house\_income}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(med\_house\_income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_boxplot}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-8-1.pdf}

From this boxplot, what can we tell about median household income in Massachusetts?

\hypertarget{empirical-cumulative-distribution-function-ecdf}{%
\subsection{Empirical Cumulative Distribution Function (ECDF)}\label{empirical-cumulative-distribution-function-ecdf}}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(med\_house\_income)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_step}\NormalTok{(}\DataTypeTok{stat =} \StringTok{"ecdf"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-9-1.pdf}

\hypertarget{barchart}{%
\subsection{Barchart}\label{barchart}}

The last univariate chart we will touch on is the bar chart. When we are faced with a single categorical variable there is not much that we can do to summarize it. The approach is to identify the frequency or relative frequency with which each level (unqiue value) occurs. This is essentially a histogram but for values which cannot have ranges and where order does not matter---though we may be interested in ordering our values.

To create a bar chart of categorical features we simply provide that feature to our aesthetic mapping and add \texttt{geom\_bar()} layer

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(county)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-10-1.pdf}

I'm sure you're looking at this chart and thinking something along the lines of ``I can't read a single label, this is awful.'' And yeah, you're totally right. In general when creating a bar chart it is better to to flip the axes. The main justification for fliping the axes is so that we can read the labels better. In addition to making the labels more legible, by flipping the axes, the comparisons between bars is perceivedly easier.

To flip the axes, we can map the \texttt{county} column to the \texttt{y} axis rather than \texttt{x} (which is done positionally).

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ county)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_bar}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-11-1.pdf}

If you find yourself in the situation where you have variables mapped to both the x and y columns we can add a \texttt{coor\_flip()} layer to the plot which will handle the flipping for us.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(county)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_bar}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{coord\_flip}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-12-1.pdf}

Be sure to keep \texttt{coord\_flip()} in your back pocket! It is a rather handy function.

\hypertarget{bivariate-visualizations}{%
\section{Bivariate visualizations}\label{bivariate-visualizations}}

We are ready to introduce a second variable into the analysis. With bivariate relationships (two-variables) we are often looking to answer, in general, if one variable changes with another. But the way we approach these relationships is dependent upon the type of variables we have to work with. We can can either be looking at the bivariate relationship of

\begin{itemize}
\tightlist
\item
  2 numeric variables,
\item
  1 numeric variable and 1 categorical,
\item
  or 2 categorical variables.
\end{itemize}

\hypertarget{two-numeric-variables}{%
\subsection{Two Numeric Variables}\label{two-numeric-variables}}

\hypertarget{scatter-plot}{%
\subsubsection{Scatter plot}\label{scatter-plot}}

When confronted with two numeric variables, your first stop should be the scatter plot. A scatter plot positions takes two continuous variables and plots each point at their (x, y) coordinate. This type of plot illustrates how the two variables change with each other---if at all. It is exceptionally useful for pinpointing linearity, clusters, points that may be disproportionately distorting a relationship, etc.

Scatter plots are useful for asking questions such as ``when x increases how does y change?'' Because of this natural formulation of statistical questions---i.e.~we are always interested in how the x affects the y---we plot the variable of interest vertically along the y axis and the independent variable along the horizontal x axis.

Take for example the question ``how does the proportion of individuals under the age of eighteen increase with the number of family households?''

Using a scatter plot, we can begin to answer this question! Notice how in the formulation of our question we are asking how does y change with x. In this formulation we should plot the \texttt{fam\_house\_per} against the \texttt{age\_u18} column.

\begin{quote}
Note: when plotting \emph{against} something. We are plotting x \emph{against} y.
\end{quote}

Recall that to plot a scatter plot we use the \texttt{geom\_point()} layer with an x and y aesthetic mapped.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(fam\_house\_per, age\_u18)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-13-1.pdf}

The above scatter plot is useful, but there is one downside we should be aware of and that is the number of points that we are plotting. Since there are over 1,400 points---as is often the case with big data---they will likely stack on top of each other hiding other points and leading to a dark uninterpretable mass! We want to be able to decipher the concentration of points as well as the shape.

\begin{quote}
When there are too many points to be interpretable this is called overplotting
\end{quote}

To improve the visualization we have a few options. We can make each point more transparent so as they stack on top of eachother they become darker. Or, we can make the points very small so that as they cluster they become a bigger and darker mass.

To implement these stylistic enhancements we need to set some aesthetic arguments inside of the geom layer. In order to change the transparency of the layer we will change the \texttt{alpha} argument. \texttt{alpha} takes a value from 0 to 1 where 0 is entirely transparent and 1 is completely opaque. Try a few values and see what floats your boat!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(fam\_house\_per, age\_u18)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{4}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-14-1.pdf}

Alternatively, we can change the size (or even a combination of both) of our points. To do this, change the \texttt{size} argument inside of \texttt{geom\_point()}. There is not a finite range of values that you can specify so experimentation is encouraged!

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(fam\_house\_per, age\_u18)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-15-1.pdf}

\begin{quote}
Remember when deciding the \texttt{alpha} and \texttt{size} parameters your are implementing stylistic changes and as such there are no \emph{correct} solution. Only marginally better solutions.
\end{quote}

\hypertarget{hexagonal-heatmap}{%
\subsubsection{Hexagonal heatmap}\label{hexagonal-heatmap}}

Scatter plots do not scale very well with hundreds or thousand of points. When the scatter plot becomes a gross mass of points, we need to find a better way to display those data. One solution to this is to create a heatmap of our points. You can think of a heatmap as the two-dimension equivalent to the histogram.

The heatmap ``divides the plane into rectangles {[}of equal size{]}, counts the number of cases in each rectangle'', and then that count is then used to color the rectangle\footnote{\texttt{geom\_bin2d()}. \url{https://ggplot2.tidyverse.org/reference/geom_bin2d.html}}. An alternative to the rectangular heatmap is the hexagonal heatmap. The hexagonal heatmap has a few minor visual benefits over the rectangular heatmap. But choosing which one is better suited to the task it up to you!

The geoms to create these heatmaps are

\begin{itemize}
\item
  \texttt{geom\_bin2d()} for creating the rectangular heatmap and
\item
  \texttt{geom\_hex()} for a hexagonal heatmap.
\item
  Convert the above scatter plot into a heat map using both above geoms.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(fam\_house\_per, age\_u18)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_bin2d}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-16-1.pdf}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(fam\_house\_per, age\_u18)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_hex}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-17-1.pdf}

Just like a histogram we can determine the number of bins that are used for aggragating the data. By adjusting the \texttt{bins} argument to \texttt{geom\_hex()} or \texttt{geom\_bin2d()} we can alter the size of each hexagon or rectangle. Again, the decision of how many bins to include is a trade-off between interpretability and accurate representation of the underlying data.

\begin{itemize}
\tightlist
\item
  Set the number of \texttt{bins} to 20
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs, }\KeywordTok{aes}\NormalTok{(fam\_house\_per, age\_u18)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_hex}\NormalTok{(}\DataTypeTok{bins =} \DecValTok{20}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-18-1.pdf}

\hypertarget{one-numeric-and-one-categorical}{%
\subsection{One numeric and one categorical}\label{one-numeric-and-one-categorical}}

The next type of bivariate relationship we will encounter is that between a numeric variable and a categorical variable. In general there are two lines of inquiry we might take. The first is similar to our approach of a single numeric variable where we are interested in measures of centrality and spread but are further interested in how those characteristics change by category (or group membership). The second seeks to compare groups based on some aggregate measure of a numeric variable.

As an example, imagine we are interested in evaluating the educational attainment rate by county in the Greater Boston Area. We can take the approach of ranking the educational attainment rate by the median or average. Or, we could also try and evaluate if the counties differ in the amount of variation.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gba\_acs \textless{}{-}}\StringTok{ }\NormalTok{acs }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\KeywordTok{toupper}\NormalTok{(county) }\OperatorTok{\%in\%}\StringTok{ }\KeywordTok{c}\NormalTok{(}\StringTok{"SUFFOLK COUNTY"}\NormalTok{, }\StringTok{"NORFOLK COUNTY"}\NormalTok{, }\StringTok{"MIDDLESEX COUNTY"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

We will explore different techniques for addressing both lines of inqury.

\hypertarget{ridgelines}{%
\subsubsection{Ridgelines}\label{ridgelines}}

Ridgelines are a wonderful method for visualizing the distribution of a numeric variable for each level in a categorical variable. Ridgeline plot a density curve for each level in your categorical variable and stacks them vertically. In doing so, we have a rather comfortable way to assess the shape of each level's distribution.

To plot a ridgeline, we need to install the package \texttt{ggridges} and use the function \texttt{ggridges::geom\_density\_ridges()}.

\begin{quote}
Reminders: install packages with \texttt{install.packages("pkg-name")}. The expression \texttt{ggridges::geom\_density\_ridges()} is used for referencing an exported function from a namespace (package name). The syntax is \emph{\texttt{pkgname::function()}}.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gba\_acs, }\KeywordTok{aes}\NormalTok{(bach, county)) }\OperatorTok{+}
\StringTok{  }\NormalTok{ggridges}\OperatorTok{::}\KeywordTok{geom\_density\_ridges}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-20-1.pdf}

The ridgeline plot very clearly illustrates the differences in distributions within the \texttt{county} variable. From this plot, we can tell that Suffolk County has rather extreme variation in the Bachelor's degree attainment rate. And when compared to Norfolk and Middlesex counties, it becomes apparent that the median Suffolk County attainment rate falls almost 20\% lower.

A plot such as the above may lead one to investigate further. Suffolk County is large and contains every single neighborhood of Boston from Back Bay, to Mission Hill, and Roxbury. We can drill down further into Suffolk County by identifying income percentiles and plotting those as well.

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-21-1.pdf}

Ridgelines are the perfect tool for exploring changes in variation among different groups. Before you run an ANOVA visualize the variation of your variables with a ridgeline plot first!

\hypertarget{boxplot}{%
\subsubsection{Boxplot}\label{boxplot}}

It's time to come back to the boxplot. The boxplot is indeed wonderful for a single variable. But much in the same way that multiple density plots is what makes the ridgeline fantastic, so does multiple box plots!

Again, when using the boxplot we are not as concerned about the \emph{shape} of the distribution but rather \emph{where} the data are. The boxplot is extremely useful for identifying skewness and potential outliers.

We can look at the distribution of educational attainment using a boxplot just like above. The only difference is the use of the \texttt{geom\_boxplot()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(gba\_acs, }\KeywordTok{aes}\NormalTok{(bach, county)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-22-1.pdf}

\hypertarget{barchart-1}{%
\subsubsection{Barchart}\label{barchart-1}}

We've already used the barchart to plot counts of categorical variables. But they are also useful for visualizing summary values of each categorical variable.

For example, if we were to plot the number of observations per county we can use our knowledge of \texttt{summarise()} to recreate the values.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_counts \textless{}{-}}\StringTok{ }\NormalTok{acs }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{n}\NormalTok{())}

\NormalTok{county\_counts}
\CommentTok{\#\textgreater{} \# A tibble: 14 x 2}
\CommentTok{\#\textgreater{}    county                n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}             \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 Barnstable County    49}
\CommentTok{\#\textgreater{}  2 Berkshire County     38}
\CommentTok{\#\textgreater{}  3 Bristol County      116}
\CommentTok{\#\textgreater{}  4 Dukes County          4}
\CommentTok{\#\textgreater{}  5 Essex County        151}
\CommentTok{\#\textgreater{}  6 Franklin County      17}
\CommentTok{\#\textgreater{}  7 Hampden County       89}
\CommentTok{\#\textgreater{}  8 Hampshire County     30}
\CommentTok{\#\textgreater{}  9 Middlesex County    283}
\CommentTok{\#\textgreater{} 10 Nantucket County      2}
\CommentTok{\#\textgreater{} 11 Norfolk County      115}
\CommentTok{\#\textgreater{} 12 Plymouth County      89}
\CommentTok{\#\textgreater{} 13 Suffolk County      168}
\CommentTok{\#\textgreater{} 14 Worcester County    160}
\end{Highlighting}
\end{Shaded}

Now that we've counted the number of points per value, we can plot that using either \texttt{geom\_bar()} and setting \texttt{stat\ =\ "identity"} \emph{or} we can use \texttt{geom\_col()}. I prefer the latter.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(county\_counts, }\KeywordTok{aes}\NormalTok{(n, county)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-24-1.pdf}

Here's the thing, barcharts, and horizontal barcharts in particular are phenomenal for ranking. But ggplot2 doesn't order the bars for us. We need to do that on our own. To do so, we will use our knowledge of \texttt{mutate()} and a new function \texttt{forcats::fct\_reorder()}.

\texttt{fct\_reorder()} is a function used for reordering categorical variables by some other numeric variable. In our case, we want to reorder \texttt{county} by \texttt{n}. So, within a \texttt{mutate()} function call we will alter \texttt{county} to be the value of the output \texttt{fct\_reorder(county,\ n)}.

\begin{quote}
If you are confused by \texttt{fct\_reorder()}, remember to check out the help documentation with \texttt{?fct\_reorder()}.
\end{quote}

The modified \texttt{county\_counts} tibble can then be piped into our \texttt{ggplot()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{county\_counts }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{county =} \KeywordTok{fct\_reorder}\NormalTok{(county, n)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(n, county)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-25-1.pdf}

This is a pattern that you will follow rather frequently---particularly when you need to rank variables. Now knowing the number of observations is useful, but we really want to use the barchart for visualizing some value of importance. Let's continue the example of educational attainment but for the entirety of Massachusetts this time.

\begin{itemize}
\tightlist
\item
  Using \texttt{group\_by()} and \texttt{summarise()}, calculate the median Bachelor degree attainment rate and call that column \texttt{med\_bach}.
\item
  Reorder \texttt{county} by \texttt{avg\_bach}
\item
  Create an ordered horizontal barchart of \texttt{avg\_bach} by \texttt{county}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{med\_bach =} \KeywordTok{median}\NormalTok{(bach)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{county =} \KeywordTok{fct\_reorder}\NormalTok{(county, med\_bach)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(med\_bach, county)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-26-1.pdf}

This brings us very naturally to our next type of plot: the lollipop plot.

\hypertarget{lollipop-plot}{%
\subsubsection{Lollipop plot}\label{lollipop-plot}}

The lollipop plot is the barchart's more fun cousin. Rather than a big thick bar we plot the summary value with a big point and draw a thin line back to it's respective axis' intercept. We can either manually create the lollipop using a creative combination of geoms, or use a geom incorporated in another package. I will almost always recommend that you don't recreate something if you do not have to. As such, we will use the \texttt{ggalt::geom\_lollipop()} function.

\begin{quote}
Remember: \emph{\texttt{pkgname::function()}}. If you do not have \texttt{pkgname} installed, install it with \texttt{install.packages("pkgname")}.
\end{quote}

We can copy our previous barplot code and only replace the geom to produce a lollipop plot! Since we are keeping \texttt{med\_bach} in the x position we will need to specify \texttt{horizontal\ =\ TRUE} in \texttt{geom\_lollipop()}. This is a quirk of the geom but an easy one to get past. I recommend setting \texttt{horizontal\ =\ FALSE} to get a firmer understanding of what is happening. There is nothing quite like purposefully breaking your code to figure out what is happening!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(county) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{med\_bach =} \KeywordTok{median}\NormalTok{(bach)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{county =} \KeywordTok{fct\_reorder}\NormalTok{(county, med\_bach)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(med\_bach, county)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\NormalTok{ggalt}\OperatorTok{::}\KeywordTok{geom\_lollipop}\NormalTok{(}\DataTypeTok{horizontal =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04b-visualization-strategies_files/figure-latex/unnamed-chunk-27-1.pdf}

\hypertarget{review}{%
\section{Review:}\label{review}}

You've now built up a repetoire of different types of visualizations that you can use in your own analyses. You've built an intuition of what types of visualization are suitable given the types of variables at your disposal.

In the next chapter we will explore ways of improving upon the plots that we already know how to build. We will explore further the \emph{Layered Grammar of Graphics} how how to improve upon our charts using scales, and facets, and breifly touch upon coordinates.

\hypertarget{grammer-of-layered-graphics-ii}{%
\chapter{Grammer of layered graphics II}\label{grammer-of-layered-graphics-ii}}

We've developed a strong foundation for building charts from the ground up by specifying our \textbf{defaults} (data, and aesthetic mappings), and adding geom \textbf{layers}. In order to take our charts to the next level we need to familiarize ourselves with the other components of the \emph{Layered Grammar of Graphics}: scales, coordinates, and facets.

For these examples we will again return to our commute dataset. We will also recreate the two columns \texttt{hh\_inc\_quin} and \texttt{edu\_attain}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/gba\_commute.csv"}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{hh\_inc\_quin =} \KeywordTok{ntile}\NormalTok{(med\_house\_income, }\DecValTok{5}\NormalTok{),}
         \DataTypeTok{edu\_attain =}\NormalTok{ bach }\OperatorTok{+}\StringTok{ }\NormalTok{master)}
\end{Highlighting}
\end{Shaded}

\hypertarget{scales}{%
\section{Scales}\label{scales}}

Recall from \emph{\protect\hyperlink{layered-i}{Grammar of Layered Graphics I}} that when we supply our aesthetic mappings our axes are filled out automatically.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(p \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(med\_house\_income, bach)))}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-3-1.pdf}

By specifying our defaults in the \texttt{ggplot()} call, we implicitly are providing the x and y axes. From those mappings, ggplot2 is able to identify the type of variable mapped to each aesthetic and its values. That inference makes it possible for us to plot without having to explicitly state what our axes are.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(med\_house\_income, bach)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

In the above chart, each column is being mapped as a continuous variable. We are able to manually specify what each scale type is by using the various \texttt{scale\_*\_type()} layers from ggplot2. These layers follow a general format of first specifying \texttt{scale} followed by which aesthetic we're scaling, and what data type. For example, to change the \texttt{med\_house\_income} axis to a discrete axis we can apply the layer \texttt{scale\_x\_discrete()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale\_x\_discrete}\NormalTok{() }
\CommentTok{\#\textgreater{} Warning: Removed 8 rows containing missing values (geom\_point).}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-5-1.pdf}

In doing so we have lost the axis labels! That is because ggplot2 considers both integers and floating point (numbers with decimals) as continuous and categorical variables as being discrete.

Nonetheless, we have a lot of functions at our disposal to alter the axes to our liking!

\hypertarget{transformations}{%
\subsection{Transformations}\label{transformations}}

In our data exploration, we will come across non-normal distributions of data. For example income is almost always right skewed and displays some sort of log-normal-ish behavior. We may not want to actually change to underlying values of that variable, but want to apply transformations for the purposes of visualization. In those cases, we can apply scale transformations.

As an example, in our visualization of income and education there is a slight right skew to \texttt{med\_house\_income}. The graphic doesn't justify a logarithmic transformation, but may benefit from a sqrt transformation. We can apply this with \texttt{scale\_x\_sqrt()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale\_x\_sqrt}\NormalTok{() }
\CommentTok{\#\textgreater{} Warning: Removed 8 rows containing missing values (geom\_point).}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-6-1.pdf}

We can apply a log10 transformation as well with \texttt{scale\_*\_log10()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale\_x\_log10}\NormalTok{() }
\CommentTok{\#\textgreater{} Warning: Removed 8 rows containing missing values (geom\_point).}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-7-1.pdf}

This is an overcorrection. The slight upward arch in the original plotting is now inverted. Nonetheless, I hope the point is made.

In addition to applying transformations, we will want to have more control over the limits of our graph. For example, say we want to have our y axis include all \emph{possible} values of {[}0,1{]}. We can tell ggplot what range of values we want our axes to contain with \texttt{lims()}. \texttt{lims()} takes a name-value pair where the name is an aesthetic and the value is a numeric vector with two elements---the first being the value at the origin and the second being at the extent of the axis\footnote{\url{https://ggplot2.tidyverse.org/reference/lims}}.

\begin{quote}
Note: {[}0,1{]} means from 0 inclusive to 1 inclusive.
\end{quote}

We can modify our y axis to have the limits of {[}0,1{]} by adding a \texttt{lims()} layer where we set the \texttt{y} aesthetic to \texttt{c(0,\ 1)}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{lims}\NormalTok{(}\DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\CommentTok{\#\textgreater{} Warning: Removed 8 rows containing missing values (geom\_point).}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-8-1.pdf}

The graph we get we when expand our y axis limits definitely contains a bit too much white space. But by expanding ths grid, we can see this sort of flattening out of education at around \$150,000 while income still continues to increase. Perhaps if we omit tose values, the relationship may seem even stronger. Let's experiment with that.

\begin{itemize}
\tightlist
\item
  Set the x axis limit to be from {[}0, 150000{]}
\item
  Set the y axis limits to be from {[}0, 0.5{]}
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{lims}\NormalTok{(}
    \DataTypeTok{x =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{150000}\NormalTok{),}
    \DataTypeTok{y =} \KeywordTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{, }\FloatTok{.5}\NormalTok{)}
\NormalTok{    )}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-9-1.pdf}

By changing the extent of our axes this relationship seems much more robust! Such a visualization could spur further validation of this ACS data.

\begin{quote}
Remember, ACS data come from samples and sometimes those samples are small. Because of these small sample sizes, we may very well get values that are not properly representative. It is on you to decide whether or not you should include or exclude the values!
\end{quote}

\hypertarget{labeling}{%
\subsection{Labeling}\label{labeling}}

The plots we create, while lovely as they are, are somewhat lacking in the labeling department. I would put money on it that no publication would accept plots with labels such as the ones above for sole reason being that our axes titles and scale labels are hard to interpret.

We've already used it befofe but to be extra clear, to add titles and axis labels to our plots (not adjusting the scale labels) we use a \texttt{labs()} layer. With \texttt{labs()}, you can label any aesthetic you have mapped as well as adding a \texttt{title}, \texttt{subtitle}, \texttt{caption}, and a \texttt{tag}.

Let's add some titles and labels to our plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(med\_house\_income, bach)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{size =} \DecValTok{1}\OperatorTok{/}\DecValTok{3}\NormalTok{) }\OperatorTok{+}
\StringTok{    }\KeywordTok{labs}\NormalTok{(}
       \DataTypeTok{y =} \StringTok{"\% of population with a Bachelor\textquotesingle{}s Degree"}\NormalTok{,}
       \DataTypeTok{x =} \StringTok{"Median Household Income"}\NormalTok{,}
       \DataTypeTok{title =} \StringTok{"Relationship between Education and Income"}\NormalTok{,}
       \DataTypeTok{subtitle =} \StringTok{"All Massachusetts Tracts"}\NormalTok{,}
       \DataTypeTok{caption =} \StringTok{"Source: Boston Area Research Initiative}\CharTok{\textbackslash{}n}\StringTok{via US Census Bureau"}
\NormalTok{      )}

\NormalTok{p}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-10-1.pdf}

Friends, it's looking pretty good. But there are just two more changes we need to make: our axes labels! The x and y axes labels are meant to illustrate dollar amounts and percentages but respectively. To change the \emph{scale} labels. we will use some helper functions from the package \href{https://scales.r-lib.org/}{\texttt{scales}}

\{scales\} provides handy functions for taking a variable and altering the labeling to match some other format. In our case, we are interested in printing our \texttt{med\_house\_income} as in a dollar format, i.e.~\texttt{2000} becomes \texttt{\textbackslash{}\$2,000}, and \texttt{bach} as a percentage, i.e.~\texttt{.4} becomes \texttt{\%40}. To alter our labels we will use \texttt{scales::dollar()}, and \texttt{scales::percent()} respectively.

\begin{quote}
Isn't it nice how well named functions can be sometimes?
\end{quote}

To produce the examples outlined above we would call the function as such:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{scales}\OperatorTok{::}\KeywordTok{dollar}\NormalTok{(}\DecValTok{2000}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "$2,000"}
\NormalTok{scales}\OperatorTok{::}\KeywordTok{percent}\NormalTok{(.}\DecValTok{4}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "40\%"}
\end{Highlighting}
\end{Shaded}

Now we have an understanding of \emph{how} the function behaves, but where do we actually change the labels? This is where we come full cirlce back to our \texttt{scale\_*\_continuous()} layer. As we mentioned earlier, \texttt{ggplot()} will handle making the scales for us. But \texttt{ggplot()} doesn't know how we want to label our variables or how they should appear on the axes. And now the impetus is on us to make these changes manually. To change the axis labels we will specify which axis we are altering using the proper scale layer---i.e.~\texttt{scale\_y\_} or \texttt{scale\_x\_}. Then, in each layer we set the \texttt{labels} argument to the respective labelling function we want---e.g.~\texttt{scales::percent} and \texttt{scales::dollar}.

\begin{quote}
Note: If you append parentheses like you normally would you will get an error. This case we want to ignore them because when they are present, R will try to evaluate that function. Rather, we are interested in providing the \emph{function object} to the \texttt{labels} argument rather than provide it with a vector of output.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale\_x\_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{dollar) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale\_y\_continuous}\NormalTok{(}\DataTypeTok{labels =}\NormalTok{ scales}\OperatorTok{::}\NormalTok{percent)}
\CommentTok{\#\textgreater{} Warning: Removed 8 rows containing missing values (geom\_point).}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-12-1.pdf}

In addition to being able to control the \textbf{defaults}, the \textbf{layers}, and now the \textbf{scales} you are well equipped to create and manipulate your own plots.

\hypertarget{coordinates}{%
\section{Coordinates}\label{coordinates}}

While you're likely to create 98\% of your visualizations without ever manipulating the coordinates, it is still good knowledge to have!

As we have mentioned and alluded to, we are working in a two-dimensional space---meaning with x and y coordinates. When working in two-dimensions, thee cartesian plane is the natural choice for a coordinate reference system. In all of our plots, this has been the default. Behind the scenes, ggplot is essentially adding a \texttt{coord\_cartesian()} layer to your plot.

\begin{quote}
Think of this much of the same way that your scales are inferred.
\end{quote}

If, however, we find the need to alter or manipulate the coordinate system the tools are available to us. We've actually already used one, \texttt{coord\_flip()}. Like, with scales, all coordinate based functions are prefixed with \texttt{coord\_()}. If you will need to use these coordinate layers, it will be to essentially change the aspect ratio of your plots.

We will encounter coordinates much more when we talk about spatial data. For now, though, all you need to know is that they exist and are a major underlying part of your plots.

\hypertarget{facets}{%
\section{Facets}\label{facets}}

The last portion of the grammar to visit is facetting. When we facet a plot we are creating what are called ``small multiples'', a term coined by the prominent Edward Tufte. A facetting, in other words, creates a graph for each unique level in a categorical variable. Think of this like a \texttt{group\_by()} for plotting.

There are two types of facetting we can do: wrapped and grid. These are done with \texttt{facet\_wrap()} and \texttt{facet\_grid()} respectively. The reference documentation sums up the differences best:

\begin{quote}
``\texttt{facet\_grid()} forms a matrix of panels defined by row and column faceting variables. It is most useful when you have two discrete variables, and all combinations of the variables exist in the data. If you have only one variable with many levels, try \texttt{facet\_wrap()}.''\footnote{\url{https://ggplot2.tidyverse.org/reference/facet_grid.html}}
\end{quote}

Let's look at \texttt{facet\_wrap()} first. To create the facetting, we need to add \texttt{facet\_wrap()} as a layer to our existing plot. There is only one argument that we are required to fulfill and that is the \texttt{facet} argument. \texttt{facet} expects a set of variables defined by the \texttt{vars()} function. \texttt{vars()} is a function used throughout the tidyverse to specify which columns are to be referenced used within the context of a the function it's being used in.

To recreate the above plot but facetting by county, we would add \texttt{facet\_wrap(vars(county))} as a layer to our plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p \textless{}{-}}\StringTok{ }\NormalTok{commute }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(}\OperatorTok{!}\KeywordTok{is.na}\NormalTok{(hh\_inc\_quin)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(med\_house\_income, edu\_attain)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }

\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet\_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(county))}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-13-1.pdf}

With \texttt{facet\_wrap()} we are able to explicitly state how many rows or columns of plots there should be. The defaults may be nice, but it's always good to be explicit about our expecvtations! We set the \texttt{nrow} or \texttt{ncol} argument to do this. Since our above example defaulted to \texttt{ncol\ =\ 3}, let's try this with three rows.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet\_wrap}\NormalTok{(}\KeywordTok{vars}\NormalTok{(county), }\DataTypeTok{nrow =} \DecValTok{3}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-14-1.pdf}

The grid works a little bit differenly. Rather than specifying which columns to facet on and the number of rows or columns, we can create a grid (or matrix) of small multiples. With \texttt{facet\_grid()} we use the \texttt{rows} and \texttt{cols}.

We can recreate our above graphs by passing \texttt{vars(county)} to either \texttt{rows} or \texttt{cols}. But where \texttt{facet\_grid()} shines is when you have data in every pairing of two categorical variables. For example we can we create a facet for each combination of \texttt{county} and \texttt{hh\_inc\_quin}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet\_grid}\NormalTok{(}\DataTypeTok{cols =} \KeywordTok{vars}\NormalTok{(county),}
             \DataTypeTok{rows =} \KeywordTok{vars}\NormalTok{(hh\_inc\_quin))}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-15-1.pdf}

When we create facets, each panel has shares the same scales. We can change this by setting the \texttt{scales} argument to one of \texttt{"free"}, \texttt{"free\_x"}, or \texttt{"free\_y"}. These in essence, ``free up'' the scales for each panel. We can choose to share the scales on the x axis by setting \texttt{scales\ =\ "free\_y"} or vice versa.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{facet\_grid}\NormalTok{(}\DataTypeTok{cols =} \KeywordTok{vars}\NormalTok{(county),}
             \DataTypeTok{rows =} \KeywordTok{vars}\NormalTok{(hh\_inc\_quin),}
             \DataTypeTok{scales =} \StringTok{"free"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04c-grammar-II_files/figure-latex/unnamed-chunk-16-1.pdf}

\begin{quote}
Note: the behavior of \texttt{scales\ =\ "free"} changes behavior when set in the context of \texttt{facet\_wrap()} vs \texttt{facet\_grid()}. The former frees the scales for each panel. The latter frees the scales for either a column or row of panels.
\end{quote}

It is also important to note that what we have gone through is by no means exhaustive. You should, at minimum, familiarize yourself with both \texttt{scale\_*\_reverse()}, and \texttt{scale\_*\_binned()} in your spare time. There are dozens, if not hundreds, of ggplot2 functions to suit your every whim. And, as you have already briefly seen, the ggplot function universe is not relegated to just ggplot2. There are many other packages which have built custom geoms and other enhancements that may benefit you.

\hypertarget{visualizing-beyond-2-dimensions}{%
\chapter{Visualizing beyond 2-dimensions}\label{visualizing-beyond-2-dimensions}}

Over the duration of the last three chapters we have cultivated a fundamental understanding of the grammar of graphics and have discussed how to craft univariate visualizations and explore bivariate relationships. In those graphics we have not gone beyond two-dimensions. We only utilized two aesthetics to map. Howver, there is many more that we can incorporate into our visualizations which will in turn enable us to explore three or for variables at once.

\begin{quote}
Note that these will not be 3D, but rather visualize three variables.
\end{quote}

To improve our graphics we will utilize the color, shape, and size aesthetics, as well as faceting. Of course, this begs the question of which aesthetic do I choose? Well, that depends upon what type of data you will be visualizing. Each aesthetic serves different purposes and can be used for a different type of variable.

In general we can use the below mappings:

\begin{itemize}
\tightlist
\item
  color -\textgreater{} continuous or discrete
\item
  shape -\textgreater{} discrete
\item
  size -\textgreater{} continuous
\end{itemize}

\hypertarget{color}{%
\section{Color}\label{color}}

Let us first take a look at the use of color. Color is, after position, the easiest visual cue for we humans to distinguish (that viz book on my coffee table) between. It is also a rather versatile visual cue as it can be used to address both continuous and discrete variables. We will first explore the use of color for discrete measurements. In this context, I do not necessarily mean discrete as in integers, but more or less groups. This is where there is not \emph{necessarily} an order or scale implied in the data. It \emph{can} however be indicative of order---think for example age groups. To explore the use of color for groups or discrete data, we will look at Boston ecometrics of social disorder as discussed previously (O'Brien 2015 CITE NEEDED). Ecometrics are stored in a file called \texttt{ecometrics.csv} the \texttt{data} directory. Read it in as \texttt{ecometrics}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{ecometrics \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/ecometrics.csv"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

At this point in your learning, I think it is appropriate to introduce you to a new package that can be used to quickly summarize and visualize your data. That is called \texttt{skimr}. Within the package there is a function called \texttt{skim()}. This package is really useful for quickly getting an understanding of a dataset as it provides useful summary statistics for each variable as well as a histogram for numeric columns.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{skimr}\OperatorTok{::}\KeywordTok{skim}\NormalTok{(ecometrics)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
#> -- Data Summary ------------------------
#>                            Values    
#> Name                       ecometrics
#> Number of rows             68        
#> Number of columns          4         
#> _______________________              
#> Column type frequency:               
#>   character                2         
#>   numeric                  2         
#> ________________________             
#> Group variables            None      
#> 
#> -- Variable type: character ----------------------------------------------------
#>   skim_variable n_missing complete_rate   min   max empty n_unique whitespace
#> 1 type                  0             1     5    34     0       15          0
#> 2 measure               0             1     4    16     0        4          0
#> 
#> -- Variable type: numeric ------------------------------------------------------
#>   skim_variable n_missing complete_rate  mean      sd    p0   p25   p50   p75
#> 1 year                  0             1 2016.    1.13  2015 2016. 2016. 2017.
#> 2 n                     0             1 1929. 1857.      68  786  1230  2690.
#>    p100 hist 
#> 1  2018 ▇▇▁▇▇
#> 2  7392 ▇▂▁▁▁
\end{verbatim}

A simple graphic here would be to evaluate the raw counts by year. A simple bar chart would look like this.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ecometrics, }\KeywordTok{aes}\NormalTok{(year, n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-5-1.pdf}

But, we are aware that there are different measurements. These were described previously and can be seen below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{distinct}\NormalTok{(ecometrics, measure)}
\CommentTok{\#\textgreater{} \# A tibble: 4 x 1}
\CommentTok{\#\textgreater{}   measure         }
\CommentTok{\#\textgreater{}   \textless{}chr\textgreater{}           }
\CommentTok{\#\textgreater{} 1 violence        }
\CommentTok{\#\textgreater{} 2 guns            }
\CommentTok{\#\textgreater{} 3 private conflict}
\CommentTok{\#\textgreater{} 4 social disorder}
\end{Highlighting}
\end{Shaded}

How can we partition our visualization to illustrate the number of counts per ecometric per year? We can use color---each measurement will receive it's own color. This will make it easier to determine the frequency of which each ecometric occurs. To do this we setting \texttt{fill} rather than \texttt{color} this is because we are working with a polygon shape. \texttt{color} is used in working with lines and points. A useful trick is to think of \texttt{color} as the border and \texttt{fill} as the body fill.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ecometrics, }\KeywordTok{aes}\NormalTok{(year, n, }\DataTypeTok{fill =}\NormalTok{ measure)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-7-1.pdf}

By mapping the fill to the \texttt{measure} variable we were able to create a stacked bar chart! It is apparent that \texttt{violence} is the most frequent of these ecometrics, followed by \texttt{private\ conflict}, \texttt{social\ disorder}, and then \texttt{guns}.

One of the downsides about the stacked barchart is that it is difficult to compare the sizes of each group relative to ones that are not adjacent. For example comparing \texttt{guns} to \texttt{social\ disorder} is made difficult as \texttt{private\ conflict} is situated between them. We can adjust our chart so that each bar is situated next to eachother. We do this by setting the argument \texttt{position\ =\ "dodge"} within the \texttt{geom\_col()} layer.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ecometrics, }\KeywordTok{aes}\NormalTok{(year, n, }\DataTypeTok{fill =}\NormalTok{ measure)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{(}\DataTypeTok{position =} \StringTok{"dodge"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-8-1.pdf}

The dodged bar chart makes it much easier to compare the heights of each bar. But now we are creating a somewhat cluttered graphic. In the situation where there are multiple groups and subgroups, it is often preferred to utilize facetting because the most important thing in any graphic is how easy it is to consume. We would rather make four plots than one messy plot.

Let's facet by \texttt{measure} and tell \texttt{facet\_wrap()} to create only one row.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ecometrics, }\KeywordTok{aes}\NormalTok{(year, n, }\DataTypeTok{fill =}\NormalTok{ measure)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{facet\_wrap}\NormalTok{(}\StringTok{"measure"}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-9-1.pdf}

This is awesome! We have four different plots one for each measurement and it is extremely easy to see how each ecoemtrics has trended over the four year period. It seems like there has been a steady decrease! With this plot, however, we are labeling the ecometrics twice: once with the panel label and once with the legend. Since each facet is labeled individually and are not situated next to any other ecometrics, the color becomes redundant. Unless there is an important reason to visualize the color when faceting, it is most likely not needed. As such, a final visualization would look like below.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(ecometrics, }\KeywordTok{aes}\NormalTok{(year, n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{facet\_wrap}\NormalTok{(}\StringTok{"measure"}\NormalTok{, }\DataTypeTok{nrow =} \DecValTok{1}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-10-1.pdf}

\hypertarget{continuous-color-scales}{%
\subsection{Continuous color scales}\label{continuous-color-scales}}

In the cases where our variable of interest is a continuous numeric one, we ought to be using a continuous color scale. These are scales that change from one color to another to illustrate a range of values---for example we could use this to visualize probabilities from 0 - 1. The below is an example of one of these color palettes.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-11-1.pdf}

Often you'll encounter visualizations that use a rainbow color palette or some other number of colors. To illustrate a range of values. This is \emph{not} recommended. When we are looking at color, we are best able to detect changes in the luminescence (perceived brightness) and saturation \footnote{Why should engineers and scientists care about color and design? \emph{Flowing Data}. \url{https://flowingdata.com/2008/04/29/why-should-engineers-and-scientists-care-about-color-and-design/}.}. As such, we should work with color palettes that are easiest to interpret. First we'll visualize what changing saturation and brightness looks like.

Below is is an image of 10 colors. Starting at the left is the color yellow (hex code \texttt{\#FFFF00FF}). Each step it is desaturated by 10 percent ending with the color grey (hex code \texttt{\#808080FF}). We can think of saturation as how much color there is.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-12-1.pdf}

Below is an example of what changing the brightness can look like.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-13-1.pdf}

If we expand the usage of brightness on both ends of the spectrum we get the below.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-14-1.pdf}

You may find that in the course of your work that individuals will use a color palette like the below.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prismatic}\OperatorTok{:::}\KeywordTok{plot.colors}\NormalTok{(}\KeywordTok{rainbow}\NormalTok{(}\DecValTok{20}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-15-1.pdf}

I strongly \textbf{advise against} this. These colors make it extremely difficult to tell changing values. Consider for a moment how you would try and tell the difference in numeric value between a magenta and a maroon. It would be rather difficult. Moreover, this color palette is not very accessible to those who are color deficient. The below is an approximation of what those with varying types of color blindness might see.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-16-1.pdf}

Compare this with the earlier example of dark to light yellows.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-17-1.pdf}

This is much more accessible to those who are color deficient as well as providing a clearer understanding of a \emph{range} of values.

\hypertarget{example}{%
\subsubsection{Example}\label{example}}

To explore this in R, we will return to the commute data set we created from BARI's Census Indicators very early on. We will visualize the relationship of commuting by automobile and median household income. Moreover, we will color each point by the rate of Bachelor's degree attainment. As educational attainment tends to increase with income, we should expect the coloring to somewhat follow household income.

Create the visualization following the below steps.

\begin{itemize}
\tightlist
\item
  Read in \texttt{data/gba\_commute.csv}
\item
  Plot \texttt{med\_house\_income} against (on the y axis) \texttt{by\_auto}
\item
  Color the plot by \texttt{bach}
\item
  Add the appropriate geometry
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{commute \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/gba\_commute.csv"}\NormalTok{)}

\NormalTok{(p \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(commute, }\KeywordTok{aes}\NormalTok{(by\_auto, med\_house\_income, }\DataTypeTok{color =}\NormalTok{ bach)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-18-1.pdf}

Wonderful! As anticipated, the color of the points are darkest at the bottom where median household income is the lowest. And as income increases, so does the brightness and saturation of our color scale. If we return to the grammar of graphics, we can take further control of the scales. In this case, the color of the scales. One of the ways we can change the color scale is to add the layer \texttt{scale\_color\_gradient()} to manually choose what colors to use in our scale. This provides a lot of flexibility to you as a information designer. Think of how you can use the color to represent what it is thatyou are visualizing. Or, how you can use colors to adhere to a color palette of your own or an organization you are working with.

\texttt{scale\_color\_gradient()} allows us to provide the colors for low values and high values. In general, you should choose a darker color to represent lower values and brighter colors for higher values. Below we add the layer and provide color codes.

\begin{quote}
You can find color codes via google by searching ``color picker.'' Or, you can use any number of free color palette generators online. I am personally a fan of \href{https://coolors.co}{coolors.co}.
\end{quote}

Below we've changed the plot to transition from a dark red to a white as values increase.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{scale\_color\_gradient}\NormalTok{(}\DataTypeTok{low =} \StringTok{"\#360002"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"white"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-19-1.pdf}

Try modifying the above plot by picking two colors that you think do a good job of visualizing a range of values.

\hypertarget{diverging-colors}{%
\subsubsection{Diverging colors}\label{diverging-colors}}

There are many a time when our variable of interest has a middle value and illustrating the center as well as the deviation from that center is important. To visually represent this we use what is called a diverging color scale. Diverging color scales are characterized by a middle color from which both ends of the spectrum originate. The center is to represent some middle value. If we contextualize this as z-scores and the color palette below, the center would be \texttt{0} and any negative scores would trend towards red. Whereas if they trended positive they would become more blue.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-20-1.pdf}

One thing to be wary of is using a divergent color scale when it is not necessary. This is an easy trap to fall into since they're pretty cool. Remember, \emph{only} use diverging color palettes if there is an existing middle value.

\hypertarget{example-1}{%
\subsubsection{Example}\label{example-1}}

Take our ecometrics again. Say we are interested in what the annual deviation is from the sample mean---the average for all years---of each ecometric. This is the perfect use case for a diverging color scale. This will require a bit of computational creativity. So lets work through this.

Let's think about each of the measures we need to calculate. We need to:

\begin{itemize}
\tightlist
\item
  Find the number of counts for each year by ecometric.
\item
  Find the average count for all years for each ecometric.
\item
  Identify the deviation from the mean.
\end{itemize}

We will work through this sequentially. First and foremost we need to calculate the total number of crime reports by ecometric by year. The dataset has more than one observation per year per ecometric. We can see this by running a quick count.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(ecometrics, measure, year)}
\CommentTok{\#\textgreater{} \# A tibble: 16 x 3}
\CommentTok{\#\textgreater{}    measure           year     n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}            \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{}  1 guns              2015     5}
\CommentTok{\#\textgreater{}  2 guns              2016     5}
\CommentTok{\#\textgreater{}  3 guns              2017     5}
\CommentTok{\#\textgreater{}  4 guns              2018     5}
\CommentTok{\#\textgreater{}  5 private conflict  2015     4}
\CommentTok{\#\textgreater{}  6 private conflict  2016     4}
\CommentTok{\#\textgreater{}  7 private conflict  2017     4}
\CommentTok{\#\textgreater{}  8 private conflict  2018     4}
\CommentTok{\#\textgreater{}  9 social disorder   2015     3}
\CommentTok{\#\textgreater{} 10 social disorder   2016     3}
\CommentTok{\#\textgreater{} 11 social disorder   2017     3}
\CommentTok{\#\textgreater{} 12 social disorder   2018     3}
\CommentTok{\#\textgreater{} 13 violence          2015     5}
\CommentTok{\#\textgreater{} 14 violence          2016     5}
\CommentTok{\#\textgreater{} 15 violence          2017     5}
\CommentTok{\#\textgreater{} 16 violence          2018     5}
\end{Highlighting}
\end{Shaded}

We need to tidy this up and ensure that each row is only one observation---in this case one ecometric per year---with the total count. The logic to accomplish this is to first \texttt{group\_by()} \emph{both} \texttt{measure} and \texttt{year} and then sum the \texttt{n} values. As such:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{ecometrics }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(measure, year) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{sum}\NormalTok{(n))}
\CommentTok{\#\textgreater{} \# A tibble: 16 x 3}
\CommentTok{\#\textgreater{} \# Groups:   measure [4]}
\CommentTok{\#\textgreater{}    measure           year     n}
\CommentTok{\#\textgreater{}    \textless{}chr\textgreater{}            \textless{}dbl\textgreater{} \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{}  1 guns              2015  3146}
\CommentTok{\#\textgreater{}  2 guns              2016  3111}
\CommentTok{\#\textgreater{}  3 guns              2017  3190}
\CommentTok{\#\textgreater{}  4 guns              2018  2585}
\CommentTok{\#\textgreater{}  5 private conflict  2015  8063}
\CommentTok{\#\textgreater{}  6 private conflict  2016  7807}
\CommentTok{\#\textgreater{}  7 private conflict  2017  7410}
\CommentTok{\#\textgreater{}  8 private conflict  2018  6592}
\CommentTok{\#\textgreater{}  9 social disorder   2015  5043}
\CommentTok{\#\textgreater{} 10 social disorder   2016  4707}
\CommentTok{\#\textgreater{} 11 social disorder   2017  4876}
\CommentTok{\#\textgreater{} 12 social disorder   2018  4168}
\CommentTok{\#\textgreater{} 13 violence          2015 18621}
\CommentTok{\#\textgreater{} 14 violence          2016 18080}
\CommentTok{\#\textgreater{} 15 violence          2017 17172}
\CommentTok{\#\textgreater{} 16 violence          2018 16630}
\end{Highlighting}
\end{Shaded}

One of the dplyr quirks is that after you \texttt{summarise()}, one level of grouping is removed. This is because we have already performed our aggregate measure and the last level of grouping is now unit of analysis (a row). Since we are currently grouped at the \texttt{measure} level and each row represent one year we are already ready to calculate the average \texttt{n} value by group. Rather than using \texttt{summarise()} as we are used to doing with summary statistics, we will use \texttt{mutate()} and create a column called \texttt{avg} with the average \texttt{n} value for each group. This is because we want to be able to perform a column-wise operation to subtract the average from each row's \texttt{n} value.

I continue by creating three new columns. The first is \texttt{avg} which is set to the mean of \texttt{n} by group. Since the resulting value of \texttt{mean(n)} is a single value, each observation in the group gets that value. Second, I create a new column called \texttt{deviation} which subtracts the new column we created from the \texttt{n} column. The lastly I created a new column to contain the name of the ecometric as well as the year. This is done to ensure that in the visualization each row can be plotted.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{annual\_metrics \textless{}{-}}\StringTok{ }\NormalTok{ecometrics }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\CommentTok{\# group by measure and year}
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(measure, year) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\CommentTok{\# find the total n for each measure and year}
\StringTok{  }\CommentTok{\# summarise loses the last level of grouping}
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n =} \KeywordTok{sum}\NormalTok{(n)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \CommentTok{\# calculate average \textasciigrave{}n\textasciigrave{} by ecometric}
    \DataTypeTok{avg =} \KeywordTok{mean}\NormalTok{(n),}
    \CommentTok{\# calculate the deviation from the mean }
    \DataTypeTok{deviation =}\NormalTok{ n }\OperatorTok{{-}}\StringTok{ }\NormalTok{avg,}
    \CommentTok{\# creating a new column that combines the name of the ecometric and the year}
    \DataTypeTok{name =}\NormalTok{ glue}\OperatorTok{::}\KeywordTok{glue}\NormalTok{(}\StringTok{"\{measure\}: \{year\}"}\NormalTok{),}
\NormalTok{  ) }
\end{Highlighting}
\end{Shaded}

I snuck in a new package right there. \texttt{glue} is a package that let's us incorporate R expressions with plain text. Whatever is inside of the brackets \texttt{\{} will be run as an R expression. So since I referenced the columns \texttt{measure} and \texttt{year} as \texttt{"\{measure\}:\ \{year\}"} each value of \texttt{name} will look something like \texttt{social\ disorder:\ 2017}.

Now that we have a data frame with all of the values we want to plot, let's go ahead and do that! We will use a lollipop chart here to do so.

\begin{quote}
Remember that \texttt{geom\_lollipop()} comes from the \texttt{ggalt} package. We can use the function without loading the whole package by referencing the \texttt{pkgname::function\_name()}.
\end{quote}

Within the chart we map \texttt{devation} to the color aesthetic.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(p \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(annual\_metrics, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{y =}\NormalTok{ deviation, }\DataTypeTok{x =}\NormalTok{ name, }\DataTypeTok{color =}\NormalTok{ deviation)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\NormalTok{ggalt}\OperatorTok{::}\KeywordTok{geom\_lollipop}\NormalTok{(}\DataTypeTok{size =} \DecValTok{2}\NormalTok{) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{coord\_flip}\NormalTok{())}
\CommentTok{\#\textgreater{} Registered S3 methods overwritten by \textquotesingle{}ggalt\textquotesingle{}:}
\CommentTok{\#\textgreater{}   method                  from   }
\CommentTok{\#\textgreater{}   grid.draw.absoluteGrob  ggplot2}
\CommentTok{\#\textgreater{}   grobHeight.absoluteGrob ggplot2}
\CommentTok{\#\textgreater{}   grobWidth.absoluteGrob  ggplot2}
\CommentTok{\#\textgreater{}   grobX.absoluteGrob      ggplot2}
\CommentTok{\#\textgreater{}   grobY.absoluteGrob      ggplot2}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-24-1.pdf}
The above graphic doesn't take into account the middle value of \texttt{0}. Because of this, we need to tell ggplot2 that this is a diverging color scale. When we were working with a normal continuous color scale we used \texttt{scale\_color\_gradient()}. Instead, since we have a middle value, to create the diverging color gradient we use \texttt{scale\_color\_gradient2()}. This adds two more arguments \texttt{mid} and \texttt{midpoint}. The former is the color of that middle value. The second is what that middle value maps to---defaults to 0, which is fine for our example.

Now if we add the below colors---a red, yellow-ish beige, and a blue---we will have a diverging color scale on the plot!

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{scale\_color\_gradient2}\NormalTok{(}\DataTypeTok{low =} \StringTok{"\#A12106"}\NormalTok{, }\DataTypeTok{mid =} \StringTok{"\#B2CEB7"}\NormalTok{, }\DataTypeTok{high =} \StringTok{"\#004C76"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-25-1.pdf}

Information design is a seemingly endless field of which we only touched on a very small amount. The R community has put a lot of work into enabling the use of color for visualization purposes. The above images of color palettes were created with the help of the wonderful packages \texttt{prismatic} and \texttt{paletteer}

To explore color more check out the packages \href{https://github.com/EmilHvitfeldt/prismatic}{\texttt{prismatic}} and \href{https://github.com/EmilHvitfeldt/paletteer}{\texttt{paletteer}} by \href{https://www.hvitfeldt.me/blog/}{Emil Hvitfeld}.

\hypertarget{shape-and-size}{%
\section{Shape and Size}\label{shape-and-size}}

We spent a fair amount of time looking at a number of the ways that color can be used. Color can be used to visualize such a wide variety and as such was deserving of such a long section. Here we will briefly look at the other two aesthetics: shape and color.

\hypertarget{shape}{%
\subsection{Shape}\label{shape}}

Shape is another useful way to visualize groups or categories in our data. In general we should use shape only if color is not an option for us. Variations in shapes can be more difficult to discern---particularly when the number of groups to be visualized reaches beyond \textasciitilde4. Moreover, shapes, depending on choice and intricacy can become overly distracting and can detract from the visualization as a whole. As a heuristic, do not map both shape and color to the same variable.

To illustrate the use of shape we will use data from Inside Airbnb \footnote{Inside Airbnb. \url{http://insideairbnb.com/}.
  \footnotemark{}: Big Belly. \url{https://data.boston.gov/dataset/big-belly-alerts-2014/resource/8fb74119-97b9-4114-b773-ea0a82142d3b}.}. This dataset contains Airbnb listings for Boston as well. The dataset can be found at \texttt{data/airbnb/airbnb.csv}. We will go into the dataset in more depth in the chapter on spatial analysis.

For this visualization we will read in the dataset, filter it down to just \texttt{"Back\ Bay"}, and then plot ponts based on their place in space---aka latitude and longitude. We will also map shape to the \texttt{room\_type}. Doing this will show us the spatial distribution of Airbnbs as well as the different kinds.

\begin{quote}
Note that while we normally say ``latitude and longitude'', that actually is saying ``y and x'' respectively. So be sure to put the latitude in the y aesthetic position and \emph{not} longitude.
\end{quote}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# read in data}
\NormalTok{airbnb \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/airbnb.csv"}\NormalTok{)}
\CommentTok{\#\textgreater{} Parsed with column specification:}
\CommentTok{\#\textgreater{} cols(}
\CommentTok{\#\textgreater{}   id = col\_double(),}
\CommentTok{\#\textgreater{}   neighborhood = col\_character(),}
\CommentTok{\#\textgreater{}   room\_type = col\_character(),}
\CommentTok{\#\textgreater{}   price = col\_double(),}
\CommentTok{\#\textgreater{}   longitude = col\_double(),}
\CommentTok{\#\textgreater{}   latitude = col\_double()}
\CommentTok{\#\textgreater{} )}

\CommentTok{\# filter to backbay}
\NormalTok{bb \textless{}{-}}\StringTok{ }\NormalTok{airbnb }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{filter}\NormalTok{(neighborhood }\OperatorTok{==}\StringTok{ "Back Bay"}\NormalTok{) }

\CommentTok{\# plot the points by shape}
\KeywordTok{ggplot}\NormalTok{(bb,  }\KeywordTok{aes}\NormalTok{(longitude, latitude, }\DataTypeTok{shape =}\NormalTok{ room\_type)) }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{alpha =} \FloatTok{.5}\NormalTok{, }\DataTypeTok{size =} \DecValTok{3}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-26-1.pdf}

By changing each point's shape by their associated room type we can get a somewhat better idea of the spatial distribution by type.

What can we tell from this visualization? It looks like most Airbnbs are overwhelmingly the entire home or appartment. We can infer from this that most Airbnbs \_are not\_being used as additional revenue for residents. But rather that each Airbnb might be a unit of housing that is no longer available to Boston residents. Could this be increasing demand for housing? Is the rise in Airbnbs creating a shortage of housing and can it be one of the factors behind the rising Boston rents?

The above visualization is good, but lets compare that with the use of color.

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-27-1.pdf}

Which do you prefer? Which do you feel does a better job getting this message across? Again, we are delving into the world of design and there is never a \emph{correct} answer. But sometimes there may be a consensus. So just do your best and ask others for their thoughts on your visualizations.

\hypertarget{size}{%
\subsection{Size}\label{size}}

Moving onto size. Size is an aesthetic that is one of the hardest for humans to properly distinguish between. Because of this, size should usually be used when comparing observations with large discrepancies between them. Doing this with ggplot is again rather straight forward as here the only difference is, is that we set the aesthetic \texttt{size} to some other column (this must be numeric).

This visualization will use data from Analyze Boston's legacy database. Analyze Boston has previously released all signals from their Big Belly trashcans for the year of 2014\footnote{\href{https://data.boston.gov/dataset/big-belly-alerts-2014/resource/8fb74119-97b9-4114-b773-ea0a82142d3b}{Analyze Boston: Big Belly Alerts 2014}}. In the words of the data dictionary:

\begin{quote}
Bigbelly's contain compactors which crush waste within their waste bins in order to reduce the volume required to store the waste deposited into the Bigbelly. The compactors are able provide an average compaction ratio of 5:1, meaning a Bigbelly can store the equivalent of 150G (gallons) of uncompactedwaste within a standard 30G waste bin. The compactor in a Bigbelly will run when it has detected that waste within the bin has reached a certain level. After a compaction cycle the waste will be crushed to occupy a smaller amount of space within the waste bin. These cycles are repeated as more waste is added to the Bigbelly. After approximately 150G of uncompacted waste has been added to the Bigbelly the compactor will not be able to compress the waste further, this condition is detected and thewaste bin is considered to be full.
\end{quote}

For the sake of example, I have created an aggregate count of signals from all Big Belly receptacles for 2014. This file can be found at \texttt{data/downtown-big-belly.csv}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{big\_belly \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/downtown{-}big{-}belly.csv"}\NormalTok{)}

\KeywordTok{glimpse}\NormalTok{(big\_belly)}
\CommentTok{\#\textgreater{} Rows: 147}
\CommentTok{\#\textgreater{} Columns: 4}
\CommentTok{\#\textgreater{} $ description \textless{}chr\textgreater{} "1 North Market Street  (in front of McCormick \& Schmic...}
\CommentTok{\#\textgreater{} $ n           \textless{}dbl\textgreater{} 47, 179, 257, 108, 171, 93, 27, 173, 55, 64, 126, 190, ...}
\CommentTok{\#\textgreater{} $ lat         \textless{}dbl\textgreater{} 42.36034, 42.35653, 42.35634, 42.35557, 42.35520, 42.35...}
\CommentTok{\#\textgreater{} $ long        \textless{}dbl\textgreater{} {-}71.05582, {-}71.06190, {-}71.06203, {-}71.06288, {-}71.06321, ...}
\end{Highlighting}
\end{Shaded}

Similar to the Airbnb data, we will visualize by latitude and longitude (\texttt{lat} and \texttt{long} respectively) while mapping the \texttt{size} aesthetic to \texttt{n}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(big\_belly, }\KeywordTok{aes}\NormalTok{(long, lat, }\DataTypeTok{size =}\NormalTok{ n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{(}\DataTypeTok{alpha =} \DecValTok{1}\OperatorTok{/}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04d-beyond-2d_files/figure-latex/unnamed-chunk-29-1.pdf}

\hypertarget{visualizing-through-time}{%
\chapter{Visualizing through time}\label{visualizing-through-time}}

Throughout this section, one type of visualization has been missing from our repetoire---the timeseries plot. This is because time series data is rather cumbersome to work with. Time series are unique because each observation represents some point in time. There is order inherent to the data. Because of this natural ordering of the data it makes the problem a bit trickier. But now that we have the visual tools and principles sorted out, we can apply them to time series data and visualize them as well.

For this section we will work with the 2014 Big Belly data again. Our goal will be to visualize the reports by fullness over the course of the year. In this chapter we will first learn how to work with dates. Then we will use our new skills to aggregate our observations and view them over time. Next, we will discuss traditional methods of time series visualization. And finally we will quickly touch on the use of animation for viewing time-series.

\hypertarget{working-with-dates}{%
\section{Working with dates}\label{working-with-dates}}

The file that we will use is located at \texttt{data/big-belly-2014.csv}. Read in the dataset and assign it to the tibble \texttt{big\_belly} and then preview it with \texttt{glimpse()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{big\_belly \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/big{-}belly{-}2014.csv"}\NormalTok{)}

\KeywordTok{glimpse}\NormalTok{(big\_belly)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 51,440
## Columns: 6
## $ description <chr> "Atlantic & Milk", "1330 Boylston @ Jersey Street", "SE...
## $ timestamp   <dttm> 2014-01-01 00:41:00, 2014-01-01 01:19:00, 2014-01-01 0...
## $ fullness    <chr> "YELLOW", "YELLOW", "YELLOW", "RED", "GREEN", "YELLOW",...
## $ collection  <lgl> FALSE, FALSE, FALSE, FALSE, TRUE, TRUE, FALSE, TRUE, FA...
## $ lat         <dbl> 42.35870, 42.34457, 42.34818, 42.34818, 42.34933, 42.34...
## $ long        <dbl> -71.05144, -71.09783, -71.09744, -71.09744, -71.07702, ...
\end{verbatim}

What we can see is that the data are rather large and contains a column with the type \texttt{\textless{}dttm\textgreater{}} which is new to us. \texttt{\textless{}dttm\textgreater{}} is the way that a tibble represents a column of the type date time.

\begin{quote}
Note that the formal class for date time data is \texttt{POSIXct}. Working with date times with any computer is a tricky process.
\end{quote}

Date time objects follow the format of \texttt{YYYY-MM-DD\ HH:MM:SS} where \texttt{Y} is year, \texttt{M} month, \texttt{D} day, \texttt{H} hours, \texttt{M} minutes, and \texttt{S} seconds.

This begs the question ``how can we aggregate based on time?'' The easiest way to do this is to group our observations by some interval. To do this we will solicit the help of the package \href{https://lubridate.tidyverse.org}{\texttt{lubridate}}. lubridate is part of the tidyverse, but it doesn't loadwith the tidyverse. As such, we will have to load the package ourselves.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(lubridate)}
\end{Highlighting}
\end{Shaded}

The package contains dozens of useful functions for working with dates in R. By no means will we go over each of them.

\begin{quote}
To explore all of the functions in a package click on the \texttt{Packages} pane and search for the package you are interested in. Click the hyper link with the package name and then all of the exported functions and objects \emph{should} be documented there.
\end{quote}

We will, however quickly touch on \texttt{ymd()}, \texttt{month()}, and \texttt{floor\_date()}.

The first is not useful in this example since \texttt{read\_csv()} already parsed the column as a date time for us, but is important nonetheless. \texttt{ymd()} will convert a character string into a date object. The letters stand for \emph{year}, \emph{month}, and \emph{date}. This function parses dates in the format of yup, you guessed it, year-month-day. For example:

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2020 01 20"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-01-20"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\StringTok{"2020 jan. 20"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-01-20"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ymd}\NormalTok{(}\StringTok{"20, january{-}20"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-01-20"
\end{verbatim}

There are others such as \texttt{mdy()} and \texttt{dmy()} which you can use to parse dates as well.

\begin{quote}
The next time you are trying to parse a date think about the way it is formatted. Does the year precede or follow the month?
\end{quote}

Immediately useful to us is the \texttt{month()} function. This will extract the month component of a given date.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{month}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] 1
\end{verbatim}

The above returned the value of \texttt{1} because January is represented by 1. If we wished to return the full name of the month we set the argument \texttt{label\ =\ TRUE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{month}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] Jan
## 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec
\end{verbatim}

Furthermore, we can tell lubridate to not abbreviate the months by setting \texttt{abbr\ =\ FALSE}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{month}\NormalTok{(}\StringTok{"2020{-}01{-}01"}\NormalTok{, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{, }\DataTypeTok{abbr =} \OtherTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] January
## 12 Levels: January < February < March < April < May < June < ... < December
\end{verbatim}

We can use the above function to extract the months from our Big Belly dataset. This will be extremely useful in aggregation. The one limitation, however, is that if there were another year present we would be grouping observations from both years into the the same month bucket. To avoid this, we can use the function \texttt{floor\_date()}. \texttt{floor\_date()} takes a date object and returns the closest date rounding down to a given unit. This may be best explained through an example. Given the date March 15th, 2020, let us round down to the near units \texttt{"week"}, \texttt{"month"}, and \texttt{"year"}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# create date object}
\NormalTok{mar\_}\DecValTok{15}\NormalTok{ \textless{}{-}}\StringTok{ }\KeywordTok{ymd}\NormalTok{(}\StringTok{"2020{-}03{-}15"}\NormalTok{)}

\CommentTok{\# round to week}
\KeywordTok{floor\_date}\NormalTok{(mar\_}\DecValTok{15}\NormalTok{, }\DataTypeTok{unit =} \StringTok{"week"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-03-15"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# round to month}
\KeywordTok{floor\_date}\NormalTok{(mar\_}\DecValTok{15}\NormalTok{, }\DataTypeTok{unit =} \StringTok{"month"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-03-01"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# round to year}
\KeywordTok{floor\_date}\NormalTok{(mar\_}\DecValTok{15}\NormalTok{, }\DataTypeTok{unit =} \StringTok{"year"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## [1] "2020-01-01"
\end{verbatim}

With these new tools lets modify our \texttt{big\_belly} tibble by creating two new columns \texttt{month} (with the labels) and \texttt{week} using \texttt{month()} and \texttt{floor\_date()} respectively and assign the result to an object called \texttt{bb}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bb \textless{}{-}}\StringTok{ }\NormalTok{big\_belly }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}
    \DataTypeTok{month =} \KeywordTok{month}\NormalTok{(timestamp, }\DataTypeTok{label =} \OtherTok{TRUE}\NormalTok{),}
    \DataTypeTok{week =} \KeywordTok{floor\_date}\NormalTok{(timestamp, }\DataTypeTok{unit =} \StringTok{"week"}\NormalTok{)}
\NormalTok{  )}

\KeywordTok{slice}\NormalTok{(bb, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 8
##   description timestamp           fullness collection   lat  long month
##   <chr>       <dttm>              <chr>    <lgl>      <dbl> <dbl> <ord>
## 1 Atlantic &~ 2014-01-01 00:41:00 YELLOW   FALSE       42.4 -71.1 Jan  
## 2 1330 Boyls~ 2014-01-01 01:19:00 YELLOW   FALSE       42.3 -71.1 Jan  
## 3 SE Brookli~ 2014-01-01 01:32:00 YELLOW   FALSE       42.3 -71.1 Jan  
## 4 SE Brookli~ 2014-01-01 01:34:00 RED      FALSE       42.3 -71.1 Jan  
## 5 Huntington~ 2014-01-01 02:10:00 GREEN    TRUE        42.3 -71.1 Jan  
## # ... with 1 more variable: week <dttm>
\end{verbatim}

\hypertarget{standard-visual-approach}{%
\section{Standard visual approach}\label{standard-visual-approach}}

When we visualize time we need to consider how we humans cognitively and visually perceive time. Time is linear. It follow a path from start to end. Because of this we want to (almost) always plot our time dimension on the horizontal x axis as time proceeds forwards not up or down. Perhaps as a product of these restrictions there are not many variations on how we can visualize time. Two plots reign in time-series: the line and the bar plot.

We can take our \texttt{bb} tibble and create a barchart of the total number of observations per month.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(month, n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04e-through-time_files/figure-latex/unnamed-chunk-9-1.pdf}

We can add a few more characters to the above code chunk so we can compare fullness over the months as well.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(month, n, }\DataTypeTok{fill =}\NormalTok{ fullness)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04e-through-time_files/figure-latex/unnamed-chunk-10-1.pdf}

Before we continue, we have to address the elephant in the room. We just made a plot with a legend that makes no sense. \texttt{GREEN} is mapped to red, \texttt{RED} is mapped to green, and \texttt{YELLOW} is mapped to blue. That's not good. We can fix this by adding manually mapping the color using one of the scale functions \texttt{scale\_fill\_manual(values\ =\ c("green",\ "red",\ "yellow"))}. Now back to the task at hand---time series.

The stacked barchart above runs into the same limitations mentioned previously. And given the number of different time groups creating a dodged bar chart will become to cluttered by putting 36 total bars on there. We could consider faceting.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(fullness, n, }\DataTypeTok{group =}\NormalTok{ month)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{facet\_wrap}\NormalTok{(}\StringTok{"month"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{04e-through-time_files/figure-latex/unnamed-chunk-11-1.pdf}
This is okay, but again, still too cluttered. In this scenario we should consider the use of a line graph. The line graph is the de facto time series plot. The lines connect points in time. Achieving this with ggplot is rather straight forward. We will put our time dimension on the x axis and add the \texttt{geom\_line()} layer. By default \texttt{geom\_line()} doesn't know what points to connect to each other when there is more than one per x value. So in our case we need to tell ggplot which points to connect by setting the \texttt{group} aesthetic. Since we want to group together the lines by \texttt{fullness} level set \texttt{group} as well as \texttt{color} to \texttt{fullness}. We can step up the plotting game by also

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(month, n, }\DataTypeTok{group =}\NormalTok{ fullness, }\DataTypeTok{color =}\NormalTok{ fullness)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_line}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04e-through-time_files/figure-latex/unnamed-chunk-12-1.pdf}

We can step up our plotting game by also adding a layer of points on top of the line to accentuate our lines.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(month, n, }\DataTypeTok{group =}\NormalTok{ fullness, }\DataTypeTok{color =}\NormalTok{ fullness)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{04e-through-time_files/figure-latex/unnamed-chunk-13-1.pdf}
The three types are graphs are all that you may ever truly need when visualizing time. But sometimes it's nice to go over the top and add all of the frills. Often a dynamic visualization may be more persuasive than a static one.For this, we can use animation and the \texttt{gganimate} package.

\hypertarget{animation-as-time}{%
\section{Animation as time}\label{animation-as-time}}

In 2008 researchers at Microsoft and a scholar from Georgia Institute of Technology published a paper titled \emph{Effectiveness of Animation in Trend Visualization}\footnote{Effectiveness of Animation in Trend Visualization. \url{https://www.cc.gatech.edu/~stasko/papers/infovis08-anim.pdf}.}. This paper explored the reaction to a talk from 2006 which captured the attention of many by using animation to visualize global trends in health. That TED talk by Hans Rosling was a moving utilization of data visualization\footnote{Hans Rosling. \url{https://www.ted.com/talks/hans_rosling_the_best_stats_you_ve_ever_seen/transcript}.}. But why? This paper presented interactive and static visualizations to discover people's perspective on animation and its alternatives. They found that

\begin{quote}
``users really liked the animation view: Study participants described it as''fun``,''exciting``, and even''emotionally touching." At the same time, though, some participants found it confusing: ``the dots flew everywhere.''\footnote{Effectiveness of Animation in Trend Visualization. \url{https://www.cc.gatech.edu/~stasko/papers/infovis08-anim.pdf}.}
\end{quote}

We now have scientific backing that animation is pretty good but also we're unsure---which seems to be in line with the rest of science. We will take the above quote as a measure of caution. That we \emph{can} creaate animations but also be careful with our use of them. For the remainder of this chapter we will learn about the the extension to ggplot2, gganimate. gganimate extends ggplot by extending the grammar to include elements pertaining solely to animation.

When working with gganimate we need to think in the context of \emph{frames}. Each frame will be a cross-section of our data. Or you can think of each frame a still in our film. In the context of time-series each frame will be our period of time. So if we continue with our Big Belly example above, each frame can be thought of as each month.

To create animations we will need to install four packages: \texttt{gganimate}, \texttt{gifski} (what a brilliant name), \texttt{png}, and \texttt{transformr}. You can do so by running \texttt{install.packages(c("gganimate",\ "gifski",\ "png",\ "transformr"))}.

For extending ggplot2, gganimate adds three main types of layers to ggplot. There are most importantly the \texttt{transition\_*()}s and the \texttt{enter\_*()} and \texttt{exit\_*()} layers. In this chapter we will only address the \texttt{transition\_*()} layers. The transitions will be used to move through our time dimension. For working with time data we will use \texttt{transition\_states()} and \texttt{transition\_reveal()}. These are best used with bar charts and line plot respectively.

Since we will be changing our frames by our time dimension---\texttt{month} in this case---we start building our ggplot without it included. So, to create an animation of the counts by fullness by month we actually just start by visualizing the total counts.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(p \textless{}{-}}\StringTok{ }\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(fullness, n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_col}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{04e-through-time_files/figure-latex/unnamed-chunk-14-1.pdf}
All that is needed to add animation to this plot is a transition layer. For the use of a bar chart we can use \texttt{transition\_states(states)} where states is the unquoted name of the column that wil be transitioned through.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(gganimate)}

\NormalTok{p }\OperatorTok{+}\StringTok{ }\KeywordTok{transition\_states}\NormalTok{(month)  }
\end{Highlighting}
\end{Shaded}

This is awesome! We've created an animation 😮. The bars change heigh with each change of state. The only bummer is that we don't know what states the animation is going through! When the animation is made there are four temporary variables made available for labeling. The documentation for \texttt{transition\_states()} notes that the following variables are available\footnote{\texttt{transition\_states()}. \url{https://gganimate.com/reference/transition_states.html}.}.

\begin{itemize}
\tightlist
\item
  \textbf{transitioning} is a boolean indicating whether the frame is part of the transitioning phase
\item
  \textbf{previous\_state} The name of the last state the animation was at
\item
  \textbf{closest\_state} The name of the state closest to this frame
\item
  \textbf{next\_state} The name of the next state the animation will be part of
\end{itemize}

We can add a label layer to make this much more informative. To reference these variables we can use glue like quote strings with the above variables.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{transition\_states}\NormalTok{(month)  }\OperatorTok{+}
\StringTok{  }\KeywordTok{labs}\NormalTok{(}\DataTypeTok{title =} \StringTok{"\{closest\_state\}"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

This ts the scaffolding for a great visualization. We can take this existing plot and add or adjust the layers to adjust the styling to be more engaging.

We can also add animation to our previous line plot. We can use the \texttt{transition\_reveal()} layer to reveal changes over time.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(month, n, }\DataTypeTok{group =}\NormalTok{ fullness, }\DataTypeTok{color =}\NormalTok{ fullness)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{transition\_reveal}\NormalTok{(month)}

\NormalTok{Error}\OperatorTok{:}\StringTok{ }\NormalTok{along data must either be integer, numeric, POSIXct, Date, difftime, orhms}
\end{Highlighting}
\end{Shaded}

Note the above error. This is telling us that since our x, or time dimension is actually being treated as a factor, it cannot move along the axis. The error message is informative enough to let us know that we need to change month to a numeric value. As a way to address this but yet maintain the nice labeling on the x axis we can create another column called \texttt{month\_integer} which is jsut the integer value of the month---so Jan is 1 etc. We can provide that value to \texttt{transition\_reveal()}. Moreover \texttt{transition\_reveal()} creates the \texttt{frame\_along} temporary variable if you'd so like to use it.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(bb, month, fullness) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{month\_integer =} \KeywordTok{as.integer}\NormalTok{(month)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(month, n, }\DataTypeTok{group =}\NormalTok{ fullness, }\DataTypeTok{color =}\NormalTok{ fullness)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_line}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{transition\_reveal}\NormalTok{(month\_integer)}
\end{Highlighting}
\end{Shaded}

This animation does a great job of illustrating the changing slopes of the fullness measures.

While it may seem a little underwhelming, these two functions can provide you with most of the functionality you will need for creating animations. Explore the functions in the package to explore how you can add to your animation and make it truly yours.

\begin{quote}
\texttt{help(package\ =\ "gganimate")} will bring up all of the help documentation for the package.
\end{quote}

And with that, we can conclude this section. Congratulations!

\hypertarget{visualization-review}{%
\chapter{Visualization Review}\label{visualization-review}}

So here we are, at the end of the third and longest section of the Urban Informatics Toolkit. We've covered a lot of ground and rather quickly so let's recap.

We started by going over the grammar of graphics. The grammar is used to define the components of a visualization and ggplot2 is the R implementation of the graphics. The grammar has five main components:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Defaults:

  \begin{itemize}
  \tightlist
  \item
    Data
  \item
    Mapping
  \end{itemize}
\item
  Layers:

  \begin{itemize}
  \tightlist
  \item
    Data
  \item
    Mapping
  \item
    Geom
  \item
    Stat
  \item
    Position
  \end{itemize}
\item
  Scales
\item
  Coordinates
\item
  Facets
\end{enumerate}

We build plots by providing data either to \texttt{ggplot()} which sets the defaults, or the data can be provided directly to the geom layers. Remember that the \texttt{geom\_*()} layers are what creates the geometry on the plots. Without them we do not populate the graphic. The layers figure out the positions, scales, and coordinates from the data. We can also adjust these to fit our preference by using \texttt{scale\_*()} and \texttt{coord\_()} layerss if we would so desire.

Following this, we explored the ways in which univariate and bivariate relationships can be explored visually. In this we explored the use of a number of different plots which can be added to your repertoire. Then we looked at how we can use facetting, color, shape, and size to explore beyond two dimensions. And finally, we briefly looked at the use of animation to explore data through time.

All of the visualization strategies can be used either independently or in combination to create compelling graphics thats tell a story from data.

In the next sectino we will cover a few more advanced and disparate topics which are important to have in your tool kit. It may be good to take a break right now before we continue.

Are you hydrated?

\hypertarget{part-more-than-hammer-and-nails}{%
\part{More than hammer and nails}\label{part-more-than-hammer-and-nails}}

\hypertarget{multiple-data-sets}{%
\chapter{Multiple data sets}\label{multiple-data-sets}}

Many times working with just one data set will not suffice. More often than not the data that we will be working with be will need to be supplemented by other data sets. These datasets have a shared relation which are what enables us to \textbf{join} them together. A join is a way of combining the columns between two sets of data while ensuring the rows are properly aligned.

The relationship that joins these tables together are expressed in the data through what is called a \textbf{common identifier}. This a variable exists in both datasets---perhaps, with a different name---and can be used as a reference to associate rows from each table together.

Recall to our definition of tidy data. The definition that we used characterized the data inside of a single table. There is actually another rule: ``each observational unit forms a table.'' So, in our previous definition a row was a single observation. When we extend the definition as such this is still the case but we have to create distinctions between ``observational units{[}s{]}.''

To explore this concept we will use data about Airbnb listings in Boston. These data come from \href{http://insideairbnb.com}{Inside Airbnb}. Inside Airbnb collects Airbnb's public listing and makes them available through a ``non-commercial set of tools and data that allows you to explore how Airbnb is really being used in cities around the world''\footnote{Inside Airbnb. \url{http://insideairbnb.com/about.html}.}.

Inside Airbnb data are another example of harnessing naturally occurring data. The listings are not generated with the intent of \emph{being} data, but by the virtue of their existence, they become data. We are able to harness them to learn about the shifting neighborhood dynamic of cities. They can tell us something about short term rentals in a locale and about where people may be visiting and when.

We will first look at two data sets: \texttt{listings}, and \texttt{hosts}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{listings \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/listings.csv"}\NormalTok{)}

\KeywordTok{glimpse}\NormalTok{(listings)}
\CommentTok{\#\textgreater{} Rows: 3,799}
\CommentTok{\#\textgreater{} Columns: 7}
\CommentTok{\#\textgreater{} $ id               \textless{}dbl\textgreater{} 3781, 5506, 6695, 8789, 10730, 10813, 10986, 16384...}
\CommentTok{\#\textgreater{} $ neighborhood     \textless{}chr\textgreater{} "East Boston", "Roxbury", "Roxbury", "Downtown", "...}
\CommentTok{\#\textgreater{} $ room\_type        \textless{}chr\textgreater{} "Entire home/apt", "Entire home/apt", "Entire home...}
\CommentTok{\#\textgreater{} $ price            \textless{}dbl\textgreater{} 125, 145, 169, 99, 150, 179, 125, 50, 154, 115, 14...}
\CommentTok{\#\textgreater{} $ minimum\_nights   \textless{}dbl\textgreater{} 28, 3, 3, 91, 91, 91, 91, 91, 29, 1, 2, 2, 2, 6, 3...}
\CommentTok{\#\textgreater{} $ availability\_365 \textless{}dbl\textgreater{} 68, 322, 274, 247, 29, 0, 364, 365, 304, 285, 62, ...}
\CommentTok{\#\textgreater{} $ host\_id          \textless{}dbl\textgreater{} 4804, 8229, 8229, 26988, 26988, 38997, 38997, 2307...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{hosts \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/hosts.csv"}\NormalTok{)}

\KeywordTok{glimpse}\NormalTok{(hosts)}
\CommentTok{\#\textgreater{} Rows: 1,335}
\CommentTok{\#\textgreater{} Columns: 9}
\CommentTok{\#\textgreater{} $ id              \textless{}dbl\textgreater{} 4804, 8229, 26988, 38997, 23078, 71783, 85130, 8577...}
\CommentTok{\#\textgreater{} $ name            \textless{}chr\textgreater{} "Frank", "Terry", "Anne", "Michelle", "Eric", "Lanc...}
\CommentTok{\#\textgreater{} $ since\_year      \textless{}dbl\textgreater{} 2008, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 201...}
\CommentTok{\#\textgreater{} $ since\_month     \textless{}chr\textgreater{} "12", "02", "07", "09", "06", "01", "02", "02", "03...}
\CommentTok{\#\textgreater{} $ since\_day       \textless{}chr\textgreater{} "03", "19", "22", "16", "24", "19", "24", "26", "23...}
\CommentTok{\#\textgreater{} $ response\_rate   \textless{}chr\textgreater{} "100\%", "100\%", "100\%", "92\%", "50\%", "98\%", "66\%",...}
\CommentTok{\#\textgreater{} $ acceptance\_rate \textless{}chr\textgreater{} "50\%", "100\%", "84\%", "17\%", "N/A", "98\%", "97\%", "...}
\CommentTok{\#\textgreater{} $ superhost       \textless{}dbl\textgreater{} 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...}
\CommentTok{\#\textgreater{} $ n\_listings      \textless{}dbl\textgreater{} 5, 2, 9, 13, 3, 40, 7, 4, 1, 2, 1, 1, 5, 1, 1, 1, 1...}
\end{Highlighting}
\end{Shaded}

Notice that the each row of the table is an observation of a different phenomenon. Each observation in \texttt{listings} is rentable dwelling and each row in \texttt{hosts} is a different individual. One host may have multiple listings. This sort of relationship is called \emph{one to many} meaning that for each host there is one or more listings. This is rather evident in the number of observations in each table---3,799 and 1,335 respectively.

To associate the host information with the listings information we need to join the two datasets together. Most often the common identifier will be a type of id (often referred to as keys). Identifiers are meant to be unique at the observational unit. If we look at the variables of our tibbles we notice that \texttt{listings} contains \texttt{host\_id} and \texttt{hosts} an \texttt{id} column. Fortunately the first two rows of each dataset share identifiers.

Before we get to joining our datasets, we need to understand the different types of joins that are available to us. When doing joins we consider two tables: one on the left hand side and the other on the right hand side referred to in documentation as \emph{x} and \emph{y} respectively. There are a plethora of joins that are possible but I want to keep the focus to four types. These are the left, right, inner, and anti joins.

The left and right join are identical, the only difference is which table we may consider the target of the join. In the case of the left join, it will

\begin{quote}
``return all rows from x, and all columns from x and y. Rows in x with no match in y will have NA {[}missing{]} values in the new columns. If there are multiple matches between x and y, all combinations of the matches are returned.''
\end{quote}

Simply, in a left join we will never lose rows from x. If there happens to be no match from y there will be missing values. The reverse is true for the right join. A right join ensures that all rows from y will remain at the end. If there are duplicate matches between x and y each of those will be returned.

For the sake of example, let's reduce our data down to the first five rows and only a few columns for each. Hosts data will be stored in the tibble \texttt{h} and listings in \texttt{l}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{h \textless{}{-}}\StringTok{ }\KeywordTok{slice}\NormalTok{(hosts, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(id, name, n\_listings)}

\NormalTok{l \textless{}{-}}\StringTok{ }\KeywordTok{slice}\NormalTok{(listings, }\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(host\_id, price, neighborhood)}

\NormalTok{h}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 3}
\CommentTok{\#\textgreater{}      id name     n\_listings}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1  4804 Frank             5}
\CommentTok{\#\textgreater{} 2  8229 Terry             2}
\CommentTok{\#\textgreater{} 3 26988 Anne              9}
\CommentTok{\#\textgreater{} 4 38997 Michelle         13}
\CommentTok{\#\textgreater{} 5 23078 Eric              3}
\NormalTok{l}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 3}
\CommentTok{\#\textgreater{}   host\_id price neighborhood}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}       }
\CommentTok{\#\textgreater{} 1    4804   125 East Boston }
\CommentTok{\#\textgreater{} 2    8229   145 Roxbury     }
\CommentTok{\#\textgreater{} 3    8229   169 Roxbury     }
\CommentTok{\#\textgreater{} 4   26988    99 Downtown    }
\CommentTok{\#\textgreater{} 5   26988   150 Downtown}
\end{Highlighting}
\end{Shaded}

In \texttt{h} we have one observation per host meaning that there are 5 distinct hosts in our dataset. In general when there is only one id per observation for a table, that is generally referred to as the \textbf{primary key} as it is used to identify observations in that table. In this case, \texttt{id} is the primary key for \texttt{h}. Whereas when we look to \texttt{l}, there are multiple observations for the \texttt{host\_id}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(l, host\_id)}
\CommentTok{\#\textgreater{} \# A tibble: 3 x 2}
\CommentTok{\#\textgreater{}   host\_id     n}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{} \textless{}int\textgreater{}}
\CommentTok{\#\textgreater{} 1    4804     1}
\CommentTok{\#\textgreater{} 2    8229     2}
\CommentTok{\#\textgreater{} 3   26988     2}
\end{Highlighting}
\end{Shaded}

When the ID is used to identify observations in another table, that is referred to as the \textbf{foreign key}. Here, \texttt{host\_id} is used to connect \texttt{l} to the \texttt{h} table and as such is a foreign key.

Say we want to know all of the host information associated with each listing we will need to join \texttt{h} to \texttt{l}. In this situations we want to keep all records of listings and only keep the host information when it is relevant. A left join with \texttt{l} as the left hand table and \texttt{h} as the right hand table will provide us with exactly what we need.

To actually perform the join, we will use the function \texttt{left\_join()} from dplyr. There are three arguments that we need to fulfill: \texttt{x}, \texttt{y}, and \texttt{by}. That is, what table will be on our left and which will be on the right? And, which column(s) will act as the common identifier.

Fulfilling the \texttt{x} and \texttt{y} arguments is straightforward, we simply supply the relevant tibbles. Marking the common identifier is a little bit tricky. \texttt{by} expects a character vector with the name of the common identifier. However, in most cases, the column names will differ by table. To connect them we need to create what is called a named vector.

In general we create a vector with \texttt{c()} and each element is separated by a comma. If we were to create a vector with the names of our identifier (from left to right) it would look like

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{, }\StringTok{"id"}\NormalTok{)}
\CommentTok{\#\textgreater{} [1] "host\_id" "id"}
\end{Highlighting}
\end{Shaded}

This is a vector of length 2. We need to create a vector of length 1 where the only element is named. We can name vector elements like \texttt{c("element\_name"\ =\ "element")}. For \texttt{left\_join()} the name of the element is the identifier in the left hand table and the element is the name of identifier in the right hand table. Putting this all together it looks like

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{)}
\CommentTok{\#\textgreater{} host\_id }
\CommentTok{\#\textgreater{}    "id"}
\end{Highlighting}
\end{Shaded}

Now we have all of the pieces that we need and can perform the join.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{left\_join}\NormalTok{(l, h, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 5}
\CommentTok{\#\textgreater{}   host\_id price neighborhood name  n\_listings}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}        \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1    4804   125 East Boston  Frank          5}
\CommentTok{\#\textgreater{} 2    8229   145 Roxbury      Terry          2}
\CommentTok{\#\textgreater{} 3    8229   169 Roxbury      Terry          2}
\CommentTok{\#\textgreater{} 4   26988    99 Downtown     Anne           9}
\CommentTok{\#\textgreater{} 5   26988   150 Downtown     Anne           9}
\end{Highlighting}
\end{Shaded}

There are no missing values in this join because each \texttt{host\_id} had a counter-part in the host table. Now, say we perform right join but keep our tables exactly where they are. What do you expect this to look like?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{right\_join}\NormalTok{(l, h, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 7 x 5}
\CommentTok{\#\textgreater{}   host\_id price neighborhood name     n\_listings}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}        \textless{}chr\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1    4804   125 East Boston  Frank             5}
\CommentTok{\#\textgreater{} 2    8229   145 Roxbury      Terry             2}
\CommentTok{\#\textgreater{} 3    8229   169 Roxbury      Terry             2}
\CommentTok{\#\textgreater{} 4   26988    99 Downtown     Anne              9}
\CommentTok{\#\textgreater{} 5   26988   150 Downtown     Anne              9}
\CommentTok{\#\textgreater{} 6   38997    NA \textless{}NA\textgreater{}         Michelle         13}
\CommentTok{\#\textgreater{} 7   23078    NA \textless{}NA\textgreater{}         Eric              3}
\end{Highlighting}
\end{Shaded}

There are two key differences here. The first is that, since the right hand table has unmatched \texttt{id}s, we should anticipate missing values in the columns from \texttt{l}---\texttt{price} and \texttt{neighborhood}. Moreover, since there are multiple matches in the left hand table, we are returned more than observation per match.

This naturally brings us to the inner join. The inner join will \emph{only} return row where there are matching observations in \emph{both} tables.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{inner\_join}\NormalTok{(l, h, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 5 x 5}
\CommentTok{\#\textgreater{}   host\_id price neighborhood name  n\_listings}
\CommentTok{\#\textgreater{}     \textless{}dbl\textgreater{} \textless{}dbl\textgreater{} \textless{}chr\textgreater{}        \textless{}chr\textgreater{}      \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1    4804   125 East Boston  Frank          5}
\CommentTok{\#\textgreater{} 2    8229   145 Roxbury      Terry          2}
\CommentTok{\#\textgreater{} 3    8229   169 Roxbury      Terry          2}
\CommentTok{\#\textgreater{} 4   26988    99 Downtown     Anne           9}
\CommentTok{\#\textgreater{} 5   26988   150 Downtown     Anne           9}
\end{Highlighting}
\end{Shaded}

Oftentimes, as with this time, the inner join behaves much like a left or right join where one of the tables has each observation matched. In general I recommend performing left or right joins so we can better keep track of missingness.

The last join to cover is the anti join. The anti join is rather unique and is used to find where there are \emph{not} matches between two tables. When using an anti join we are returned all observations from x where there are \emph{not} matches in y. From our previous right join, we know that there are not matches for two of the host ids in \texttt{h}. To find these using a join we can put \texttt{h} in the x position and \texttt{l} in the y position. Note that since we are switching the order of our tibbles we need to rearrange the vector we supplied to \texttt{by}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anti\_join}\NormalTok{(h, l, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"id"}\NormalTok{ =}\StringTok{ "host\_id"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 2 x 3}
\CommentTok{\#\textgreater{}      id name     n\_listings}
\CommentTok{\#\textgreater{}   \textless{}dbl\textgreater{} \textless{}chr\textgreater{}         \textless{}dbl\textgreater{}}
\CommentTok{\#\textgreater{} 1 38997 Michelle         13}
\CommentTok{\#\textgreater{} 2 23078 Eric              3}
\end{Highlighting}
\end{Shaded}

If we reverse this, we should receive an empty tibble.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{anti\_join}\NormalTok{(l, h, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{))}
\CommentTok{\#\textgreater{} \# A tibble: 0 x 3}
\CommentTok{\#\textgreater{} \# ... with 3 variables: host\_id \textless{}dbl\textgreater{}, price \textless{}dbl\textgreater{}, neighborhood \textless{}chr\textgreater{}}
\end{Highlighting}
\end{Shaded}

Anti joins are often a good way to sanity check your data when looking for completeness or missingness. Be sure to keep this one in your back pocket!

\hypertarget{exercise-2}{%
\section{Exercise}\label{exercise-2}}

For this exercise your goal is to identify the average price and availability of Airbnb rentals by host. Then plot

\begin{itemize}
\tightlist
\item
  Read in the reviews dataset
\item
  Count the total number of reviews by listing, create column \texttt{n\_reviews}
\item
  join to listings
\item
  join that to host, name it \texttt{airbnb\_full}.
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{reviews \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/reviews.csv"}\NormalTok{)}
\CommentTok{\#\textgreater{} Parsed with column specification:}
\CommentTok{\#\textgreater{} cols(}
\CommentTok{\#\textgreater{}   listing\_id = col\_double(),}
\CommentTok{\#\textgreater{}   date = col\_date(format = "")}
\CommentTok{\#\textgreater{} )}

\NormalTok{airbnb\_full \textless{}{-}}\StringTok{ }\KeywordTok{group\_by}\NormalTok{(reviews, listing\_id) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{n\_reviews =} \KeywordTok{n}\NormalTok{()) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{left\_join}\NormalTok{(listings, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"listing\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{left\_join}\NormalTok{(hosts, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{))}

\KeywordTok{glimpse}\NormalTok{(airbnb\_full)}
\CommentTok{\#\textgreater{} Rows: 2,668}
\CommentTok{\#\textgreater{} Columns: 16}
\CommentTok{\#\textgreater{} $ listing\_id       \textless{}dbl\textgreater{} 3781, 5506, 6695, 8789, 10730, 10813, 18711, 22195...}
\CommentTok{\#\textgreater{} $ n\_reviews        \textless{}int\textgreater{} 2, 26, 30, 2, 4, 35, 9, 11, 23, 21, 3, 20, 10, 15,...}
\CommentTok{\#\textgreater{} $ neighborhood     \textless{}chr\textgreater{} "East Boston", "Roxbury", "Roxbury", "Downtown", "...}
\CommentTok{\#\textgreater{} $ room\_type        \textless{}chr\textgreater{} "Entire home/apt", "Entire home/apt", "Entire home...}
\CommentTok{\#\textgreater{} $ price            \textless{}dbl\textgreater{} 125, 145, 169, 99, 150, 179, 154, 115, 148, 275, 9...}
\CommentTok{\#\textgreater{} $ minimum\_nights   \textless{}dbl\textgreater{} 28, 3, 3, 91, 91, 91, 29, 1, 2, 2, 6, 3, 2, 2, 2, ...}
\CommentTok{\#\textgreater{} $ availability\_365 \textless{}dbl\textgreater{} 68, 322, 274, 247, 29, 0, 304, 285, 62, 247, 197, ...}
\CommentTok{\#\textgreater{} $ host\_id          \textless{}dbl\textgreater{} 4804, 8229, 8229, 26988, 26988, 38997, 71783, 8513...}
\CommentTok{\#\textgreater{} $ name             \textless{}chr\textgreater{} "Frank", "Terry", "Terry", "Anne", "Anne", "Michel...}
\CommentTok{\#\textgreater{} $ since\_year       \textless{}dbl\textgreater{} 2008, 2009, 2009, 2009, 2009, 2009, 2010, 2010, 20...}
\CommentTok{\#\textgreater{} $ since\_month      \textless{}chr\textgreater{} "12", "02", "02", "07", "07", "09", "01", "02", "0...}
\CommentTok{\#\textgreater{} $ since\_day        \textless{}chr\textgreater{} "03", "19", "19", "22", "22", "16", "19", "24", "2...}
\CommentTok{\#\textgreater{} $ response\_rate    \textless{}chr\textgreater{} "100\%", "100\%", "100\%", "100\%", "100\%", "92\%", "98...}
\CommentTok{\#\textgreater{} $ acceptance\_rate  \textless{}chr\textgreater{} "50\%", "100\%", "100\%", "84\%", "84\%", "17\%", "98\%",...}
\CommentTok{\#\textgreater{} $ superhost        \textless{}dbl\textgreater{} 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1,...}
\CommentTok{\#\textgreater{} $ n\_listings       \textless{}dbl\textgreater{} 5, 2, 2, 9, 9, 13, 40, 7, 4, 1, 1, 1, 5, 5, 1, 1, ...}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{host\_summary \textless{}{-}}\StringTok{ }\KeywordTok{left\_join}\NormalTok{(listings, hosts, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{group\_by}\NormalTok{(superhost) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{summarise}\NormalTok{(}\DataTypeTok{avg\_avail =} \KeywordTok{mean}\NormalTok{(availability\_}\DecValTok{365}\NormalTok{),}
            \DataTypeTok{avg\_price =} \KeywordTok{mean}\NormalTok{(price),}
            \DataTypeTok{avg\_min\_nights =} \KeywordTok{mean}\NormalTok{(minimum\_nights))}
\end{Highlighting}
\end{Shaded}

\hypertarget{statistics}{%
\chapter{Statistics}\label{statistics}}

In the course of your analyses you will both want to and need to conduct statistical tests. It is important that you are equipped to perform these tests in R as well. Teaching you statistical concepts is outside of the scope of this book. For an introduction to statistical concepts using R, I recommend reading David Dalpiaz's free and open \href{https://daviddalpiaz.github.io/r4sl/}{\emph{R for Statistical Learning}}\footnote{\emph{R for Statistical Learning}. \url{https://daviddalpiaz.github.io/r4sl/}.}. In this section we will review \emph{how} to implement statistical tests and extract useful information from them as well. We will cover t-tests, ANOVA, and linear regression.

To explore these statistics we will use data from Inside Airbnb. Of interest is the relationship between price and superhosts, price and room type, and finally how both superhosts and room type contribute to price.

\hypertarget{the-data-1}{%
\section{The data}\label{the-data-1}}

We will use data from both the \texttt{hosts} and \texttt{listings} datasets. The former contains superhost data while the later has both the price and room type information. First we will read in both of these datasets.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{listings \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/listings.csv"}\NormalTok{)}
\NormalTok{hosts \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/hosts.csv"}\NormalTok{)}

\KeywordTok{glimpse}\NormalTok{(listings)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 3,799
## Columns: 7
## $ id               <dbl> 3781, 5506, 6695, 8789, 10730, 10813, 10986, 16384...
## $ neighborhood     <chr> "East Boston", "Roxbury", "Roxbury", "Downtown", "...
## $ room_type        <chr> "Entire home/apt", "Entire home/apt", "Entire home...
## $ price            <dbl> 125, 145, 169, 99, 150, 179, 125, 50, 154, 115, 14...
## $ minimum_nights   <dbl> 28, 3, 3, 91, 91, 91, 91, 91, 29, 1, 2, 2, 2, 6, 3...
## $ availability_365 <dbl> 68, 322, 274, 247, 29, 0, 364, 365, 304, 285, 62, ...
## $ host_id          <dbl> 4804, 8229, 8229, 26988, 26988, 38997, 38997, 2307...
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{glimpse}\NormalTok{(hosts)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 1,335
## Columns: 9
## $ id              <dbl> 4804, 8229, 26988, 38997, 23078, 71783, 85130, 8577...
## $ name            <chr> "Frank", "Terry", "Anne", "Michelle", "Eric", "Lanc...
## $ since_year      <dbl> 2008, 2009, 2009, 2009, 2009, 2010, 2010, 2010, 201...
## $ since_month     <chr> "12", "02", "07", "09", "06", "01", "02", "02", "03...
## $ since_day       <chr> "03", "19", "22", "16", "24", "19", "24", "26", "23...
## $ response_rate   <chr> "100%", "100%", "100%", "92%", "50%", "98%", "66%",...
## $ acceptance_rate <chr> "50%", "100%", "84%", "17%", "N/A", "98%", "97%", "...
## $ superhost       <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...
## $ n_listings      <dbl> 5, 2, 9, 13, 3, 40, 7, 4, 1, 2, 1, 1, 5, 1, 1, 1, 1...
\end{verbatim}

In order to join these two tibbles together we need to figure out what the common identifiers are. In \texttt{listings} we can infer that the \textbf{primary key} is \texttt{id} while the \textbf{foreign key} is \texttt{host\_id}. While the \texttt{hosts} tibble only has one \texttt{id} column. Clearly the join needs to be between \texttt{host\_id} and \texttt{id} from \texttt{listings} and \texttt{hosts} respectively. We will perform a left join then select the columns \texttt{price}, \texttt{room\_type} and \texttt{superhost} and assign that to the object \texttt{airbnb}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{airbnb \textless{}{-}}\StringTok{ }\KeywordTok{left\_join}\NormalTok{(listings, hosts, }\DataTypeTok{by =} \KeywordTok{c}\NormalTok{(}\StringTok{"host\_id"}\NormalTok{ =}\StringTok{ "id"}\NormalTok{)) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{select}\NormalTok{(price, room\_type, superhost) }

\KeywordTok{glimpse}\NormalTok{(airbnb)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Rows: 3,799
## Columns: 3
## $ price     <dbl> 125, 145, 169, 99, 150, 179, 125, 50, 154, 115, 148, 275,...
## $ room_type <chr> "Entire home/apt", "Entire home/apt", "Entire home/apt", ...
## $ superhost <dbl> 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, ...
\end{verbatim}

Before we can engage in any statistical testing we should do our due diligence and first visualize the relationship before we test it. Because we are comparing a group to a continuous variable a boxplot will suffice.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(airbnb, }\KeywordTok{aes}\NormalTok{(price, superhost, }\DataTypeTok{group =}\NormalTok{ superhost)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-statistics_files/figure-latex/unnamed-chunk-3-1.pdf}

\begin{quote}
Note that we are setting the group to \texttt{superhost} this is because it is dummy coded as a numeric value. ggplot is attempting to consider it numeric rather than categorical.
\end{quote}

Clearly there are some noticeable outliers above the \$2,500 mark. Let's filter out these values before we get on to our testing and while we're at it, let's convert \texttt{superhost} to a factor with \texttt{factor()}. Save the the filtered results to \texttt{bnb\_filt}. Recreate the above visualization with the new object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{bnb\_filt \textless{}{-}}\StringTok{ }\KeywordTok{filter}\NormalTok{(airbnb, price }\OperatorTok{\textless{}}\StringTok{ }\DecValTok{2500}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{mutate}\NormalTok{(}\DataTypeTok{superhost =} \KeywordTok{factor}\NormalTok{(superhost))}

\KeywordTok{ggplot}\NormalTok{(bnb\_filt, }\KeywordTok{aes}\NormalTok{(price, superhost)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_boxplot}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-statistics_files/figure-latex/unnamed-chunk-4-1.pdf}

The first thing you may notices is that we no longer had to specify the \texttt{group} aesthetic because we converted \texttt{superhost} to a non-numeric format. From the above visualization it looks like that being a superhost does not necessarily increase the price of a listing. We can now test these means by using \texttt{t.test()}. There are a number of ways in which we can use this function but the most generalizable way is to use what is called the \textbf{formula} interface.

\hypertarget{the-formula-interface}{%
\section{The formula interface}\label{the-formula-interface}}

The formula interface is a way of defining \emph{statistical} formulae. Or maybe a bit more clearly it let's us tell R which columns to use when fitting a model\footnote{\emph{R for Dummies}. \url{https://www.dummies.com/programming/r/how-to-use-the-formula-interface-in-r/}.}. The general format it takes is \texttt{y\ \textasciitilde{}\ x} which reads \emph{y as a function of x}. In the case of a t-test the \texttt{y} is the variable that we will be testing the means of and the \texttt{x} is what group to compare. If our data are already in a tidy format---like our Airbnb data---this will be rather easy to adhere to.

\hypertarget{t-tests}{%
\section{T-tests}\label{t-tests}}

To perform a t-test we will use the \texttt{t.test()} function with the arguments \texttt{formula} and \texttt{data}. An example call looks like \texttt{t.test(y\ \textasciitilde{}\ x,\ data\ =\ df)}. In our case our \texttt{y} is \texttt{price} because it is our variable of interest or our dependent variable. Since we are curious how \texttt{price} changes by \texttt{superhost} status, we will put \texttt{superhost} in the \texttt{x} spot.

Conduct a t-test and store the results in \texttt{price\_t}. Print it out afterwards.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(price\_t \textless{}{-}}\StringTok{ }\KeywordTok{t.test}\NormalTok{(price }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{superhost, }\DataTypeTok{data =}\NormalTok{ bnb\_filt))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
##  Welch Two Sample t-test
## 
## data:  price by superhost
## t = 2.3261, df = 2029.4, p-value = 0.02011
## alternative hypothesis: true difference in means is not equal to 0
## 95 percent confidence interval:
##   1.949967 22.904491
## sample estimates:
## mean in group 0 mean in group 1 
##        175.1989        162.7717
\end{verbatim}

The above is a somewhat cluttered buch of numbers and words. But in there we can see our t-value (\texttt{t\ =\ 2.3261}), degrees of freedom (\texttt{df\ =\ 2029.4}), and our p-value (\texttt{p-value\ =\ 0.02011}). From this test we can tell that with an alpha level of 0.05 we can reject the null hypothesis.

\hypertarget{tidying-up-after-our-models}{%
\subsection{Tidying up after our models}\label{tidying-up-after-our-models}}

While this is useful, we're going to, at some point, want to extract these statistics in some usable format. Enter \href{https://broom.tidyverse.org/}{\texttt{broom}}. From the documentation:

\begin{quote}
"\texttt{broom} summarizes key information about models in tidy tibble()s. broom provides three verbs to make it convenient to interact with model objects.

\begin{itemize}
\tightlist
\item
  tidy() summarizes information about model components
\item
  glance() reports information about the entire model
\item
  augment() adds informations about observations to a dataset\footnote{\{broom\} \url{https://broom.tidyverse.org/}.}
\end{itemize}
\end{quote}

Make sure that broom is installed with \texttt{install.packages("broom")}. Once that is installed use the function \texttt{tidy()} on the \texttt{price\_t} object,

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(price\_t)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 10
##   estimate estimate1 estimate2 statistic p.value parameter conf.low conf.high
##      <dbl>     <dbl>     <dbl>     <dbl>   <dbl>     <dbl>    <dbl>     <dbl>
## 1     12.4      175.      163.      2.33  0.0201     2029.     1.95      22.9
## # ... with 2 more variables: method <chr>, alternative <chr>
\end{verbatim}

The result is a tibble that can be easily manipulated and worked with. But naturally we will want to explore beyond just two groups. And in that case we must perform an analysis of variance (ANOVA).

\hypertarget{anova}{%
\section{ANOVA}\label{anova}}

The ANOVA test is used in the case when t-tests cannot. That is, they are used when we want to know if there is a difference in means between groups when there are two or more groups. To perform an ANOVA we use the \texttt{aov()} function---an initialism for \textbf{a}analysis \textbf{o}f \textbf{v}ariance---with the same arguments that we used in \texttt{t.test()}. The only difference here is that the \texttt{x} is a column that has more than two groups---\texttt{room\_type}. We can fit the ANOVA model with \texttt{price} as our \texttt{y} and \texttt{room\_type} as our \texttt{x}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(price\_aov \textless{}{-}}\StringTok{ }\KeywordTok{aov}\NormalTok{(price }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{room\_type, }\DataTypeTok{data =}\NormalTok{ bnb\_filt))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Call:
##    aov(formula = price ~ room_type, data = bnb_filt)
## 
## Terms:
##                 room_type Residuals
## Sum of Squares   15152015  70853604
## Deg. of Freedom         3      3789
## 
## Residual standard error: 136.7473
## Estimated effects may be unbalanced
\end{verbatim}

When we print out the ANOVA model object we actually don't see the results we were anticipating. To get thos we have to pass the model object to the function \texttt{summary()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{summary}\NormalTok{(price\_aov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##               Df   Sum Sq Mean Sq F value Pr(>F)    
## room_type      3 15152015 5050672   270.1 <2e-16 ***
## Residuals   3789 70853604   18700                   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

Now here we find the results of our test: p \textless{} 0.001. You may notice already that there is some inconsistency in which that ways that models are interacted with. That is why we use broom, to have one common way of working with model objects. If we'd like to access the model results in a consistent way we can again use \texttt{broom::tidy()}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(price\_aov)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 2 x 6
##   term         df     sumsq   meansq statistic    p.value
##   <chr>     <dbl>     <dbl>    <dbl>     <dbl>      <dbl>
## 1 room_type     3 15152015. 5050672.      270.  7.32e-159
## 2 Residuals  3789 70853604.   18700.       NA  NA
\end{verbatim}

Remember though that ANOVA tests if there is \emph{any} variation between any two groups. The results of the test do not tell us \emph{which} groups are different. And this is when we turn to Tukey's Honestly Significant Difference (HSD). Tukey's HSD creates a set of confidence intervals to compare each unique combination of our variables. To perform the test in R we pass the ANOVA model object to the function \texttt{TukeyHSD()}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(price\_hsd \textless{}{-}}\StringTok{ }\KeywordTok{TukeyHSD}\NormalTok{(price\_aov))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   Tukey multiple comparisons of means
##     95% family-wise confidence level
## 
## Fit: aov(formula = price ~ room_type, data = bnb_filt)
## 
## $room_type
##                                     diff        lwr         upr     p adj
## Hotel room-Entire home/apt    -74.062500 -130.09198  -18.033019 0.0038353
## Private room-Entire home/apt -132.658355 -144.68495 -120.631764 0.0000000
## Shared room-Entire home/apt  -123.687500 -211.84397  -35.531026 0.0017890
## Private room-Hotel room       -58.595855 -115.00228   -2.189435 0.0381673
## Shared room-Hotel room        -49.625000 -153.58946   54.339462 0.6097851
## Shared room-Private room        8.970855  -79.42567   97.367379 0.9937854
\end{verbatim}

The results of the above test show that there are rather significant difference between hotel rooms and entire homes or apartments, private rooms and entire homes, shared rooms and entire homes, as well as private room and hotel room. With the use of broom and ggplot we can begin to visualize these restuls.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(price\_hsd\_tidy \textless{}{-}}\StringTok{ }\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(price\_hsd))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 6
##   term      comparison                   estimate conf.low conf.high adj.p.value
##   <chr>     <chr>                           <dbl>    <dbl>     <dbl>       <dbl>
## 1 room_type Hotel room-Entire home/apt     -74.1    -130.     -18.0      3.84e-3
## 2 room_type Private room-Entire home/apt  -133.     -145.    -121.       1.38e-8
## 3 room_type Shared room-Entire home/apt   -124.     -212.     -35.5      1.79e-3
## 4 room_type Private room-Hotel room        -58.6    -115.      -2.19     3.82e-2
## 5 room_type Shared room-Hotel room         -49.6    -154.      54.3      6.10e-1
## 6 room_type Shared room-Private room         8.97    -79.4     97.4      9.94e-1
\end{verbatim}

With the tidied HSD object we can create a graph of point estimates and error bars.

\begin{quote}
I personally like to call these Tie Fighter plots because they resemble the space ships from Star Wars.
\end{quote}

We can being by plotting the point estimates of each comparison.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{(p \textless{}{-}}\StringTok{ }\KeywordTok{ggplot}\NormalTok{(price\_hsd\_tidy, }\KeywordTok{aes}\NormalTok{(estimate, comparison)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-statistics_files/figure-latex/unnamed-chunk-12-1.pdf}

Next we can add a horizontal error bar (\texttt{geom\_errorbarh()}) layer to the plot. This layer requires some additional aesthetics that we will set in the layer itself. These are \texttt{xmin} and \texttt{xmax}. Respectively they are used to mark the minimum and maximum extents of the error bars. In the case of the HSD object, the bounds of th confidence intervals have been already calculated for us and can be found in the columns \texttt{conf.low} and \texttt{conf.high}. We can pass these to the \texttt{xmin} and \texttt{xmax} aesthetic arguments.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{p }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_errorbarh}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xmin =}\NormalTok{ conf.low, }\DataTypeTok{xmax =}\NormalTok{ conf.high))}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-statistics_files/figure-latex/unnamed-chunk-13-1.pdf}

\hypertarget{linear-regression}{%
\section{Linear regression}\label{linear-regression}}

When we want to move on to inference with linear models, we turn to the \texttt{lm()} function. This, like the \texttt{t.test()} and \texttt{aov()} functions requires both a formula and data. The difference is that the formulas that we will use are a bit more complex because we will often be using many variables. To predict y as some function of multiple inputs we have to declare all of those inputs in our formula which takes form of \texttt{y\ \textasciitilde{}\ x1\ +\ x2\ +\ ...}. So if we were to create a linear model that predicts price as a function of room type and whether or the host is a superhost, our formula will look like \texttt{price\ \textasciitilde{}\ room\_type\ +\ superhost}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{price\_lm \textless{}{-}}\StringTok{ }\KeywordTok{lm}\NormalTok{(price }\OperatorTok{\textasciitilde{}}\StringTok{ }\NormalTok{room\_type }\OperatorTok{+}\StringTok{ }\NormalTok{superhost, }\DataTypeTok{data =}\NormalTok{ bnb\_filt)}

\KeywordTok{summary}\NormalTok{(price\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = price ~ room_type + superhost, data = bnb_filt)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -219.01  -64.01  -26.01   20.99 1780.99 
## 
## Coefficients:
##                        Estimate Std. Error t value Pr(>|t|)    
## (Intercept)            219.0073     3.0450  71.925  < 2e-16 ***
## room_typeHotel room    -74.3532    21.8938  -3.396 0.000691 ***
## room_typePrivate room -132.7222     4.7004 -28.237  < 2e-16 ***
## room_typeShared room  -123.5526    34.3168  -3.600 0.000322 ***
## superhost1               0.7244     4.9720   0.146 0.884172    
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 136.8 on 3788 degrees of freedom
## Multiple R-squared:  0.1762, Adjusted R-squared:  0.1753 
## F-statistic: 202.5 on 4 and 3788 DF,  p-value: < 2.2e-16
\end{verbatim}

The output of this is very similar to that of the ANOVA model. Perhaps we can visualize it the same way?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(price\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 5
##   term                  estimate std.error statistic   p.value
##   <chr>                    <dbl>     <dbl>     <dbl>     <dbl>
## 1 (Intercept)            219.         3.04    71.9   0.       
## 2 room_typeHotel room    -74.4       21.9     -3.40  6.91e-  4
## 3 room_typePrivate room -133.         4.70   -28.2   2.34e-159
## 4 room_typeShared room  -124.        34.3     -3.60  3.22e-  4
## 5 superhost1               0.724      4.97     0.146 8.84e-  1
\end{verbatim}

Unfortunately when we tidy this up we don't have the same columns. However we can ask for them explicitly by setting the argument \texttt{conf.int\ =\ TRUE}.

\begin{quote}
You can find all possible arguments for \texttt{tidy()} in the exported object \texttt{broom::argument\_glossary}.
\end{quote}

Using a similar structure as above, we can create a coefficient plot.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{tidy}\NormalTok{(price\_lm, }\DataTypeTok{conf.int =} \OtherTok{TRUE}\NormalTok{) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{ggplot}\NormalTok{(}\KeywordTok{aes}\NormalTok{(estimate, term)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_point}\NormalTok{() }\OperatorTok{+}\StringTok{ }
\StringTok{  }\KeywordTok{geom\_errorbarh}\NormalTok{(}\KeywordTok{aes}\NormalTok{(}\DataTypeTok{xmin =}\NormalTok{ conf.low, }\DataTypeTok{xmax =}\NormalTok{ conf.high))}
\end{Highlighting}
\end{Shaded}

\includegraphics{09-statistics_files/figure-latex/unnamed-chunk-16-1.pdf}

Unlike the t-test, a linear model provides much more information that will become useful such as goodness of fit measures, residuals, and predicted values. To extract these we can use the functions \texttt{glance()} and \texttt{augment()} from broom.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{glance}\NormalTok{(price\_lm)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 11
##   r.squared adj.r.squared sigma statistic   p.value    df  logLik    AIC    BIC
##       <dbl>         <dbl> <dbl>     <dbl>     <dbl> <int>   <dbl>  <dbl>  <dbl>
## 1     0.176         0.175  137.      203. 1.29e-157     5 -24035. 48081. 48118.
## # ... with 2 more variables: deviance <dbl>, df.residual <int>
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{broom}\OperatorTok{::}\KeywordTok{augment}\NormalTok{(price\_lm) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{slice}\NormalTok{(}\DecValTok{1}\OperatorTok{:}\DecValTok{5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 5 x 10
##   price room_type superhost .fitted .se.fit .resid    .hat .sigma .cooksd
##   <dbl> <chr>     <fct>       <dbl>   <dbl>  <dbl>   <dbl>  <dbl>   <dbl>
## 1   125 Entire h~ 1            220.    4.66  -94.7 0.00116   137. 1.12e-4
## 2   145 Entire h~ 1            220.    4.66  -74.7 0.00116   137. 6.94e-5
## 3   169 Entire h~ 1            220.    4.66  -50.7 0.00116   137. 3.20e-5
## 4    99 Entire h~ 1            220.    4.66 -121.  0.00116   137. 1.81e-4
## 5   150 Entire h~ 1            220.    4.66  -69.7 0.00116   137. 6.04e-5
## # ... with 1 more variable: .std.resid <dbl>
\end{verbatim}

\hypertarget{spatial-analysis}{%
\chapter{Spatial Analysis}\label{spatial-analysis}}

In the very beginning of the book we discussed some of the benefits of administrative data. One of the benefits mentioned is that administrative data are inherently \textbf{spatial}. Most data tend to be spatial in nature. This is because most events happen at a physical place. In the context of administrative data, we know that all data recorded at that level must fall within the municipal boundaries. Often we know the location within the municipality to a much finer scale---i.e.~the block group, the voting ward, or even the exact point location.

Identifying whether or not your data is spatial is easiest when there are geographic components present such as latitude and longitude. However, your data can also be spatial even if it isn't explicitly stated. Some things you can keep an eye out for are things like neighborhood, towns, county, etc. as these are location specific. It is likely that you will be performing analyses rooted in space and accounting for things such as counties but perhaps without using any geospatial techniques.

In this section we will go over the very basics of geospatial analysis.

\hypertarget{types-of-spatial-data}{%
\section{Types of spatial data}\label{types-of-spatial-data}}

Within the field of Geographic Information Systems (\textbf{GIS}) there are two general umbrellas in which data fall under. These are vector and raster data.

\textbf{Vector} data is what you will find yourself working with most frequently. Most simply put they are \emph{``points, lines, and polygons.''} The basis of vector data is the coordinate point. Just like the scatter plots we have built, each coordinate point is a combination of an x and y value (longitude and latitude respectively). This combination of x and y will tell us where something is. By combining two or more points we can trace along a path---think of the connect the dots diagrams you would do at restaurants as a kid---and create line segments. If, however, at any point these lines close, you now have a polygon.

The other umbrella of data is known as \textbf{raster} data. Raster data are to deal with more complex data that cannot easily be captured by a single point. Rasters are used to ``represent spatially continuous phenomenon'' \footnote{R Spatial. \url{https://rspatial.org/raster/spatial/Spatialdata.pdf}.}. Raster analysis is done to evaluate things like changing vegetation, elevation and slope modeling, analysing reflective surfaces, among much more. Raster analysis typically relies on satellite imagery or LiDAR laser point cloud data. Raster analysis is an extremely complex topic and requires devoted attention. As such, we will not cover it in this book. But, know that it exists and is out there!

\includegraphics{static/2019-points-lines-polygons.png}

\hypertarget{working-with-spatial-data}{%
\section{Working with spatial data}\label{working-with-spatial-data}}

Working with spatial data is made rather straightforward by the \href{https://github.com/r-spatial/sf}{\texttt{sf}} package.

sf is shorthand for \emph{simple features}. sf let's us represent physical objects or phenomena in that occur the real world through data. It is built upon an international standard that ``describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them.'' (sf vinette, 1).

All simple features are representation of vector data. That is that they are composed of points. These points are usually represented two-dimensionally with longitude and latitude (x and y). We can associate a third dimension of altitude if so desired to extend to three dimensions: longitude, latitude, and altitude (x, y, \& z). In our cases, however, this will not be used.

In this section we will working with the \texttt{locations} Airbnb dataset. \texttt{locations} contains the longitude and latitude of Airbnb listings in Boston. These are an example of point data (which contain two-dimension).

\hypertarget{creating-simple-features-from-a-tibble}{%
\section{Creating simple features from a tibble}\label{creating-simple-features-from-a-tibble}}

Begin by installing the \texttt{sf} (simple-features) package and loading the \texttt{locations} Airbnb dataset.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# install.packages("sf")}
\KeywordTok{library}\NormalTok{(sf)}
\KeywordTok{library}\NormalTok{(tidyverse)}

\NormalTok{locations \textless{}{-}}\StringTok{ }\KeywordTok{read\_csv}\NormalTok{(}\StringTok{"data/airbnb/locations.csv"}\NormalTok{)}

\KeywordTok{head}\NormalTok{(locations)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 6 x 3
##      id longitude latitude
##   <dbl>     <dbl>    <dbl>
## 1  3781     -71.0     42.4
## 2  5506     -71.1     42.3
## 3  6695     -71.1     42.3
## 4  8789     -71.1     42.4
## 5 10730     -71.1     42.4
## 6 10813     -71.1     42.3
\end{verbatim}

So far everything is the same. We have read in our dataset and created a tibble. The next step is to make this tibble a simple feature. Fortunately, sf keeps this process rather simple for us by representing spatial data in native R data formats---namely, the data frame. To make simple features from an existing tibble, we need to cast the object as an \texttt{sf} object. And we do this with \texttt{st\_as\_sf()}.

Generally when we cast objects we use functions like \texttt{as.integer()} or \texttt{as\_tibble()}. Here, there is a prefixed \texttt{st\_}. This stands for \emph{spatial transformation}. All transformations are prefixed as such---this is in an effort to keep continuity between GIS tools. Most functions cast objects to other classes with no function arguments. \texttt{st\_as\_sf()} unfortunately cannot read your mind and is not aware of what the geometry is in the tibble. As such, we need to use the \texttt{coords} argument in \texttt{st\_as\_sf()}. \texttt{coords} gives us the ability to tell sf what the columns are that contain our coordinate points. For point data we need to provide a character vector of length two with the x and y dimensions aka longitude and latitude.

\begin{quote}
Note that we are likely used to saying \emph{lat, long}, but that actually maps to \emph{y, x}. This is something that trips everyone up! Just make sure you put longitude in the x spot and latitude in the y spot.
\end{quote}

To convert the \texttt{locations} data frame to a simple feature we will use \texttt{st\_as\_sf()} and set the \texttt{coords} argument to \texttt{c("longitude",\ "latitude")}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{st\_as\_sf}\NormalTok{(locations,}
         \DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"longitude"}\NormalTok{, }\StringTok{"latitude"}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 3799 features and 1 field
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549
## CRS:            NA
## # A tibble: 3,799 x 2
##       id             geometry
##    <dbl>              <POINT>
##  1  3781 (-71.02991 42.36413)
##  2  5506 (-71.09559 42.32981)
##  3  6695 (-71.09351 42.32994)
##  4  8789 (-71.06265 42.35919)
##  5 10730  (-71.06185 42.3584)
##  6 10813 (-71.08904 42.34961)
##  7 10986 (-71.05075 42.36352)
##  8 16384  (-71.07132 42.3581)
##  9 18711 (-71.06096 42.32212)
## 10 22195  (-71.0793 42.34558)
## # ... with 3,789 more rows
\end{verbatim}

Now that we have successfully created a simple feature we can see that we no longer have the columns \texttt{longitude} and \texttt{latitude} but a \texttt{geometry} column instead. Notice that when printed, the object tells us what type of geometry we are working with, it's dimensions, and the bounding box for these points.

\begin{quote}
A bounding box is the furthest extent that our data reaches in both latiude and longitude.
\end{quote}

The printed object informs us that there are actually two missing pieces of information the epsg and proj4string. This is because we failed to specify a \textbf{coordinate reference system} (CRS). While this book is not intended as an introduction to GIS, this is still worth briefly expanding upon. We use a CRS because we are trying to place points in two-dimensions when the Earth is round! Try peeling and orange and laying the peel flat. It's impossible. There is now way to visualize a circle as a rectangle without introducing \emph{some} error. This is what a CRS accounts for. There are many CRS for each type of map projection and each type of unit. Most, if not all, of the frustration you may encounter when working with spatial data will be due to mismatching CRS.

Fortunately we will \emph{most likely} be working with data that is collected using the \textbf{WGS84} reference system. This is a CRS that is used to define a global reference system that is used consistently throughout government agencies, and typically in online data recording. The Airbnb data uses this references system.

Most other online data sources use this reference system as well. For example Google and Twitter provide their data using this CRS. The times when you are most likely to encounter a CRS isn't WGS84 is when working with data from local agencies that need highly accurate and tailored spatial data. This would be agencies like water departments, and forestry groups, etc.

To ensure that our data are properly represented in space, we need to provide the CRS in the creation of our simple features. We do this by specifying the \texttt{crs} argument. \texttt{crs} will accept a number that indicates what projection you are using. There are too many CRS identifiers to commit to memory. This information is usually recorded in the original data source. Be sure to confirm the spatial dimensions! For WGS84, the CRS identifier is \texttt{4326}. This is probably worth committing to memory.

\url{https://confluence.qps.nl/qinsy/latest/en/world-geodetic-system-1984-wgs84-182618391.html\#id-.WorldGeodeticSystem1984(WGS84)v9.1-WGS84definitions}

We will now create an object called \texttt{loc\_sf} using \texttt{st\_as\_sf()} and providing both the \texttt{coords} and the \texttt{crs}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{loc\_sf \textless{}{-}}\StringTok{ }\KeywordTok{st\_as\_sf}\NormalTok{(locations,}
         \DataTypeTok{coords =} \KeywordTok{c}\NormalTok{(}\StringTok{"longitude"}\NormalTok{, }\StringTok{"latitude"}\NormalTok{),}
         \DataTypeTok{crs =} \DecValTok{4326}\NormalTok{)}

\NormalTok{loc\_sf}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 3799 features and 1 field
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549
## CRS:            EPSG:4326
## # A tibble: 3,799 x 2
##       id             geometry
##    <dbl>          <POINT [°]>
##  1  3781 (-71.02991 42.36413)
##  2  5506 (-71.09559 42.32981)
##  3  6695 (-71.09351 42.32994)
##  4  8789 (-71.06265 42.35919)
##  5 10730  (-71.06185 42.3584)
##  6 10813 (-71.08904 42.34961)
##  7 10986 (-71.05075 42.36352)
##  8 16384  (-71.07132 42.3581)
##  9 18711 (-71.06096 42.32212)
## 10 22195  (-71.0793 42.34558)
## # ... with 3,789 more rows
\end{verbatim}

Since an sf object is also a data frame we are able to perform all of the operations that we may with a normal tibble such as selecting columns, joining, mutating etc.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{count}\NormalTok{(loc\_sf)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 1 feature and 1 field
## geometry type:  MULTIPOINT
## dimension:      XY
## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549
## CRS:            EPSG:4326
## # A tibble: 1 x 2
##       n                                                                 geometry
##   <int>                                                         <MULTIPOINT [°]>
## 1  3799 ((-71.1728 42.34835), (-71.17174 42.34854), (-71.17173 42.34923), (-71.~
\end{verbatim}

Notice that it keeps the geometry even when counting. When our data is spatial, we have to incorporate the geometry into our computations. This often times leads to slower processing times. So if you do not immediately need the geometry, my recommendation is that you join it back on as late as possible. You can cast an sf object to a tibble with \texttt{as\_tibble()}

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{as\_tibble}\NormalTok{(loc\_sf) }\OperatorTok{\%\textgreater{}\%}\StringTok{ }
\StringTok{  }\KeywordTok{count}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## # A tibble: 1 x 1
##       n
##   <int>
## 1  3799
\end{verbatim}

Notice that we now lose the geometry column. This is because we have stopped keeping track of the geometry.

\hypertarget{plotting-sf-objects-with-ggplot}{%
\section{Plotting sf objects with ggplot}\label{plotting-sf-objects-with-ggplot}}

Plotting sf objects is made rather straightforward with ggplot2. Since sf objects contain a ton of spatial information this is inferred from ggplot. As such, we are not required to map the aesthetics for x and y. We simple provide just the data argument to ggpplot and then add a \texttt{geom\_sf()} layer. Inside of \texttt{geom\_sf()} we can provide any and all arguments that we may like such as color, size, shape, etc. as this will be passed to the underlying \texttt{geom\_*}---in the case of points, it will be \texttt{geom\_point()}.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(loc\_sf) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{(}\DataTypeTok{shape =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-spatial-analysis_files/figure-latex/unnamed-chunk-6-1.pdf}

This is great as we can already somewhat see the shape of Boston and Suffolk County. Since we have these Airbnb points located in space, we know we are able to associate them with their respective Census tracts.To do so we need another spatial data set which contains the shapes of each tract. In the next section we will read a dataset containing the shapes of each tract in Suffolk county. Following we will perform a spatial join to associate the points with the tracts.

\hypertarget{connecting-points-to-polygons}{%
\section{Connecting points to polygons}\label{connecting-points-to-polygons}}

Now that we have the point locations of each Airbnb listing we need to identify which tracts they belong to. In the \texttt{data} folder there is a file called \texttt{suffolk\_acs.geojson}. This is a common spatial data format which is based on \texttt{json}. The difference is that \texttt{geojson} contains a lot of fields specific to spatial data.

Reading in data of this format is just as easy as in reading in a csv file. Using \texttt{sf::read\_sf()} we can pass the path of the geojson file and be returned an sf object.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{acs\_tracts \textless{}{-}}\StringTok{ }\KeywordTok{read\_sf}\NormalTok{(}\StringTok{"data/suffolk\_acs.geojson"}\NormalTok{)}

\NormalTok{acs\_tracts}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 203 features and 1 field
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012
## CRS:            4326
## # A tibble: 203 x 2
##    fips                                                                 geometry
##    <chr>                                                      <MULTIPOLYGON [°]>
##  1 250250921~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29301, -7~
##  2 250251006~ (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28961, -7~
##  3 250250101~ (((-71.11093 42.35047, -71.11093 42.3505, -71.11092 42.35054, -71~
##  4 250250704~ (((-71.06944 42.346, -71.0691 42.34661, -71.06884 42.3471, -71.06~
##  5 250251401~ (((-71.13397 42.25431, -71.13353 42.25476, -71.13274 42.25561, -7~
##  6 250259812~ (((-71.04707 42.3397, -71.04628 42.34037, -71.0449 42.34153, -71.~
##  7 250250511~ (((-71.01324 42.38301, -71.01231 42.38371, -71.01162 42.3842, -71~
##  8 250259816~ (((-71.00113 42.3871, -71.001 42.38722, -71.00074 42.3875, -71.00~
##  9 250250909~ (((-71.05079 42.32083, -71.0506 42.32076, -71.05047 42.32079, -71~
## 10 250251103~ (((-71.11952 42.28648, -71.11949 42.2878, -71.11949 42.28792, -71~
## # ... with 193 more rows
\end{verbatim}

The first things you'll notice here is that it looks similar to our \texttt{loc\_sf} object and, more importantly, that the CRS was picked up for us! If we briefly look under the hood of our file, we can see that in the third line the CRS is stated. You don't need to understand what is happening here. Just know that sometimes spatial data sets already have this information for you.

\begin{verbatim}
## {
## "type": "FeatureCollection",
## "name": "suffolk_acs",
## "crs": { "type": "name", "properties": { "name": "urn:ogc:def:crs:OGC:1.3:CRS84" } },
\end{verbatim}

Let's see what this file looks like!

\begin{Shaded}
\begin{Highlighting}[]
  \KeywordTok{ggplot}\NormalTok{(acs\_tracts) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-spatial-analysis_files/figure-latex/unnamed-chunk-9-1.pdf}

Wonderful! There are two stylist adjustments I'd make here so that visualizing is a little easier. The first is to change the line width to something thinner, and adjust the transparency of tracts so that they are a little lighter. This makes the map a bit easier to read all in all.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(acs\_tracts) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{(}\DataTypeTok{lwd =} \FloatTok{0.25}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-spatial-analysis_files/figure-latex/unnamed-chunk-10-1.pdf}

Now, here is where understanding the grammar of graphics comes in handy. We now have two different data sets that would be good to visualize together. Recall that when we specify the data in the top level \texttt{ggplot()} call that sets the default for every single layer. If we do that with multiple objects that may cause some conflicts. We do know, however, that we can set the data per layer. So, taking these two points together, we can plot both \texttt{loc\_sf} \emph{and} \texttt{acs\_tracts} on the same graph if we set the data argument in each respective \texttt{geom\_sf()} layer.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{() }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ acs\_tracts, }\DataTypeTok{lwd =} \FloatTok{0.25}\NormalTok{, }\DataTypeTok{alpha =} \FloatTok{0.5}\NormalTok{) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{(}\DataTypeTok{data =}\NormalTok{ loc\_sf, }\DataTypeTok{shape =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-spatial-analysis_files/figure-latex/unnamed-chunk-11-1.pdf}
With the above plot we can get a sense of the density of Airbnb listings in Boston. There seems to be greater density near Back Bay and Beacon Hill. It would be great to be able to know how many listings there are for each tract and the average listing price. To do this, we need to perform two joins. The first one is spatial---joining point to polygon based on which tract each point intersects. The second is to join the listings information on to the spatially joined data set. In doing this we will have utilized data from three different sources!

\hypertarget{spatial-joins}{%
\subsection{Spatial Joins}\label{spatial-joins}}

Like a regular join, the intent behind a spatial join is to add the attributes of one data source to another. The utility of a spatial join comes when there is no shared attribute \emph{other than} space. More often than not when we want to perform a spatial join we are looking for what is called the \textbf{intersection}. That is essentially where two spatial features touch in some manner. The other type of spatial join that we will find useful is the nearest neighbor where we take the attributes to the next closest object.

To perform a spatial join we use the function \texttt{sf::st\_join()} which has three main arguments: \texttt{x}, \texttt{y}, and \texttt{join}. The default join type is \texttt{st\_intersects} which will join attributes (columns) from \texttt{y} where \texttt{x} \textbf{intersects} (meaning touches or is within). The ordering of our \texttt{x} and \texttt{y} is very important as this will be the difference between a left or a right join. This ordering also determines what type of geometry we will be returned. Whatever type of geometry is in the \texttt{x} position is what will be returned.

Let's try using \texttt{st\_join()} to join the tract level information to our point data.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{points\_join \textless{}{-}}\StringTok{ }\KeywordTok{st\_join}\NormalTok{(loc\_sf, acs\_tracts) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## although coordinates are longitude/latitude, st_intersects assumes that they are planar
## although coordinates are longitude/latitude, st_intersects assumes that they are planar
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{points\_join}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 3799 features and 2 fields
## geometry type:  POINT
## dimension:      XY
## bbox:           xmin: -71.1728 ymin: 42.23576 xmax: -70.99595 ymax: 42.39549
## CRS:            EPSG:4326
## # A tibble: 3,799 x 3
##       id             geometry fips       
##    <dbl>          <POINT [°]> <chr>      
##  1  3781 (-71.02991 42.36413) 25025051200
##  2  5506 (-71.09559 42.32981) 25025081400
##  3  6695 (-71.09351 42.32994) 25025081400
##  4  8789 (-71.06265 42.35919) 25025030300
##  5 10730  (-71.06185 42.3584) 25025030300
##  6 10813 (-71.08904 42.34961) 25025010104
##  7 10986 (-71.05075 42.36352) 25025030300
##  8 16384  (-71.07132 42.3581) 25025020101
##  9 18711 (-71.06096 42.32212) 25025090700
## 10 22195  (-71.0793 42.34558) 25025010600
## # ... with 3,789 more rows
\end{verbatim}

This is great! We now have the \texttt{fips} (census tract code) associated with each listing \texttt{id}. But what happens when we plot the data?

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(points\_join) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{(}\DataTypeTok{shape =} \StringTok{"."}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-spatial-analysis_files/figure-latex/unnamed-chunk-13-1.pdf}
It is the same as before. What we would like to do at this moment is to plot the tracts and color by the number of listings contained in them. This means that we need to change up the order of our join.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{polygon\_join \textless{}{-}}\StringTok{ }\KeywordTok{st\_join}\NormalTok{(acs\_tracts, loc\_sf)}
\NormalTok{polygon\_join}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 3814 features and 2 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012
## CRS:            4326
## # A tibble: 3,814 x 3
##    fips                                                         geometry      id
##    <chr>                                              <MULTIPOLYGON [°]>   <dbl>
##  1 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  3.63e6
##  2 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  7.22e6
##  3 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  7.29e6
##  4 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  3.36e7
##  5 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  3.63e7
##  6 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  3.89e7
##  7 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  4.02e7
##  8 25025092~ (((-71.06249 42.29221, -71.06234 42.29273, -71.06226 42.29~  4.18e7
##  9 25025100~ (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28~  2.64e7
## 10 25025100~ (((-71.05147 42.28931, -71.05136 42.28933, -71.05032 42.28~  3.77e7
## # ... with 3,804 more rows
\end{verbatim}

Notice that there are 3,814 rows! That is well over the original 193 tracts. If we tried plotting this right away we may overwork R. What we need to do is \emph{count} the number of observations per fips code first.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{tract\_listings \textless{}{-}}\StringTok{ }\KeywordTok{count}\NormalTok{(polygon\_join, fips)}

\NormalTok{tract\_listings}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## Simple feature collection with 203 features and 2 fields
## geometry type:  MULTIPOLYGON
## dimension:      XY
## bbox:           xmin: -71.19125 ymin: 42.22793 xmax: -70.9201 ymax: 42.45012
## CRS:            4326
## # A tibble: 203 x 3
##    fips           n                                                     geometry
##    <chr>      <int>                                           <MULTIPOLYGON [°]>
##  1 250250001~    53 (((-71.1609 42.35863, -71.16049 42.35881, -71.16021 42.3589~
##  2 250250002~    18 (((-71.16782 42.35328, -71.16775 42.35351, -71.16764 42.353~
##  3 250250002~     4 (((-71.16057 42.35267, -71.16018 42.35269, -71.16005 42.352~
##  4 250250003~    19 (((-71.1748 42.35051, -71.17475 42.35066, -71.17471 42.3508~
##  5 250250003~    19 (((-71.17458 42.35024, -71.17287 42.35005, -71.17283 42.350~
##  6 250250004~    16 (((-71.15473 42.34121, -71.15455 42.34156, -71.15436 42.341~
##  7 250250004~    23 (((-71.16613 42.34043, -71.16612 42.34059, -71.16611 42.340~
##  8 250250005~    18 (((-71.16922 42.33807, -71.16909 42.33825, -71.16894 42.338~
##  9 250250005~    16 (((-71.15336 42.33819, -71.15308 42.33833, -71.15296 42.338~
## 10 250250005~     9 (((-71.1501 42.33719, -71.14984 42.33767, -71.14966 42.3379~
## # ... with 193 more rows
\end{verbatim}

After counting we now have our original 203 rows. This is the spatial equivalent of summarizing our data where all of the geometries of the aggregated rows (\texttt{fips}) are \textbf{dissolved}. Dissolving combines all of the geometries of by a shared attribute---it is \texttt{fips} in our case---into a single geometry. Since each \texttt{fips} has the same geometry,the resultant geometries are unaffected. But be aware of the behavior when grouping and summarizing sf objects!

Let's try plotting these counts now.

\begin{Shaded}
\begin{Highlighting}[]
\KeywordTok{ggplot}\NormalTok{(tract\_listings, }\KeywordTok{aes}\NormalTok{(}\DataTypeTok{fill =}\NormalTok{ n)) }\OperatorTok{+}
\StringTok{  }\KeywordTok{geom\_sf}\NormalTok{(}\DataTypeTok{lwd =} \FloatTok{0.25}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{10-spatial-analysis_files/figure-latex/unnamed-chunk-16-1.pdf}

\hypertarget{resources}{%
\subsubsection{Resources}\label{resources}}

\begin{itemize}
\tightlist
\item
  \url{http://wiki.gis.com/wiki/index.php/Dissolve}
\item
  For a full list of spatial joins by data type I recommend visiting \url{https://desktop.arcgis.com/en/arcmap/latest/manage-data/tables/spatial-joins-by-feature-type.htm}.
\item
  \url{https://geocompr.robinlovelace.net/}
\item
  \url{https://rud.is/books/30-day-map-challenge/points-01.html}
\end{itemize}

  \bibliography{book.bib,packages.bib}

\end{document}
