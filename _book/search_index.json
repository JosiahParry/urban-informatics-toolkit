[
["index.html", "Urban Informatics Toolkit Chapter 1 Welcome 1.1 Timeline 1.2 What is Urban Informatics 1.3 What to expect 1.4 Reminders 1.5 Setting the Context: 1.6 outline / notes", " Urban Informatics Toolkit Josiah Parry 2020-02-16 Chapter 1 Welcome 1.1 Timeline To Do: put dates to these. Core Toolkit Draft intro to R, RStudio (R files, and R projects) done 2020-02-15 Draft exploratory analysis Draft filtering data Draft creating variables Draft %&gt;% Draft data structures Draft summary statistics Draft group_by and summarize Draft multiple data sources UI Specific: Draft approaches to UI (inductive vs deductive) Draft schools of UI (hybrid approach) Draft ecometrics Advanced Topics: Draft data reshaping Draft spatial analysis Draft text analysis Editing Review data sources Review scenarios Create post-chapter exercises Create post-chapter TL;DR Make into package Create rstudio.cloud image Welcome to the Urban Informatics Toolkit! This book will provide you with an introduction to some of the history of Urban Informatics as a field, important concepts in the field, and provide you with the skills required to jumpstart your own exploration of the urban commons. 1.2 What is Urban Informatics 1.3 What to expect This book is partitioned into chapters where each goes over a small subset of skills that is important to know and a few case studies. the first half of the book will introduce you to the basic tools that you will need to know to become self-sufficient. From there on, we will work with case studies that also introduce new topics and further more advances skills In Part II each chapter, with a few exceptions, will introduce you to a social phenomenon. We will ask a question, and use a relevant dataset to answer this question. While we work to approach each question, we will pause to learn about the tools that we are using, understand what they are doing and why. There will always be a TL;DR for each chapter recapping what we covered, and links to further resources. technical chapters will be interspersed with non-techinical chapters. 1.4 Reminders Learning to program can be exceptionally difficult at times. It can be a roller coaster of emotions. It is expected that you will not understand everything the first go around. Do not get down on yourself. I encourage you to take breaks and not push yourself too hard. I understand you have deadlines, but sometimes it is better if you take a break, eat a healthy snack, go exercise, sleep, be social, etc and then come back. You will be much happier and your work will be even better and that I promise you! Perhaps I am showing my cards a bit too much, but I have always found that with tough assignments it is better to go to bed early and finish them in the morning than it is to stay up exceptionally late. Moral of that story is, get some sleep and take care of yourself! I’ll be sure to remind you from time to time. 1.5 Setting the Context: 1.5.1 Big Data 1.5.2 Open Data 1.6 outline / notes The study of Urban Informatics is inherently intertwined with big data. Working with big data requires a set of skills that extends beyond quantitative and qualitative analysis expertise. Students need to know how to access, manipulate, analyze, communicate and share data artifacts. The intention of this practicum is to develop educational content to cultivate an expertise in data. An online text and accompanying exercises will be created to teach students data science in a manner that is tailored toward Urban Informatics studies. Writing this content will provide an opportunity to synthesize the learnings and skills taught in the Urban Informatics program. The manner in which it will be done will also further access to the field and assist in the cultivation of future students. I have always struggled with overly academic writing and overly technical and obtuse descriptions of concepts, tasks, and phenomena that can be written in easy to understand ways. my goal for this text is to provide you with a non-technical instruction to a technical topic. I want this to be friendly and not adversarial nor patronizing. I will do my best to try and explain this language and topics the way that I would to my friends over a beer. intro is what is ui What is administrative data Current examples of the data driven city Why we need to learn data with theory The data workflow Question then data Data then questions new approach due to massive data There are many insights yet to be discovered R The hard part is getting started. I want to make this as easy as possible with social science and UI examples bear with me for a bit. * Basics as a calculator * Visualization / Visualizing relationships / Visuializing relationships * the type of the data that we have will dictate how we visualize it * Univariate * Bivariate * Data Manipulation * filtering data Tidy and untidy data * tidy data: * this is just to introduce you to the idea and get an idea of what it looks like. No need to understand the code or anything that will show up afterwards. * the principles * demonstrate what it looks like The boring part data structures Administrative data data cleaning manopulatoon Aggregation Introduce interacting with public data via webportals and APIs Geospatial analysis: data are often grounded in space its important to understand where things happen 3 Common data that are associated with space 311 - point (vector data) census tracts - polygon (vector) sattelite imagery - raster (we wont cover this) polygon is the most simple to work with visualization of aggregate data aggregating by polygon (building permits) points are a bit trickier often we want to join this Text analysis unstructured text as data what is the scope and limitations of text data? working with text data: stop words tokenization "],
["the-basics.html", "Chapter 2 The basics 2.1 What is R and why do I care? 2.2 The RStudio IDE 2.3 Installing R &amp; RStudio 2.4 Preventative Care 2.5 Before we embark", " Chapter 2 The basics 2.1 What is R and why do I care? What is R? R is the 18th letter of the alphabet, the fourth letter in QWERTY—like the keyboard—and, most importantly, R is a software package for statistical computing. R is a decendant of the S statistical programming language whose naissance can be traced back to 1976 in Bell Labratories (https://web.archive.org/web/20181014111802/http://ect.bell-labs.com/sl/S/). As S developed, people sought to commercialize the language. In 1993, the license as well as development and selling rights were given to a private company. From then on, and what is still the case today, S became available only as the commercialized S-PLUS (Martin, 1996). Later, seeing a need for an improved statitistical software environment, two researchers from the University of Aukland created a new statistical programming language, this became known as R. R was developed in the image of S. However, one important early decision to make the R-project free and open source changed its fate dramatically. Today, the R-project is developed and maintained by a group known as the R Core who “represent multiple statistical disciplines and are based at academic, not-for-profit and industry-affiliated institutions on multiple continents” (R SLDC, pg8). They define R as below. R is an integrated suite of software facilities for data manipulation, calculation and graphical display. https://www.r-project.org/about.html To simplify it, R can be thought of as a fancy calculator. R was designed to do math, specifically statistics. R was designed to be extended to incude further capabilities than just statistics. Indeed it has been. While R is for all intents and purposes a programming language, one should, in theory, feel like they are doing data analysis and not programming (chambers). R is unique from other commercial statistical software such as stata and SPSS. Very fundamentally, R is a free project. While it is monetarily free, free refers to “liberty, not price” (gnu.org). In order to truly understand the adventure you will be embarking on shortly, I think it is important you familiarize yourself with the four freedoms of free software. These are: The freedom to run the program as you wish, for any purpose (freedom 0). The freedom to study how the program works, and change it so it does your computing as you wish (freedom 1). Access to the source code is a precondition for this. The freedom to redistribute copies so you can help others (freedom 2). The freedom to distribute copies of your modified versions to others (freedom 3). By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this. These freedoms are a large part of the success of R as a language. Because of the free nature of R, academics and industry experts from around the globe are contributing to the language. This means that many new statistical techniques are first implemented in R. The contributions that people make to R are changing the ways in which people perform data analysis. Because of this, we need to start contextualizing the tooling we use as part of the scientific process—not apart from it. When you engage in your analyses and work on contribute to the vast body of scientific literature, remember that without the tools you are using, much of it would not be possible. When you engage in science, think to yourself how you are adhering to the four essential freedoms. Are you enabling others to do with your findings as they wish? Will your research be accessible to the greater community? What will you do to “give the whole community a chance to benefit from your [work]”? https://cran.r-project.org/ 2.2 The RStudio IDE When R is downloaded, interacting with it is somewhat of a cumbersome process. While some people love it, it can feel like programming in the matrix. For this reason, we will use RStudio to program in R. RStudio is an integrated development environment (IDE). This means that most of the features that you will need to develop in R will all be in one place. RStudio gives you a place to write your R code, execute it, view the awesome graphics you produce, and much more. I like to think of R as typesetting a printing press and using RStudio like using Mircrosoft word. Chester Ismay and Albert Kim’s Modern Dive, provide another excellent analogy of R and RStudio. They describe R as the engine of a car, and RStudio as the dashboard (https://moderndive.com/1-getting-started.html). Let’s get familiar with RStudio. You need to know where you are when working within RStudio. There are 4 quadrants that we work with (called panes). This lovely graphic was created graphics by Thomas mock. Source 2.2.1 The Editor The editor. The top left pane. This is where you will actually write your code. You will see in the image above that there is tab with the name of the R file being edited, mpg-plot.R. The simplest way in which R code is written, is in documents with the .R extention. Think of the R script as your word document. This is where you put the writing that you want to keep. There is also a second type of R file called an RMarkdown document, .Rmds. These are a special type of file that lets us intersperse regular prose with code chunks. Rmd is extremely flexibile and enabled the user to render their content in many different formats such as a pdf, powerpoint, html, and others. For example, this book is written with RMarkdown. But, to keep things simple, we will use R scripts for the vast majority of this book, plus they’re my favorite way to interact with R. 2.2.2 The console Let us now avert our attention to the bottom left pane. This is known as the console. The console is where your R code is actually execute. When you run a line or chunk of code from your editor, you will see it processed in the console. I often treat my console as my scratch paper. This is a place where I can explore R objects and code without affecting the primary R file. You should become comfortable typing your R code in the editor and see it executed in the console. 2.2.3 Output Now moving over to the right. This is the most versatile quadrant of RStudio. You will primarily use this quadrant to look at things. There is a pane for navigating your files, looking at help documentation, viewing the charts that you produce, and any interactive applications you may develop. 2.3 Installing R &amp; RStudio Now that you are somewhat familiar with R and RStudio, it is time to install them. I recommend installing R and then RStudio. R can be downloaded from the Central R Archival Network (CRAN). CRAN is the official location for all things R. CRAN provides access to the R software, license, copyright, and software extensions (called R packages). Go to CRAN to download R. RStudio is provided by RStudio, Public Benefit Corporation (RStuio, PBC). To install RStudio navigate to the download page. Once both have been installed you can open RStudio to get started. Look for the circular R logo If you get lost navigating the RStudio IDE, be sure to refer to the cheat sheet. 2.4 Preventative Care 2.4.1 R Projects Once you open up RStudio you will be able to get rocking and rolling. Though, I want to instill some best practices from the get go. This following section will save you and anyone who you collaborate with an undescribable amount of headaches. Folks are tempted to open up RStudio and begin doing analysis. That is all well and good though this leads to many problems. We need to contextualize each and every analysis as it’s own project. Currently I mean project in the conceptual manner. If there is a common overarching theme, intent, or purpose, that analysis should be delineated as its own project and should be identifiable from others. You probably already have a notion of projects implemented in your life. Consider your school work. I suspect, and frankly hope, that you have a somewhat organized folder structure where each semester is its own folder, and each course is its own folder within that. An example of what some of my folder organization looks like below. fall/ big-data-for-cities/ projects/ urban-theory/ literature/ spring/ info-design/ data/ intro-data-mining/ In the above case we would consider each course as its own project. The important thing to keep in mind here is that each project is self-contained. By working inside of self-contained folders we can ensure that there are no problems with accessing files. R uses the concept of a working directory. Think of the working directory as “where do I start when I am looking for things?” Imagine R is using the working directory spring but you are working on your info-design work. Conceptually, you feel as if you’re working from the info-design folder, but R is actually under the impression you are working from spring. So when you try to load some data from the data folder inside of info-design (which looks like info-design/data), you have to tell R how to get there. And the way R would get there is probably different than you think at the time. To prevent this, we can essentially level set with R by creating a project. An R project gently imposes the standalone structure that we will need to prevent most of the headaches described above. The way to create a new project is by navigating to File &gt; New Project. Step 1: new Project Click New Directory. This will create a new folder for you. Next, RStudio will prompt you to specify what kind of project to create. Today, that will be a generic New Project. In the future, I suspect you will end up creating Shiny applications and much more. Step 2: New Project The final step is to specify what the project will be called and where to put it. In the below image I name the new directory uitk, short for Urban Informatics Toolkit, and place it in the directory R. Be sure to select which folder you want your project to live in. A few tips: Think about where you will be able to find the project again Where would it make the most sense for the project to live? Do not put spaces or periods in the directory name. Use _ or - if you feel the need. Step 3: Naming the project This will open up a new RStudio session. You will notice in your Files pane that there is now a uitk.Rproj file there. That file is what tells RStudio about the project, so don’t delete it! If you would like to open up an RStudio project you can either open the .Rproj file from your file navigator or open it by following File &gt; Open Project. 2.4.2 Your Workspace In another effort to impose good habits and reproducibility standards I will suggest you change one setting in RStudio. Navigate to Tools &gt; Global Options now change the below setting. This setting makes it so that your analysis is dependent upon the code you write, not the things you create while interactively programming. A general rule of thumb is that your R script should be able to run from top to buttom successfully. 2.5 Before we embark Lastly, I want to emphasize that R and RStudio can be used for so much more than statistical analysis. It can be used to make aRt. It can be used to make beautiful graphics for the BBC. R can be found in the infrastructure of our modern world. R is utilized in our global financial institutions, civil rights groups such as the ACLU, investigative journalism, national defense, and so much more. Do not feel that the only thing you will get from learning R is how to do some simple statistics. Now, let’s get going. "],
["r-as-a-calculator.html", "Chapter 3 R as a calculator 3.1 Arithmetic Operators 3.2 Variable assignment 3.3 Functions 3.4 Extensions to R 3.5 Loading Packages", " Chapter 3 R as a calculator Before we get going, let’s find our footing. R is a statistical programming language. That means that R does math and pretty well too. In this chapter you’ll learn the basics of using R including: arithmetic operators creating and assigning variables using functions Executing code: - (either with command + enter on the highlighted code or the line where your cursor is) or the run button above i recommend learning the key strokes. it will be immensely helpful this brings us to the console 3.1 Arithmetic Operators Do you remember PEMDAS? If not, a quick refresher that PEMDAS specifies the order of operations for a math equation. Do the math inside of the parentheses, then the exponents, and then the multiplication or the division before addition or subtraction. We can’t write the math out, so we need to type it out. Below are the basic arithmetic operators ^ : exponentiation (exponents) [E] * : multiplication [M] / : division [D] + : addition [A] - : subtraction [S] These can be used together in parentheses [P] ( ) to determine the order of operations (PEMDAS) Try out some mathematic expressions in the console. 3.2 Variable assignment I’m sure you recall some basic algebra questions like \\(y = 3x - 24\\). In this equation, x and y are variables that represents some value. We will often need to create variables to represent some value or set of values. In R, we refer to variables as objects. Objects can be a single number, a set of words, matrixes, and so many other things. To create an object we need to assign it to some value. Object assignment is done with the assignment operator which looks like &lt;-. You can automagically insert the assignment operator with opt + -. Let’s work with the above example. We will solve for y when x is equal to 5. First, we need to assign 5 to the variable x. x &lt;- 5 If you want to see the contents of an object, you can print it. To print an object you can type the name of it. x ## [1] 5 We can reference the value that x stores in other mathematic expressions. Now what does y equal? Now solve for y in the above equation! y &lt;- 3 * x - 24 y ## [1] -9 3.3 Functions Functions are a special kind of R object. Very simply, a function is an object that performs some action and (usually) produces an output. Functions exist to simplify a task. You can identify a function by the parentheses that are appended to the function name. A function looks like function_name(). R has many functions that come built in. The collection of functions that come out of the box with R are called *Base R**. An example of a simple base R function is sum(). sum() takes any number of inputs and calculates the sum of those inputs. We can run sum() without providing any inputs. sum() ## [1] 0 We can provide more inputs (formally called function arguments) to sum(). For example to find the sum of 10 we write sum(10) ## [1] 10 The sum of a single number is the number itself. We can provide more arguments to sum(). Additional arguments are specified by separating them with commas—e.g. function(argument_1, argument_2). To find the sum of 10, 3, and 2 we write sum(10, 3, 2). sum(10, 3, 2) ## [1] 15 Much of the analysis we will do is done with functions. You will become much more comfortable with them rather quickly. If you ever need to know how a function works, you can look at its help page by typing ?function_name() in your console. That will bring up the documentation page in the bottom right pane. 3.4 Extensions to R While R was created as a statistical programming language, it was designed with the intention of being extended to include even more functionality. Extensions to R are called packages. R packages often provide a set of functions to accomplish a specific kind of task. To analyse, manipulate, and visualize our data, we will use a number of different packages to do so. Throughout this book we will become familiar with a set of packages that together are known as the Tidyverse. R packages do not come installed out of the box. We will need to install them our selves. Base R includes a function called install.packages(). install.packages() will download a specified package from CRAN and install it for us. To download packages, we must tell install.packages() which package to download. We will provide the name of the package as the only argument to install.packages(). The name of the package needs to be put into quotations such as install.packages(&quot;package-name&quot;). Note: By putting text into quotations we are creating what is called a character string. Reminder: we create objects with the assignment operator &lt;-. When we don’t use quotes (create a character string), R thinks we are referring to an object we have created. 3.4.1 Exercise Use your new knowlege of functions and installing packages to install the tidyverse. install.packages(&quot;tidyverse&quot;) 3.5 Loading Packages Now that you have installed the tidyverse, you are going to need to know how to make it available to you for use. To load a package, we use the function library(). Oddly, though, when specifying which package to load, we do not put that name in quotations. Note: It is best practice to load all of your packages at the top of your R script. library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ──────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Notice the message above. When we load the tidyverse, we are actually loading eight packages at once! Thesee are the packages listed under “Attaching packages.” You have now successfully installed and loaded the tidyverse. Next, we will begin to learn how to visually analyze data! 3.5.1 Resources: https://techterms.com/definition/string "],
["visual-analysis.html", "Chapter 4 Visual Analysis 4.1 Data Exploration 4.2 The American Community Survey", " Chapter 4 Visual Analysis 4.1 Data Exploration the proliferation of data has provided vast troves of data asking to be explored in essensce we are at the beginning of a new gold rush for the purposes of scientific inquiry, analysts have historically been rather close to the data generation mechanism much of the open and public data that supports much of urban informatics and the digital humanities was not collected with the intent of being analysed per se this has led to somewhat of a change in the way that data analysis has been approached the process of exploratory data analysis becomes extremely handy in developing insights from data in R for Data Science Garret Grolemund and Hadley Wickham describe the inductive approach of exploratory data analysis as the following: Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. when a research is close to the data generation process, they are more likely to have preconceived hypotheses and an expectation of what they may find this condition isn’t always the case when working with open data we do not necessarily know what we are looking for at the outset with open data and even any dataset in fact, you never know what you may find if you being to dig. You may find the seedling that sprouts into your next study below is an image from R for Data Science which accurately describes the data exploration process there are 6 steps in this illustration, 3 of which happen in a cyclical process this chapter focuses on visualizing our data the importance of visualization cannot be understanded An older National Institute of Standards and Technology (NIST) statistical manual contains this gem The reason for the heavy reliance on graphics is that by its very nature the main role of EDA is to open-mindedly explore, and graphics gives the analysts unparalleled power to do so, enticing the data to reveal its structural secrets, and being always ready to gain some new, often unsuspected, insight into the data. In combination with the natural pattern-recognition capabilities that we all possess, graphics provides, of course, unparalleled power to carry this out. source In the following section, you will become acquainted with visual analysis and the American Community Survey. 4.2 The American Community Survey we’ll work with data from the american community survey. this is data you will get really familiar with what is the acs and why do we love it? https://www.vox.com/explainers/2015/12/3/9845152/acs-survey-defunded random sample of individuals across the us random samples are used as they representative and statistically non-biased the information is used to determine funding tells us about age, ethnicity, country of origin, occupation, education, voting behavior, etc. this information is available in the decennial census (in the constitution) ACS tells us about rates rather than the actual number of people in a thing one of the major problems is that some populations are under-represented https://prospect.org/economy/insidious-way-underrepresent-minorities/ What is the relationship between education and income? we have a data frame loaded this is very similar to a table in excel each column is a variable acs_edu ## # A tibble: 1,456 x 7 ## med_house_income less_than_hs hs_grad some_coll bach white black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 0.0252 0.196 0.221 0.325 0.897 0.0122 ## 2 69625 0.0577 0.253 0.316 0.262 0.885 0.0171 ## 3 70679 0.0936 0.173 0.273 0.267 0.733 0.0795 ## 4 74528 0.0843 0.253 0.353 0.231 0.824 0.0306 ## 5 52885 0.145 0.310 0.283 0.168 0.737 0.0605 ## 6 64100 0.0946 0.294 0.317 0.192 0.966 0.00256 ## 7 37093 0.253 0.394 0.235 0.101 0.711 0.0770 ## 8 87750 0.0768 0.187 0.185 0.272 0.759 0.0310 ## 9 97417 0.0625 0.254 0.227 0.284 0.969 0.00710 ## 10 43384 0.207 0.362 0.262 0.124 0.460 0.105 ## # … with 1,446 more rows we can visualize the relationship to get a better understanding. we can look at income and college grad rates start building a visualization with ggplot() function with the acs_edu object. Functions are characterised by the parentheses at the end of them. Functions do things. Objects hold information. ggplot(acs_edu) we add to this plot determine what we want to plot with the aesthetics aes() function inside of the ggplot(). want to specify x and y. These are called arguments. To set argument we use the = sign. set x to bach and y to med_house_income ggplot(acs_edu, aes(x = bach, y = med_house_income)) notice how the plot is being filled a bit more? now we need to specify what type of plot we will be creating. add geometry, or geoms. We use the plus sign + to signify that we are adding on top of the basic graph there are many different kinds of charts we can use we will get into these a bit more later. a common way of visualizing a relationship between two variables is with a scatterplot a scatter plot graphs points for each x and y pair. you’ve likely made a few of these in your primary education to add points to the graph we use geom_point(), remember, we are adding a layer so we use the plus sign for legibility we add each new layer on a new line. R will indent for you. Good style is important. We’ll get into this later ggplot(acs_edu, aes(x = bach, y = med_house_income)) + geom_point() notice that there is a positive linear trend. lets break that down: when the point point up to the right, that is positive, down to the left is negative. for each unit we go up on the x (bach) we tend to go up on the y (hh income) linear means it resembles a line what are we looking for in a scatter plot? we’re looking for low variation and a consistent pattern or line we want the points to be very close imagine we drew a line going through the middle of the points, we’d want each point to be either on that line or extremely close. if the line is further away, that means there is a lot more variation to finish this up we can add some informative labels we will add a labels layer with the function labs() give them more legible labels. we will give each axis a better name and give the plot a title the arguments we will set to the labs function are x, y and title. Rather intuitive, huh? x = “% of population with a Bachelor’s Degree” y = “Median Household Income” title = “Relationship between Education and Income” note that for each argument I have a new line. again, this helps with legibility ggplot(acs_edu, aes(x = bach, y = med_house_income)) + geom_point() + labs(x = &quot;% of population with a Bachelor&#39;s Degree&quot;, y = &quot;Median Household Income&quot;, title = &quot;Relationship between Education and Income&quot;) what can we determine from this graph? in the sociology literature there is a lot about the education gap between white and black folks can we see this in a graph? we can modify our existing plot to illustrate this too. we can map the % white to the color of the chart we add to this within the aesthetics. The aes()thetics is where we will determine things like size, group, shape, etc. set the color argument to the white column while we’re at it, we can add a subtitle to inform that we’re also coloring by % white ggplot(acs_edu, aes(x = bach, y = med_house_income, color = white)) + geom_point() + labs(x = &quot;% of population with a Bachelor&#39;s Degree&quot;, y = &quot;Median Household Income&quot;, title = &quot;Relationship between Education and Income&quot;, subtitle = &quot;Colored by whiteness&quot;) what can we conclude now? 4.2.0.1 Misc Notes: this chapter is to introduce the concept of exploratory data analysis this should be done at a later point NIST EDA: https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm https://r4ds.had.co.nz/explore-intro.html iteration this chapter focuses on visualization the process of exploratory anlaysis is naturally inductive "],
["schools-of-urban-informatics.html", "Chapter 5 Schools of Urban Informatics", " Chapter 5 Schools of Urban Informatics Sociological origins the chicago school Park and Burgess the key findings from The City Discuss how these findings were generated inductive approaches to theory, ONLY the sante fe school this is the complete other end of the example sante fe is a group of physical scientists looking to use hyper-modern techniques and apply theory to observed social data this group has a general set of theories that they use to evaluate urban life where does the Boston School fit in? let’s step back to understand the two general approaches to scientific inquiry Inductive - finding something from the data (think chicago) Deductive - having a theory and setting out to explicitly test that (think psychology experiment) Overview of inductive Overview of deductive the Boston School is a hybrid approach Next Chapter: Let us use the example of broken windows theory and the development of ecometrics some questions to ponder: what is a strnegth of the chicago school? what is the strength of the santa fe institute? https://www.santafe.edu/research/projects/cities-scaling-sustainability what do you think is the ideal approach? is it one or the other? a blend? something entirely new? "],
["broken-windows-and-the-origin-of-ecometrics.html", "Chapter 6 Broken Windows and the origin of Ecometrics", " Chapter 6 Broken Windows and the origin of Ecometrics To Cover: sampson &amp; raudenbush systematic social observation: https://www.journals.uchicago.edu/doi/abs/10.1086/210356 ecometrics: where was this first coined for the social sciences???? 1980 “Broken Windows” in the Atlantic Posited that the presence of physical disorder led to crime this influenced policing policies like stop and frisk led to the infamous implementation of CompStat the simplicity of the theory was appealing to public officials. media lapped it up social scientists were quick to shoot down the theory, seemed to not matter come 1999 Sampson and Raudenbush seek to test this find no conclusion much more work comes from it most noteably is the use of ecometrics in assessing this. I say most noteably because of its approach it combines an inductive approach with a deductive one. this is the quintessence of the Boston approach, of course it comes from Dan O’Brien. why?: big data (actually big-ish which is rare in social science) The development and use of ecometrics (induction) deliberately testing an existing theory (deduction) Absolutely crushing it. what is an ecometric? why is it inductive? taking naturally occuring data, and extracting latent (already existing variables) Note: a case study should be recreating these ecometrics Explore what the data description from BARI looks like library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ──────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ecometrics &lt;- readr::read_csv(&quot;data/911/911-ecometrics-2014-19.csv&quot;) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## TYPE = col_character(), ## tycod = col_character(), ## typ_eng = col_character(), ## sub_tycod = col_character(), ## sub_eng = col_character(), ## SocDis = col_double(), ## PrivateConflict = col_double(), ## Violence = col_double(), ## Guns = col_double(), ## Frequency_2015 = col_double(), ## Frequency_2016 = col_double(), ## Frequency_2017 = col_double(), ## Frequency_2018 = col_double(), ## yr.intro = col_double(), ## last.yr = col_double() ## ) glimpse(ecometrics) ## Observations: 302 ## Variables: 15 ## $ type &lt;chr&gt; &quot;AB===&gt;&gt;&gt;&quot;, &quot;ABAN===&gt;&gt;&gt;&quot;, &quot;ABANBU&quot;, &quot;ABANCELL&quot;,… ## $ tycod &lt;chr&gt; &quot;AB&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;… ## $ typ_eng &lt;chr&gt; &quot;ASSAULT AND BATTERY&quot;, &quot;ABANDONED CALL&quot;, &quot;ABAND… ## $ sub_tycod &lt;chr&gt; &quot;===&gt;&gt;&gt;&quot;, &quot;===&gt;&gt;&gt;&quot;, &quot;BU&quot;, &quot;CELL&quot;, &quot;INCCAL&quot;, &quot;PH… ## $ sub_eng &lt;chr&gt; &quot;PICK A SUB-TYPE&quot;, &quot;PICK A SUB-TYPE&quot;, &quot;FROM A B… ## $ soc_dis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ private_conflict &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ violence &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,… ## $ guns &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ frequency_2015 &lt;dbl&gt; 27, 40, 12768, 72081, 686, 1734, 3143, 194, 122… ## $ frequency_2016 &lt;dbl&gt; 29, 26, 13287, 56074, 1188, 1205, 2687, 397, 19… ## $ frequency_2017 &lt;dbl&gt; 29, 17, 12599, 39323, 124, 724, 1944, 253, 162,… ## $ frequency_2018 &lt;dbl&gt; 14, 10, 6422, 19947, 70, 506, 840, 262, 169, 78… ## $ yr_intro &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,… ## $ last_yr &lt;dbl&gt; 2019, 2018, 2019, 2019, 2019, 2019, 2019, 2019,… Check out raw_911 &lt;- read_csv(&quot;data/911/911-raw.csv&quot;, n_max = 5000) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## INCIDENT_NUMBER = col_character(), ## OFFENSE_CODE = col_character(), ## OFFENSE_CODE_GROUP = col_character(), ## OFFENSE_DESCRIPTION = col_character(), ## DISTRICT = col_character(), ## REPORTING_AREA = col_double(), ## SHOOTING = col_character(), ## OCCURRED_ON_DATE = col_datetime(format = &quot;&quot;), ## YEAR = col_double(), ## MONTH = col_double(), ## DAY_OF_WEEK = col_character(), ## HOUR = col_double(), ## UCR_PART = col_character(), ## STREET = col_character(), ## Lat = col_double(), ## Long = col_double(), ## Location = col_character() ## ) raw_911 %&gt;% filter(str_detect(offense_description, coll(&quot;battery&quot;, T))) ## # A tibble: 342 x 17 ## incident_number offense_code offense_code_gr… offense_descrip… district ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 I192078613 00802 Simple Assault ASSAULT SIMPLE … A7 ## 2 I192078603 00802 Simple Assault ASSAULT SIMPLE … A7 ## 3 I192078600 00413 Aggravated Assa… ASSAULT - AGGRA… C11 ## 4 I192078573 00802 Simple Assault ASSAULT SIMPLE … B2 ## 5 I192078563 00413 Aggravated Assa… ASSAULT - AGGRA… B3 ## 6 I192078563 00802 Simple Assault ASSAULT SIMPLE … B3 ## 7 I192078538 00802 Simple Assault ASSAULT SIMPLE … C6 ## 8 I192078531 00802 Simple Assault ASSAULT SIMPLE … C11 ## 9 I192078530 00413 Aggravated Assa… ASSAULT - AGGRA… C6 ## 10 I192078529 00413 Aggravated Assa… ASSAULT - AGGRA… E5 ## # … with 332 more rows, and 12 more variables: reporting_area &lt;dbl&gt;, ## # shooting &lt;chr&gt;, occurred_on_date &lt;dttm&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, ## # day_of_week &lt;chr&gt;, hour &lt;dbl&gt;, ucr_part &lt;chr&gt;, street &lt;chr&gt;, ## # lat &lt;dbl&gt;, long &lt;dbl&gt;, location &lt;chr&gt; 6.0.1 Resources: https://www.annualreviews.org/doi/abs/10.1146/annurev-criminol-011518-024638?journalCode=criminol Large-scale data use, ecometrics to assess disorder: https://journals.sagepub.com/doi/abs/10.1177/0022427815577835 "],
["reading-data.html", "Chapter 7 Reading data 7.1 Background 7.2 Actually Reading Data 7.3 Other common data formats", " Chapter 7 Reading data We spent the last chapter performing our first exploratory visual analysis. From our visualizations we were able to inductively conlcude that as both median household income and the proportion of the population with a bachelors degree increases, so does the share of the population is white. This is a finding we will work to address empirically at a later point. While we were able to make wonderful visualizations, we did skip a number of steps in the exploratory analysis process! Arguably most importantly we skipped the importing of our datasets. The ACS dataset was already loaded into R for you. This almost always will not be the case. As such, we’re going to spend this chapter learning about some of the common data formats that you will often work with. { image of the visualization step } 7.1 Background There are three general sources where we as social scientists will recieve or access data: 1) text files, 2) databases, and 3) application programming interfaces (APIs). Frankly, though this is the age of “big data,” we are not always able to interface directly with these sources. But through partnership efforts between the public and private we able to share data. For example, BARIs work with the Boston Police Department provides them with annual access to crime data. But BARIs access is limited. They do not have credentials to log in to the database and perform their own queries. What they are usually presented with is a flat text file(s) that contains the data requisite for analysis. And this is what we will focus in this chapter. Flat text files will be sufficient for 85% of all of your data needs Now, what do I mean by flat text file? A flat text file is a file that stores data in plain text—I know, this seems somewhat confusing. In otherwords, you can open up a text file and actually read the data with your own eyes or a screenreader. For a long while tech pundits believed—and some still do—that text data will be a thing of the past. Perhaps this may be true in the future, but plain text still persists and there are some good reasons for that. Since plain text is extremely simple it is lightweight and usually does not take up that much memory. Also, because there is no fancy embelishing of the data in a plain text file, they can be easily shared from machine to machine without concern of dependent tools and software. Not to mention that we humans can actually be rather hands on and inspect the source of the data ourselves. 7.2 Actually Reading Data Within the tidyverse there is a package called readr which we use for reading in rectangular data from text files. Aside: I am still unsure if it is pronounced read-arr or read-er. So just do your best. I just threw the phrase rectangular data at you. It is only fair to actually describe what that means. If you were to look at rectangular data in something like excel it would resemble a rectangle. In fancy speak, rectangular data is a two-dimensional data structure with rows and columns. We will learn more about the “proper” way to shape rectangular data in the “tidying data” chapter. For now, all you need to know is that there are rows and columns in rectangular data. To get started, let us load the tidyverse. This will load readr for us. library(tidyverse) You most likely have seen and encountered flat text files in the wild inthe form of a csv. It is important to know what csv stands for because it will help you understand what it actually is. it stands for comma separated values. _csv_s are a flat text data file where the data is rectangular! Each new line of the file indicates that there is a new row. Within each row, each comma indicates a new column. If you opened one up in a text editor like text edit or notepad a csv would look something like below. column_a, column_b, column_c, 10, &quot;these are words&quot;, .432, 1, &quot;and more words&quot;, 1.11 To read a csv we use the readr::read_csv() function. read_csv() will read in the csv file and create a tibble. A tibble is type of a data structure that we will be interacting with the most throughout this book. A tibble is a rectangular data structure with rows and columns. Since a csv contains rectangular data, it is natural for it to be stored in a tibble. Note: the syntax above is used for referencing a function from a namespace (package name). The syntax is pkgname::function(). This means the read_csv() function from the package readr. This is something you will see frequently on websites like StackOverflow. Have a look at the arguments of read_csv() by entering ?read_csv() into the console. You will notice that there are many arguments that you can set. These are there to give you a lot of control over how R will read your data. For now, and most of the time, we do not need to be concerned about these extra arguments. All we need to do is tell R where our data file lives. If you haven’t deduced from the help page yet, we will supply only the first argument file. This argument is either a path to a file, a connection, or literal data (either a single string or a raw vector). Note: When you see the word string, that means values inside of quotations—i.e. “this is a string”. We will read in the dataset that was already loaded in the first chapter. These data are stored in the file named acs_edu.csv. We can try reading this as the file path. read_csv(&quot;acs_edu.csv&quot;) ## Error: &#39;acs_edu.csv&#39; does not exist in current working directory ## (&#39;/Users/Josiah/GitHub/urban-commons-toolkit&#39;). Oops. We’ve got red text and that is never fun. Except, this is a very important error message that, frankly, you will get a lot. Again it says: Error: ‘acs_edu.csv’ does not exist in current working directory I’ve bolded two portions of this error message. Take a moment to think through what this error is telling you. For those of you who weren’t able to figure it out or just too impatient (like myself): this error is telling us that R looked for the file we provided acs_edu.csv but it could not find it. This usually means to me that I’ve either misspelled the file name, or I have not told R to look in the appropriate folder (a.k.a. directory). acs_edu.csv actually lives in a directory called data. To tell R—or any computer system, really—where that file is we write data/acs_edu.csv. This tells R to first enter the data directory and then look for the acs_edu.csv file. Now, read the acs_edu.csv file! read_csv(file = &quot;data/acs_edu.csv&quot;) ## Parsed with column specification: ## cols( ## med_house_income = col_double(), ## less_than_hs = col_double(), ## hs_grad = col_double(), ## some_coll = col_double(), ## bach = col_double(), ## white = col_double(), ## black = col_double() ## ) ## # A tibble: 1,456 x 7 ## med_house_income less_than_hs hs_grad some_coll bach white black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 0.0252 0.196 0.221 0.325 0.897 0.0122 ## 2 69625 0.0577 0.253 0.316 0.262 0.885 0.0171 ## 3 70679 0.0936 0.173 0.273 0.267 0.733 0.0795 ## 4 74528 0.0843 0.253 0.353 0.231 0.824 0.0306 ## 5 52885 0.145 0.310 0.283 0.168 0.737 0.0605 ## 6 64100 0.0946 0.294 0.317 0.192 0.966 0.00256 ## 7 37093 0.253 0.394 0.235 0.101 0.711 0.0770 ## 8 87750 0.0768 0.187 0.185 0.272 0.759 0.0310 ## 9 97417 0.0625 0.254 0.227 0.284 0.969 0.00710 ## 10 43384 0.207 0.362 0.262 0.124 0.460 0.105 ## # … with 1,446 more rows This is really good! Except, all that happened was that the function was ran. The data it imported was not saved anywhere which means we will not be able to interact with it. What we saw was the output of the data. In order to interact with the data we need to assign it to an object. Reminder: we assign object with the assignment operator &lt;-—i.e. new_obj &lt;- read_csv(&quot;file-path.csv&quot;). Objects are things that we interact with such as a tibble. Functions such as read_csv() usually, but not always, modify or create objects. In order to interact with the data, let us store the output into a tibble object called acs. acs &lt;- read_csv(file = &quot;data/acs_edu.csv&quot;) Notice how now there was no data printed in the console. This is a good sign! It means that R read the data and stored it properly into the acs object. When we don’t store the function results, the results are (usually) printed out. To print an object, we can just type it’s name into the console. acs ## # A tibble: 1,456 x 7 ## med_house_income less_than_hs hs_grad some_coll bach white black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 0.0252 0.196 0.221 0.325 0.897 0.0122 ## 2 69625 0.0577 0.253 0.316 0.262 0.885 0.0171 ## 3 70679 0.0936 0.173 0.273 0.267 0.733 0.0795 ## 4 74528 0.0843 0.253 0.353 0.231 0.824 0.0306 ## 5 52885 0.145 0.310 0.283 0.168 0.737 0.0605 ## 6 64100 0.0946 0.294 0.317 0.192 0.966 0.00256 ## 7 37093 0.253 0.394 0.235 0.101 0.711 0.0770 ## 8 87750 0.0768 0.187 0.185 0.272 0.759 0.0310 ## 9 97417 0.0625 0.254 0.227 0.284 0.969 0.00710 ## 10 43384 0.207 0.362 0.262 0.124 0.460 0.105 ## # … with 1,446 more rows This is sometimes a little overwhelming of a view. For previewing data, the function dplyr::glimpse() (there is the namespace notation again) is a great option. Try using the function glimpse() with the first argument being the acs object. glimpse(acs) ## Observations: 1,456 ## Variables: 7 ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, 3709… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426318,… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298192,… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.2… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124279,… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.7… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.030640… 7.2.1 Exercise Recreate a plot from the previous chapter using our newly imported acs tibble object. ggplot(acs, aes(x = bach, y = med_house_income)) + geom_point(alpha = 0.25) + labs(x = &quot;% of population with a Bachelor&#39;s Degree&quot;, y = &quot;Median Household Income&quot;, title = &quot;Relationship between Education and Income&quot;) 7.3 Other common data formats While csv files are going to be the most ubiquitous, you will invariably run into other data formats. The workflow is almost always the same. If you want to read excel files, you can use the function readxl::read_excel() from the readxl package. acs_xl &lt;- readxl::read_excel(&quot;data/acs_edu.xlsx&quot;) glimpse(acs_xl) ## Observations: 1,456 ## Variables: 7 ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, 3709… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426318,… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298192,… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.2… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124279,… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.7… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.030640… Another common format is a tsv which stands for tab separated format. readr::read_tsv() will be able to assist you here. If for some reason there are special delimiters like |, the readr::read_delim() function will work best. For example readr::read_delim(&quot;file-path&quot;, delim = &quot;|&quot;) would do the trick! Additionally, another extremely common data type is json which is short for javascript object notation. json is a data type that you will usually not read directly from a text file but interact with from an API. If you do happen to encounter a json flat text file, use the jsonlite package. jsonlite::read_json(). In the next chapter we will begin to work with dplyr and start getting hands on with our data. "],
["general-data-manipulation.html", "Chapter 8 General data manipulation 8.1 Scenario: 8.2 Getting physical with the data 8.3 Selecting Rows 8.4 Revisiting commmuting 8.5 Asking good questions 8.6 filter 8.7 mutate 8.8 writing data lol", " Chapter 8 General data manipulation Now that you have the ability to read in data, it is important that you get comfortable handling it. Some people call the process of rearranging, cleaning, and reshaping data massaging, plumbing, engineering, and myriad other names. Here, we will refer to this as data manipulation. This is a preferrable catch-all term that does not ilicit images of toilets or Phoebe Buffay (she was a masseuse!). You may have heard of the 80/20 rule, or at least one of the many 80/20 rules. The 80/20 rule I’m referring to, is the idea that data scientists will spend 80% or more of their time cleaning and manipulating their data. The other 20% is the analysis part—creating statistics and models. I mention this because working with data is mostly data manipulation and only some statistics. Be prepared to get your hands dirty with data—euphamisms like this is probably why the phrase data plumbing ever came about. In this chapter we will learn how to preview data, select columns, arrange rows, and filter rows. This is where we get hands on with our data. This is just about as physical as we will be able to get unless you want to open up the raw csv and edit the data there! 8.0.0.1 CHECK THIS LATER Note: I do not recommend editing any csvs directly. That statement was in jest. We will cover the concept of reproducibility. This is all to say that you will find yourself with messy, unsanitized, gross, not fun to look at data most of the time. Because of this, it is really important that we have the skills to clean our data. Right now we’re going to go over the foundational skills we will learn how to select columns and rows, filter data, and create new columns, and arrange our data. To do this, we will be using the dplyr package from the tidyverse. The data we read in in the last chapter was only a select few variables from the annual census data release that the team at the Boston Area Research Initiative (BARI) provides. These census indicators are used to provide a picture into the changing landscape of Boston and Massachussettes more gnerally. In this chapter we will work through a rather real life scenario that you may very well encounter using the BARI data. 8.1 Scenario: A local non-profit is interested in the commuting behavior of Greater Boston residents. Your advisor suggested that you assist the non-profit in their work. You’ve just had coffee with the project manager to learn more about what their specific research question is. It seems that the extension of the Green Line is of great interest to them. They spoke at length about the history of the Big Dig and its impact on commuters working in the city. They look at their watch and realize they’re about to miss the commuter train home. You shake their hand, thank them for their time (and the free coffee because you’re a grad student), and affirm that you will email them in a week or so with some data for them to work with. https://www.mass.gov/green-line-extension-project-glx https://en.wikipedia.org/wiki/Big_Dig You’re back at your apartment, french press next your laptop, though not too close, notes open, and ready to begin. You review your notes and realize, while you now have a rather good understanding of what the Green Line Extensions is and the impact that the Big Dig had, you really have no idea what about commuting behavior in Greater Boston they are interested in. You realize you did not even confirm what constitutes the Greater Boston area. You push down the coffee grinds and pour your first cup of coffee. This will take at least two cups of coffee. The above scenario sounds like something out of a stress dream. This is scenario that I have found myself in many times and I am sure that you will find yourself in at one point as well. The more comfortable you get with data analysis and asking good questions, the more guided and directed you can make these seemingly vague objectives. At the end of this chapter, we will expound upon ways to prevent this in the future. 8.2 Getting physical with the data The data we used in both chapters one and two were curated from the annual census indicator release from BARI. This is the dataset from which acs_edu was created. We will use these data to provide relevant data relating to commuting in the Greater Boston Area. To do so, we will be using the tidyverse to read in and manipulate our data (as we did last chapter). Recall that we will load the tidyverse using library(tidyverse). Refresher: the tidyverse is a collection of packages used for data analysis. When you load the tidyverse it loads readr, ggplot2, and dplyr for us, among other packages. For now, though, these are the only relevant packages. Try it: Load the tidyverse Read in the file ACS_1317_TRACT.csv located in the data directory, store it in an object called acs_raw library(tidyverse) acs_raw &lt;- read_csv(&quot;data/ACS_1317_TRACT.csv&quot;) Wonderful! You’ve got the hang of reading data in which is truly no small feat. Once we have the data accessible from R, it is important to get familiar with what the data are. This means we need to know which variables are available to us and get a feel for what the values in those variables represent. Try printing out the acs_raw object in your console. acs_raw Oof, yikes. It’s a bit messy! Not to mention that R did not even print out all of the columns. That’s because it ran out of room. When we’re working with wide data (many columns), it’s generally best to view only a preview of the data. The function dplyr::glimpse() can help us do just that. Provide acs_raw as the only argument to glimpse(). glimpse(acs_raw) ## Observations: 1,478 ## Variables: 59 ## $ ct_id_10 &lt;dbl&gt; 25027728100, 25027729200, 25027730700, 2502… ## $ name &lt;chr&gt; &quot;Census Tract 7281, Worcester County, Massa… ## $ total_pop &lt;dbl&gt; 4585, 2165, 6917, 7278, 5059, 6632, 3259, 2… ## $ pop_den &lt;dbl&gt; 332.5741, 1069.6977, 2112.9526, 1345.5905, … ## $ sex_ratio &lt;dbl&gt; 1.1315667, 1.3179872, 1.1329016, 1.1156977,… ## $ age_u18 &lt;dbl&gt; 0.2340240, 0.1810624, 0.1705942, 0.2033526,… ## $ age1834 &lt;dbl&gt; 0.2023991, 0.1510393, 0.2143993, 0.2272602,… ## $ age3564 &lt;dbl&gt; 0.3980371, 0.4609700, 0.4371838, 0.4359714,… ## $ age_o65 &lt;dbl&gt; 0.1655398, 0.2069284, 0.1778228, 0.1334158,… ## $ for_born &lt;dbl&gt; 0.04383860, 0.08729792, 0.20586960, 0.15526… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779,… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.03… ## $ asian &lt;dbl&gt; 0.006324973, 0.021709007, 0.019950846, 0.03… ## $ hispanic &lt;dbl&gt; 0.070229008, 0.047575058, 0.136330779, 0.07… ## $ two_or_more &lt;dbl&gt; 0.012431843, 0.027713626, 0.017348562, 0.01… ## $ eth_het &lt;dbl&gt; -17032656, -3685242, -26905553, -36365806, … ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, … ## $ pub_assist &lt;dbl&gt; 0.020782396, 0.004667445, 0.021067926, 0.03… ## $ gini &lt;dbl&gt; 0.4084, 0.3886, 0.4693, 0.4052, 0.5327, 0.4… ## $ fam_pov_per &lt;dbl&gt; 0.04754601, 0.06521739, 0.05843440, 0.02486… ## $ unemp_rate &lt;dbl&gt; 0.03361345, 0.03127572, 0.06124498, 0.03983… ## $ total_house_h &lt;dbl&gt; 1636, 857, 2753, 2878, 2326, 2635, 1245, 80… ## $ fam_house_per &lt;dbl&gt; 0.7970660, 0.6977830, 0.6589175, 0.6567060,… ## $ fem_head_per &lt;dbl&gt; 0.08985330, 0.12018670, 0.11442063, 0.12126… ## $ same_sex_coup_per &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000… ## $ grand_head_per &lt;dbl&gt; 0.000000000, 0.005834306, 0.000000000, 0.00… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052,… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124… ## $ master &lt;dbl&gt; 0.15942502, 0.09705160, 0.12059807, 0.06714… ## $ prof &lt;dbl&gt; 0.0405096374, 0.0098280098, 0.0397403108, 0… ## $ doc &lt;dbl&gt; 0.033322444, 0.004914005, 0.033248082, 0.00… ## $ commute_less10 &lt;dbl&gt; 0.07874016, 0.25109361, 0.05369894, 0.14119… ## $ commute1030 &lt;dbl&gt; 0.5131234, 0.4636920, 0.6620965, 0.3941685,… ## $ commute3060 &lt;dbl&gt; 0.33114611, 0.18110236, 0.17709226, 0.31263… ## $ commute6090 &lt;dbl&gt; 0.055555556, 0.082239720, 0.086832334, 0.13… ## $ commute_over90 &lt;dbl&gt; 0.021434821, 0.021872266, 0.020279920, 0.01… ## $ by_auto &lt;dbl&gt; 0.9267791, 0.9704861, 0.9248285, 0.9037018,… ## $ by_pub_trans &lt;dbl&gt; 0.000000000, 0.004340278, 0.020301783, 0.01… ## $ by_bike &lt;dbl&gt; 0.002879473, 0.000000000, 0.000000000, 0.00… ## $ by_walk &lt;dbl&gt; 0.002468120, 0.012152778, 0.004938272, 0.02… ## $ total_house_units &lt;dbl&gt; 1701, 886, 2928, 3181, 2511, 2713, 1581, 85… ## $ vacant_unit_per &lt;dbl&gt; 0.03821282, 0.03273138, 0.05976776, 0.09525… ## $ renters_per &lt;dbl&gt; 0.1088020, 0.2100350, 0.2804214, 0.3425990,… ## $ home_own_per &lt;dbl&gt; 0.8911980, 0.7899650, 0.7195786, 0.6574010,… ## $ med_gross_rent &lt;dbl&gt; 1640, 894, 1454, 954, 1018, 867, 910, 1088,… ## $ med_home_val &lt;dbl&gt; 349000, 230200, 207200, 268400, 223200, 232… ## $ med_yr_built_raw &lt;dbl&gt; 1988, 1955, 1959, 1973, 1964, 1966, 1939, 1… ## $ med_yr_built &lt;chr&gt; &quot;1980 to 1989&quot;, &quot;1950 to 1959&quot;, &quot;1950 to 19… ## $ med_yr_moved_inraw &lt;dbl&gt; 2004, 2003, 2007, 2006, 2006, 2000, 2011, 2… ## $ med_yr_rent_moved_in &lt;dbl&gt; 2012, 2010, 2012, 2011, 2011, 2009, 2012, 2… ## $ area_acres &lt;dbl&gt; 9407.9167, 1294.2828, 2123.4927, 3581.9026,… ## $ town_id &lt;dbl&gt; 134, 321, 348, 185, 153, 151, 316, 348, 28,… ## $ town &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… ## $ fips_stco &lt;dbl&gt; 25027, 25027, 25027, 25027, 25027, 25027, 2… ## $ county &lt;chr&gt; &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WOR… ## $ area_acr_1 &lt;dbl&gt; 23241.514, 8867.508, 24609.965, 9615.644, 1… ## $ m_atown &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… Much better, right? It is frankly still a lot of text, but the way it is presented is rather useful. Each variable is written followed by its data type, i.e. &lt;dbl&gt;, and then a preview of values in that column. If the &lt;dbl&gt; does not make sense yet, do not worry. We will go over data types in depth later. Data types are not the most fun and I think it is important we have fun! acs_raw is the dataset from which acs_edu was created. As you can see, there are many, many, many different variables that the ACS data provide us with. These are only the tip of the iceberg. Pause Now have a think. Looking at the preview of these data, which columns do you think will be most useful to the non-profit for understanding commuter behavior? Note: “all of them” is not always the best answer. By providing too much data one may moved to inaction because they now determine what variables are the most useful and how to use them. HAVE SOURCES ON PARADOX OF CHOICE AND INFORMATION OVERLOAD. If you spotted the columns commute_less10, commute1030, commute3060, commute6090, and commute_over90, your eyes and intuition have served you well! These variables tell us about what proportion of the sampled population in a given census tract have commute times that fall within the indicated duration range, i.e. 30-60. PERSONAL NOTE: USE COMMUTE VARIABLES FOR PIVOTING AND THEN SEPARATING INTO NEW VARS 8.2.1 select()ing So now we have an intuition of the most important variables, but the next problem soon arises: how do we isolate just these variables? Whenever you find yourself needing to select or deselect columns from a tibble dplyr::select() will be the main function that you will go to. What does it do?: select() selects variables from a tibble and returns another tibble. Before we work through how to use select(), refer to the help documentation and see if you can get somewhat of an intuition by typing ?select() into the console. Once you press enter the documentation page should pop up in RStudio. There are a few reasons why I am directing you towards the function documentation. To get you comfortable with navigating the RStudio IDE Expose you to the R vocabulary Soon you’ll be too advanced for this book and will have to figure out the way functions work on your own! Perhaps the help documentation was a little overwhelming and absolutely confusing. That’s okay. It’s just an exposure! With each exposure things will make more sense. Let’s tackle these arguments one by one. .data: A tbl. All main verbs are S3 generics and provide methods for tbl_df(), dtplyr::tbl_dt() and dbplyr::tbl_dbi(). What I want you to take away from this argument definition is a tbl. Whenever you read tbl think to your self “oh, that is just a tibble.” If you recall, when we read rectangular data with readr::read_csv() or any other readr::read_*() function we will end up with a tibble. To verify that this is the case, we can double check our objects using the function is.tbl(). This function takes an object and returns a logical value (TRUE or FALSE) if the statement is true. Let’s double check that acs_raw is in fact a tbl. is.tbl(acs_raw) ## [1] TRUE Aside: Each object type usually has a function that let’s you test if objects are that type that follow the structure is.obj_type() or the occasional is_obj_type(). We will go over object types more later. We can read the above as if we are asking R the question “is this object a tbl?” The resultant output of is.tbl(acs_raw) is TRUE. Now we can be doubly confident that this object can be used with select(). The second argument to select() is a little bit more difficult to grasp, so don’t feel discouraged if this isn’t clicking right away. There is a lot written in this argument definition and I feel that not all of it is necessary to understand from the get go. ...: One or more unquoted expressions separated by commas. You can treat variable names like they are positions, so you can use expressions like x:y to select ranges of variables. ..., referred to as “dots” means that we can pass any number of arguments to the function. Translating “one or more unquoted expressions separated by commas” into regular person speak reiterates that there can be multiple other arguments passed into select(). “Unquoted expressions” means that if we want to select a column we do not put that column name in quotes. “You can treat variable names like they are positions” translates to “if you want the first column you can write the number 1 etc.” and because of this, if you want the first through tenth variable you can pass 1:10 as an argument to .... The most important thing about ... is that we do not assign ... as an argument, for example . ... = column_a is not the correct notation. We provide column_a alone. As always, this makes more sense once we see it in practice. We will now go over the many ways in which we can select columns using select(). Once we have gotten the hang of selecting columns we will return back to assisting our non-profit. We will go over: selecting by name selecting by position select helpers 8.2.2 select()ing exercises select() enables us to choose columns from a tibble based on their names. But remember that these will be unquoted column names. Try it: select the column name from acs_raw select(acs_raw, name) ## # A tibble: 1,478 x 1 ## name ## &lt;chr&gt; ## 1 Census Tract 7281, Worcester County, Massachusetts ## 2 Census Tract 7292, Worcester County, Massachusetts ## 3 Census Tract 7307, Worcester County, Massachusetts ## 4 Census Tract 7442, Worcester County, Massachusetts ## 5 Census Tract 7097.01, Worcester County, Massachusetts ## 6 Census Tract 7351, Worcester County, Massachusetts ## 7 Census Tract 7543, Worcester County, Massachusetts ## 8 Census Tract 7308.02, Worcester County, Massachusetts ## 9 Census Tract 7171, Worcester County, Massachusetts ## 10 Census Tract 7326, Worcester County, Massachusetts ## # … with 1,468 more rows The column name was passed to .... Recall that dots allows us to pass “one ore more unquoted expressions separated by commas.” To test this statement out, select town in addition to name from acs_raw Try it: select name and town from acs_raw select(acs_raw, name, town) ## # A tibble: 1,478 x 2 ## name town ## &lt;chr&gt; &lt;chr&gt; ## 1 Census Tract 7281, Worcester County, Massachusetts HOLDEN ## 2 Census Tract 7292, Worcester County, Massachusetts WEST BOYLSTON ## 3 Census Tract 7307, Worcester County, Massachusetts WORCESTER ## 4 Census Tract 7442, Worcester County, Massachusetts MILFORD ## 5 Census Tract 7097.01, Worcester County, Massachusetts LEOMINSTER ## 6 Census Tract 7351, Worcester County, Massachusetts LEICESTER ## 7 Census Tract 7543, Worcester County, Massachusetts WEBSTER ## 8 Census Tract 7308.02, Worcester County, Massachusetts WORCESTER ## 9 Census Tract 7171, Worcester County, Massachusetts BERLIN ## 10 Census Tract 7326, Worcester County, Massachusetts WORCESTER ## # … with 1,468 more rows Great, you’re getting the hang of it. Now, in addition to to selecting columns solely based on their names, we can also select a range of columns using the format col_from:col_to. In writing this select() will register that you want every column from and including col_from up until and including col_to. Let’s refresh ourselves with what our data look like: glimpse(acs_raw) ## Observations: 1,478 ## Variables: 59 ## $ ct_id_10 &lt;dbl&gt; 25027728100, 25027729200, 25027730700, 2502… ## $ name &lt;chr&gt; &quot;Census Tract 7281, Worcester County, Massa… ## $ total_pop &lt;dbl&gt; 4585, 2165, 6917, 7278, 5059, 6632, 3259, 2… ## $ pop_den &lt;dbl&gt; 332.5741, 1069.6977, 2112.9526, 1345.5905, … ## $ sex_ratio &lt;dbl&gt; 1.1315667, 1.3179872, 1.1329016, 1.1156977,… ## $ age_u18 &lt;dbl&gt; 0.2340240, 0.1810624, 0.1705942, 0.2033526,… ## $ age1834 &lt;dbl&gt; 0.2023991, 0.1510393, 0.2143993, 0.2272602,… ## $ age3564 &lt;dbl&gt; 0.3980371, 0.4609700, 0.4371838, 0.4359714,… ## $ age_o65 &lt;dbl&gt; 0.1655398, 0.2069284, 0.1778228, 0.1334158,… ## $ for_born &lt;dbl&gt; 0.04383860, 0.08729792, 0.20586960, 0.15526… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779,… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.03… ## $ asian &lt;dbl&gt; 0.006324973, 0.021709007, 0.019950846, 0.03… ## $ hispanic &lt;dbl&gt; 0.070229008, 0.047575058, 0.136330779, 0.07… ## $ two_or_more &lt;dbl&gt; 0.012431843, 0.027713626, 0.017348562, 0.01… ## $ eth_het &lt;dbl&gt; -17032656, -3685242, -26905553, -36365806, … ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, … ## $ pub_assist &lt;dbl&gt; 0.020782396, 0.004667445, 0.021067926, 0.03… ## $ gini &lt;dbl&gt; 0.4084, 0.3886, 0.4693, 0.4052, 0.5327, 0.4… ## $ fam_pov_per &lt;dbl&gt; 0.04754601, 0.06521739, 0.05843440, 0.02486… ## $ unemp_rate &lt;dbl&gt; 0.03361345, 0.03127572, 0.06124498, 0.03983… ## $ total_house_h &lt;dbl&gt; 1636, 857, 2753, 2878, 2326, 2635, 1245, 80… ## $ fam_house_per &lt;dbl&gt; 0.7970660, 0.6977830, 0.6589175, 0.6567060,… ## $ fem_head_per &lt;dbl&gt; 0.08985330, 0.12018670, 0.11442063, 0.12126… ## $ same_sex_coup_per &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000… ## $ grand_head_per &lt;dbl&gt; 0.000000000, 0.005834306, 0.000000000, 0.00… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052,… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124… ## $ master &lt;dbl&gt; 0.15942502, 0.09705160, 0.12059807, 0.06714… ## $ prof &lt;dbl&gt; 0.0405096374, 0.0098280098, 0.0397403108, 0… ## $ doc &lt;dbl&gt; 0.033322444, 0.004914005, 0.033248082, 0.00… ## $ commute_less10 &lt;dbl&gt; 0.07874016, 0.25109361, 0.05369894, 0.14119… ## $ commute1030 &lt;dbl&gt; 0.5131234, 0.4636920, 0.6620965, 0.3941685,… ## $ commute3060 &lt;dbl&gt; 0.33114611, 0.18110236, 0.17709226, 0.31263… ## $ commute6090 &lt;dbl&gt; 0.055555556, 0.082239720, 0.086832334, 0.13… ## $ commute_over90 &lt;dbl&gt; 0.021434821, 0.021872266, 0.020279920, 0.01… ## $ by_auto &lt;dbl&gt; 0.9267791, 0.9704861, 0.9248285, 0.9037018,… ## $ by_pub_trans &lt;dbl&gt; 0.000000000, 0.004340278, 0.020301783, 0.01… ## $ by_bike &lt;dbl&gt; 0.002879473, 0.000000000, 0.000000000, 0.00… ## $ by_walk &lt;dbl&gt; 0.002468120, 0.012152778, 0.004938272, 0.02… ## $ total_house_units &lt;dbl&gt; 1701, 886, 2928, 3181, 2511, 2713, 1581, 85… ## $ vacant_unit_per &lt;dbl&gt; 0.03821282, 0.03273138, 0.05976776, 0.09525… ## $ renters_per &lt;dbl&gt; 0.1088020, 0.2100350, 0.2804214, 0.3425990,… ## $ home_own_per &lt;dbl&gt; 0.8911980, 0.7899650, 0.7195786, 0.6574010,… ## $ med_gross_rent &lt;dbl&gt; 1640, 894, 1454, 954, 1018, 867, 910, 1088,… ## $ med_home_val &lt;dbl&gt; 349000, 230200, 207200, 268400, 223200, 232… ## $ med_yr_built_raw &lt;dbl&gt; 1988, 1955, 1959, 1973, 1964, 1966, 1939, 1… ## $ med_yr_built &lt;chr&gt; &quot;1980 to 1989&quot;, &quot;1950 to 1959&quot;, &quot;1950 to 19… ## $ med_yr_moved_inraw &lt;dbl&gt; 2004, 2003, 2007, 2006, 2006, 2000, 2011, 2… ## $ med_yr_rent_moved_in &lt;dbl&gt; 2012, 2010, 2012, 2011, 2011, 2009, 2012, 2… ## $ area_acres &lt;dbl&gt; 9407.9167, 1294.2828, 2123.4927, 3581.9026,… ## $ town_id &lt;dbl&gt; 134, 321, 348, 185, 153, 151, 316, 348, 28,… ## $ town &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… ## $ fips_stco &lt;dbl&gt; 25027, 25027, 25027, 25027, 25027, 25027, 2… ## $ county &lt;chr&gt; &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WOR… ## $ area_acr_1 &lt;dbl&gt; 23241.514, 8867.508, 24609.965, 9615.644, 1… ## $ m_atown &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… Try it: select the columns age_u18 through age_o65. select(acs_raw, age_u18:age_o65) ## # A tibble: 1,478 x 4 ## age_u18 age1834 age3564 age_o65 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.202 0.398 0.166 ## 2 0.181 0.151 0.461 0.207 ## 3 0.171 0.214 0.437 0.178 ## 4 0.203 0.227 0.436 0.133 ## 5 0.177 0.203 0.430 0.190 ## 6 0.163 0.237 0.439 0.162 ## 7 0.191 0.326 0.380 0.102 ## 8 0.202 0.183 0.466 0.148 ## 9 0.188 0.150 0.462 0.200 ## 10 0.244 0.286 0.342 0.128 ## # … with 1,468 more rows Now to really throw you off! You can even reverse the order of these ranges. Try it: select columns from age_o65 to age_u18. select(acs_raw, age_o65:age_u18) ## # A tibble: 1,478 x 4 ## age_o65 age3564 age1834 age_u18 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.166 0.398 0.202 0.234 ## 2 0.207 0.461 0.151 0.181 ## 3 0.178 0.437 0.214 0.171 ## 4 0.133 0.436 0.227 0.203 ## 5 0.190 0.430 0.203 0.177 ## 6 0.162 0.439 0.237 0.163 ## 7 0.102 0.380 0.326 0.191 ## 8 0.148 0.466 0.183 0.202 ## 9 0.200 0.462 0.150 0.188 ## 10 0.128 0.342 0.286 0.244 ## # … with 1,468 more rows NOTE: This is an awful scenario. SOMEONE HALP Alright, so now we have gotten the hang of selecting columns based on their names. But equally important is the ability to select columns based on their position. Consider the situation in which you regularly receive georeferenced data from a research partner and the structure of the dataset is rather consistent except that they frequently change the name of the coordinate columns. Sometimes the columns are x and y. Sometimes they are capitalized X and Y, lon and lat, or even long and lat. It eats you up inside! But you know that while the names may change, their positiions never do—they’re always the last two columns. You decide to program a solution rather than having a conversation with your research partner—though, I recommend you both level set on reproducibility standards. “…You can treat variable names like they are positions…” The above was taken from the argument definition of dots .... Like providing the name of the column, we can also provide their positions (also referred to as an index). In our previous example, we selected the name column. We can select this column by it’s position too. name is the second column in our tibble. We select it by position like so: select(acs_raw, 2) ## # A tibble: 1,478 x 1 ## name ## &lt;chr&gt; ## 1 Census Tract 7281, Worcester County, Massachusetts ## 2 Census Tract 7292, Worcester County, Massachusetts ## 3 Census Tract 7307, Worcester County, Massachusetts ## 4 Census Tract 7442, Worcester County, Massachusetts ## 5 Census Tract 7097.01, Worcester County, Massachusetts ## 6 Census Tract 7351, Worcester County, Massachusetts ## 7 Census Tract 7543, Worcester County, Massachusetts ## 8 Census Tract 7308.02, Worcester County, Massachusetts ## 9 Census Tract 7171, Worcester County, Massachusetts ## 10 Census Tract 7326, Worcester County, Massachusetts ## # … with 1,468 more rows Try it: select age_u18 and age_o65 by their position select(acs_raw, 6, 9) ## # A tibble: 1,478 x 2 ## age_u18 age_o65 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.166 ## 2 0.181 0.207 ## 3 0.171 0.178 ## 4 0.203 0.133 ## 5 0.177 0.190 ## 6 0.163 0.162 ## 7 0.191 0.102 ## 8 0.202 0.148 ## 9 0.188 0.200 ## 10 0.244 0.128 ## # … with 1,468 more rows You may see where I am going with this. Just like column names, we can select a range of columns using the same method index_from:index_to. Try it: select the columns from age_u18 to age_o65 using : and the column position select the columns in reverse order by their indexes select(acs_raw, 6:9) ## # A tibble: 1,478 x 4 ## age_u18 age1834 age3564 age_o65 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.202 0.398 0.166 ## 2 0.181 0.151 0.461 0.207 ## 3 0.171 0.214 0.437 0.178 ## 4 0.203 0.227 0.436 0.133 ## 5 0.177 0.203 0.430 0.190 ## 6 0.163 0.237 0.439 0.162 ## 7 0.191 0.326 0.380 0.102 ## 8 0.202 0.183 0.466 0.148 ## 9 0.188 0.150 0.462 0.200 ## 10 0.244 0.286 0.342 0.128 ## # … with 1,468 more rows select(acs_raw, 9:6) ## # A tibble: 1,478 x 4 ## age_o65 age3564 age1834 age_u18 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.166 0.398 0.202 0.234 ## 2 0.207 0.461 0.151 0.181 ## 3 0.178 0.437 0.214 0.171 ## 4 0.133 0.436 0.227 0.203 ## 5 0.190 0.430 0.203 0.177 ## 6 0.162 0.439 0.237 0.163 ## 7 0.102 0.380 0.326 0.191 ## 8 0.148 0.466 0.183 0.202 ## 9 0.200 0.462 0.150 0.188 ## 10 0.128 0.342 0.286 0.244 ## # … with 1,468 more rows Base R Side Bar: To help build your intuition, I want to point out some base R functionality. Using the colon : with integers (whole numbers) is actually not a select() specific functionality. This is something that is rather handy and built directly into R. Using the colon operator, we can create ranges of numbers in the same exact way as we did above. If we want create the range of numbers from 1 to 10, we write 1:10. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. In our scenario, we want to select the last two columns. We may not know their names or their position. Luckily, there’s a function for that. last_col() is a handy function that enables us to select the last column. There is also an option to get an offset from the last column. An offset would allow us to grab the second to last column by setting the offset to 1. By setting the offset, last_col() will from the offset + 1 from the last column. So if the offset is set to 1, we would be grabbing the second to last column. Let’s give it a shot: select(acs_raw, last_col()) ## # A tibble: 1,478 x 1 ## m_atown ## &lt;chr&gt; ## 1 HOLDEN ## 2 WEST BOYLSTON ## 3 WORCESTER ## 4 MILFORD ## 5 LEOMINSTER ## 6 LEICESTER ## 7 WEBSTER ## 8 WORCESTER ## 9 BERLIN ## 10 WORCESTER ## # … with 1,468 more rows select(acs_raw, last_col(offset = 1)) ## # A tibble: 1,478 x 1 ## area_acr_1 ## &lt;dbl&gt; ## 1 23242. ## 2 8868. ## 3 24610. ## 4 9616. ## 5 18993. ## 6 15763. ## 7 9347. ## 8 24610. ## 9 8431. ## 10 24610. ## # … with 1,468 more rows select(acs_raw, last_col(offset = 1):last_col()) ## # A tibble: 1,478 x 2 ## area_acr_1 m_atown ## &lt;dbl&gt; &lt;chr&gt; ## 1 23242. HOLDEN ## 2 8868. WEST BOYLSTON ## 3 24610. WORCESTER ## 4 9616. MILFORD ## 5 18993. LEOMINSTER ## 6 15763. LEICESTER ## 7 9347. WEBSTER ## 8 24610. WORCESTER ## 9 8431. BERLIN ## 10 24610. WORCESTER ## # … with 1,468 more rows last_col() comes from another packages called tidyselect which is imported with dplyr. This package contains a number of helper functions. There are 9 total helpers and you’ve already learned one of them. We will briefly review four more of these. I’m sure you are able to deduce how the functions work solely based on their names. The functions are: starts_with(): a string to search that columns start with ends_with(): a string to search that columns end with contains(): a string to search for in the column names at any position everything(): selects the remaining columns Each of these function take a character string and searches the column headers for them. Try it out: find all columns that start with &quot;med&quot; select(acs_raw, starts_with(&quot;med&quot;)) ## # A tibble: 1,478 x 7 ## med_house_income med_gross_rent med_home_val med_yr_built_raw ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 1640 349000 1988 ## 2 69625 894 230200 1955 ## 3 70679 1454 207200 1959 ## 4 74528 954 268400 1973 ## 5 52885 1018 223200 1964 ## 6 64100 867 232700 1966 ## 7 37093 910 170900 1939 ## 8 87750 1088 270100 1939 ## 9 97417 1037 379600 1981 ## 10 43384 1017 156500 1939 ## # … with 1,468 more rows, and 3 more variables: med_yr_built &lt;chr&gt;, ## # med_yr_moved_inraw &lt;dbl&gt;, med_yr_rent_moved_in &lt;dbl&gt; select columns that end with &quot;per&quot; select(acs_raw, ends_with(&quot;per&quot;)) ## # A tibble: 1,478 x 8 ## fam_pov_per fam_house_per fem_head_per same_sex_coup_p… grand_head_per ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0475 0.797 0.0899 0 0 ## 2 0.0652 0.698 0.120 0 0.00583 ## 3 0.0584 0.659 0.114 0 0 ## 4 0.0249 0.657 0.121 0 0 ## 5 0.198 0.531 0.158 0 0.00946 ## 6 0.0428 0.665 0.0603 0 0.0353 ## 7 0.0762 0.632 0.227 0 0.00643 ## 8 0.101 0.636 0.0582 0.297 0.0260 ## 9 0.0149 0.758 0.0721 0 0.00434 ## 10 0.0954 0.460 0.225 0 0.0279 ## # … with 1,468 more rows, and 3 more variables: vacant_unit_per &lt;dbl&gt;, ## # renters_per &lt;dbl&gt;, home_own_per &lt;dbl&gt; find any column that contains the string &quot;yr&quot; select(acs_raw, contains(&quot;yr&quot;)) ## # A tibble: 1,478 x 4 ## med_yr_built_raw med_yr_built med_yr_moved_inraw med_yr_rent_moved_in ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1988 1980 to 1989 2004 2012 ## 2 1955 1950 to 1959 2003 2010 ## 3 1959 1950 to 1959 2007 2012 ## 4 1973 1970 to 1979 2006 2011 ## 5 1964 1960 to 1969 2006 2011 ## 6 1966 1960 to 1969 2000 2009 ## 7 1939 Prior to 1940 2011 2012 ## 8 1939 Prior to 1940 2006 2012 ## 9 1981 1980 to 1989 2004 2012 ## 10 1939 Prior to 1940 2011 NA ## # … with 1,468 more rows select columns that start with med then select everything else select(acs_raw, contains(&quot;yr&quot;), everything()) ## # A tibble: 1,478 x 59 ## med_yr_built_raw med_yr_built med_yr_moved_in… med_yr_rent_mov… ct_id_10 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1988 1980 to 1989 2004 2012 2.50e10 ## 2 1955 1950 to 1959 2003 2010 2.50e10 ## 3 1959 1950 to 1959 2007 2012 2.50e10 ## 4 1973 1970 to 1979 2006 2011 2.50e10 ## 5 1964 1960 to 1969 2006 2011 2.50e10 ## 6 1966 1960 to 1969 2000 2009 2.50e10 ## 7 1939 Prior to 19… 2011 2012 2.50e10 ## 8 1939 Prior to 19… 2006 2012 2.50e10 ## 9 1981 1980 to 1989 2004 2012 2.50e10 ## 10 1939 Prior to 19… 2011 NA 2.50e10 ## # … with 1,468 more rows, and 54 more variables: name &lt;chr&gt;, ## # total_pop &lt;dbl&gt;, pop_den &lt;dbl&gt;, sex_ratio &lt;dbl&gt;, age_u18 &lt;dbl&gt;, ## # age1834 &lt;dbl&gt;, age3564 &lt;dbl&gt;, age_o65 &lt;dbl&gt;, for_born &lt;dbl&gt;, ## # white &lt;dbl&gt;, black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, ## # two_or_more &lt;dbl&gt;, eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, ## # pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, ## # total_house_h &lt;dbl&gt;, fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, ## # same_sex_coup_per &lt;dbl&gt;, grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, ## # hs_grad &lt;dbl&gt;, some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, ## # doc &lt;dbl&gt;, commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, town &lt;chr&gt;, fips_stco &lt;dbl&gt;, ## # county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, m_atown &lt;chr&gt; 8.3 Selecting Rows Though a somewhat infrequent event, it will be handy to know how to select rows. There are two ways in which we can select our rows. The first is by specifying exactly which rows by their position. The other way is to filter down our data based on a condition—i.e. median household income within a range. The functions to do this are slice() and filter() respectively. The remainder of this chapter will introduce you to slice(). We will learn how to filter in the next chapter. Like select() we can also select rows. But rows do not have names, so we must select the rows based on their position. Given your familiarity with selecting by column position this should be a cake walk for you. Similar to last_col() we have the function n(). n() is a rather handy little function which tells us how many observations there are in a tibble. This allows to specify the last row of a tibble. slice(acs_raw, n()) ## # A tibble: 1 x 59 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 5821 2760. 0.885 0.181 0.204 0.435 ## # … with 51 more variables: age_o65 &lt;dbl&gt;, for_born &lt;dbl&gt;, white &lt;dbl&gt;, ## # black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, two_or_more &lt;dbl&gt;, ## # eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, ## # fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, total_house_h &lt;dbl&gt;, ## # fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, same_sex_coup_per &lt;dbl&gt;, ## # grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, hs_grad &lt;dbl&gt;, ## # some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, doc &lt;dbl&gt;, ## # commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt; Unlike last_col(), n() provides us with a number. Instead of specifying an offset we can instead subtract directly from the output of n(). To grab the last three rows we can write (n() - 3):n(). We put n()-3 inside of parantheses so R knows to process n() - 3 first. slice(acs_raw, (n() - 3):n()) ## # A tibble: 4 x 59 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 2519 3083. 0.806 0.202 0.268 0.397 ## 2 2.50e10 Cens… 3500 5392. 1.05 0.205 0.277 0.395 ## 3 2.50e10 Cens… 5816 2677. 1.20 0.191 0.233 0.458 ## 4 2.50e10 Cens… 5821 2760. 0.885 0.181 0.204 0.435 ## # … with 51 more variables: age_o65 &lt;dbl&gt;, for_born &lt;dbl&gt;, white &lt;dbl&gt;, ## # black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, two_or_more &lt;dbl&gt;, ## # eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, ## # fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, total_house_h &lt;dbl&gt;, ## # fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, same_sex_coup_per &lt;dbl&gt;, ## # grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, hs_grad &lt;dbl&gt;, ## # some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, doc &lt;dbl&gt;, ## # commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt; Try it: select the first row, rows 100-105, and the last row slice(acs_raw, 1, 100:105, n()) ## # A tibble: 8 x 59 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 4585 333. 1.13 0.234 0.202 0.398 ## 2 2.50e10 Cens… 5223 2402. 1.21 0.183 0.171 0.450 ## 3 2.50e10 Cens… 5586 592. 1.09 0.278 0.116 0.413 ## 4 2.50e10 Cens… 4474 1119. 0.962 0.282 0.0847 0.427 ## 5 2.50e10 Cens… 6713 674. 0.928 0.223 0.216 0.423 ## 6 2.50e10 Cens… 6676 3541. 0.999 0.249 0.266 0.395 ## 7 2.50e10 Cens… 8141 820. 1.25 0.258 0.169 0.410 ## 8 2.50e10 Cens… 5821 2760. 0.885 0.181 0.204 0.435 ## # … with 51 more variables: age_o65 &lt;dbl&gt;, for_born &lt;dbl&gt;, white &lt;dbl&gt;, ## # black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, two_or_more &lt;dbl&gt;, ## # eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, ## # fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, total_house_h &lt;dbl&gt;, ## # fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, same_sex_coup_per &lt;dbl&gt;, ## # grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, hs_grad &lt;dbl&gt;, ## # some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, doc &lt;dbl&gt;, ## # commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt; 8.4 Revisiting commmuting We’ve just spent a fair amount of time learning how to work with our data. It’s now time to return to the problem at hand. We still haven’t addressed what data will be of use to our partner at the non-profit. While urban informatics is largely technical, it is still mostly intellectual. We have to think through problems and be methodical with our data selection and curation. We have to think about what our data tells us and why it is important. During these exercises, I hope you were looking at the data and thinking about what may be helpful to the non-profit. Again, the goal is to provide them with what is useful, but not more than they need. 8.4.1 Exercise It is now incumbent upon you to curate the data BARI Census Indicator dataset for the non-profit. Refamiliarize yourself with the data. Select a subset of columns that you believe will provide the best insight into commuting behavior in the Greater Boston Area. When making decisions like this, I like to think of a quote from The Master of Disguise: “Answer these questions for yourself: who? Why? Where? How?” Save the resultant tibble to an object named commute or something else informative. Below is one approach to this question. For this, I have selected all columns pertaining to commute time (columns that start with commute), the method by which people commute (begin with by), meidan household income, and the name of the census tract. The name of the census tract will be helpful for identifying “where”. commute &lt;- select(acs_raw, name, starts_with(&quot;commute&quot;), starts_with(&quot;by&quot;), med_house_income) 8.5 Asking good questions 8.6 filter library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ──────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() acs_raw &lt;- read_csv(&quot;data/ACS_1317_TRACT.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## name = col_character(), ## med_yr_built = col_character(), ## town = col_character(), ## county = col_character(), ## m_atown = col_character() ## ) ## See spec(...) for full column specifications. logical operators 8.6.1 scenario we have now selected the columns we need to provide 8.7 mutate 8.7.1 scenario The non-profit has emailed you back and indicated that they want to report on the income quintiles and requested that you do this for them. You’re a rockstar and a kickass programmer so you’re like, hell yah. they also ask for: tract a combined measure of bach masters and doctoral acs_raw %&gt;% mutate(hh_inc_quin = ntile(med_house_income, 5)) ## # A tibble: 1,478 x 60 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 4585 333. 1.13 0.234 0.202 0.398 ## 2 2.50e10 Cens… 2165 1070. 1.32 0.181 0.151 0.461 ## 3 2.50e10 Cens… 6917 2113. 1.13 0.171 0.214 0.437 ## 4 2.50e10 Cens… 7278 1346. 1.12 0.203 0.227 0.436 ## 5 2.50e10 Cens… 5059 2894. 1.30 0.177 0.203 0.430 ## 6 2.50e10 Cens… 6632 472. 1.11 0.163 0.237 0.439 ## 7 2.50e10 Cens… 3259 8022. 1.25 0.191 0.326 0.380 ## 8 2.50e10 Cens… 2097 5191. 0.908 0.202 0.183 0.466 ## 9 2.50e10 Cens… 3098 239. 0.990 0.188 0.150 0.462 ## 10 2.50e10 Cens… 3982 17065. 1.19 0.244 0.286 0.342 ## # … with 1,468 more rows, and 52 more variables: age_o65 &lt;dbl&gt;, ## # for_born &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, ## # two_or_more &lt;dbl&gt;, eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, ## # pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, ## # total_house_h &lt;dbl&gt;, fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, ## # same_sex_coup_per &lt;dbl&gt;, grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, ## # hs_grad &lt;dbl&gt;, some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, ## # doc &lt;dbl&gt;, commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt;, hh_inc_quin &lt;int&gt; i need to introduce measures of central tendency and summar stats in between this groups summarizing using the name column I can consider introducing string methods - this would probably be better done with text reviews or something 8.8 writing data lol They define the greater boston area as Suffolk, Middlesex, and Norfolk counties. before we go ahead and start cleaning this data, we need to learn the tools to do so. Please bear with me! recall how to load the tidyverse we’ll read in the ACS_1317_TRACT.csv file located in the data directory putting this together the file path is data/ACS_1317_TRACT.csv. store it in the variable acs_raw To learn these tools we will work with a role-play / workthrough. a local non-profit is interested in learning about the demographic characteristics of the greater boston area. They are specifically interested to learn more about the relationship between the age, race, and economic status. They’ve come to you to provide them with the relevant data. you have acces to the annual BARI census data and you will curate the data for them. "],
["visualizing-trends-and-relationships.html", "Chapter 9 Visualizing Trends and Relationships 9.1 Univariate visualizations 9.2 Bivariate visualizations 9.3 Expanding bivariate visualizations to trivariate &amp; other tri-variate 9.4 The Grammar of Layered Graphics", " Chapter 9 Visualizing Trends and Relationships library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ──────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() acs_messy &lt;- read_csv(&quot;data/ACS_1317_TRACT.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## name = col_character(), ## med_yr_built = col_character(), ## town = col_character(), ## county = col_character(), ## m_atown = col_character() ## ) ## See spec(...) for full column specifications. acs &lt;- acs_messy %&gt;% separate(name, sep = &quot;, &quot;, into = c(&quot;tract&quot;, &quot;county&quot;, &quot;state&quot;)) %&gt;% mutate(tract = str_remove(tract, &quot;Census Tract &quot;)) %&gt;% na.omit() most data analyses start with a visualization. the data we have will dictate the type of visualizations we create there are many many different ways in which data can be represented generally these can be bucketed into a few major categories numeric integer double character think groups, factors, nominal, anything that doesn’t have a numeric value that makes sense to count, aggregate, etc. time / order 9.1 Univariate visualizations what are we looking for in univariate visualizations? the shape of the distribution measures of central tendency where do the data cluster? is there a center? more than one? how much variation is in the data? is the distribution flatter or steeper? 9.1.1 histogram puts data into n groups or bins or buckets. ggplot calls them bins you can identify how many obs fit into a bucket ggplot(acs, aes(age_u18)) + geom_histogram(bins = 15) 9.1.2 density plot A density plot is a representation of the distribution of a numeric variable visualizes the distribution of data over a continuous interval it’s called a density plot because it uses a kernel density. you do not need to know what this is, just that it shows a continuous representation of the distribution unlike histograms you cannot determine how many obs fall in a bucket as there are no buckets. ggplot(acs, aes(age_u18)) + geom_density() 9.1.3 box plot box plots are another way to show the distribution unlike histograms and density plots which show the shape of the distribution box plots are concerned with illustrating any potential outliers aside: an outlier is a value that differs substantially from other obs based on 5 summary values: the first quartile median (the middle value) the third quartile the minimum and the maximum: these aren’t actually the max and min, these are we have to set this aesthetic to y refs: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 ggplot(acs, aes(y = age_u18)) + geom_boxplot() its easier to evaluate a box plot when it’s horizontal. we can flip any ggplot with a coord_flip() layer ggplot(acs, aes(y = age_u18)) + geom_boxplot() + coord_flip() 9.1.3.1 understanding the box plot 9.2 Bivariate visualizations with bi-variate relationships we’re looking to answer, in general, if one variable affects the other. we usually will be comparing two numeric variables or one numeric and one categorical variable in the former situtation we’re looking to see if there is a related trend, i.e. when one goes up does the other go down or vice versa in the latter scenario, we want to know if the distribution of the data changes for different groups 9.2.1 scatter plot (2 continuous) we made one previously this is two continuous on each axis. the variable of interest is on the y example: we can hypothesize that where there are more under 18 there are more families we can ask how does the prop of population under 18 vary with prop of family households? when there are many points (which is often the case w/ big data) we can change the transparency (often called opacity) so we can see where there is the most overlap. Inside of the geom_point() we set the alpha argument to a value between 0 and 1 where 1 is not transparent and 0 is invisible. this is artistic preference and there is no one true answer ggplot(acs, aes(fam_house_per, age_u18)) + geom_point(alpha = 0.25) 9.2.2 boxplot (1 continuous 1 categorical) we can use boxplots to compare groups for this, we set the categorical variable to the x aesthetic ggplot(acs, aes(county, age_u18)) + geom_boxplot() + coord_flip() 9.2.3 barplot (1 categorical 1 continuous / discrete) the geom_bar() will count the number observations of the specified categorical variables ggplot(acs, aes(county)) + geom_bar() + coord_flip() 9.2.4 lollipop chart barplot’s more fun cousin, the lollipop chart the package ggalt makes a geom for us so we don’t have to create it manually we do, however, have to count the observations ourself. we use the function count() to do this. the arguments are x, or the tibble, and ... these are the columns we want to count important note: in the tidyverse, the first argument is almost always the tibble we are working with this will become very useful at a later point remember, to install packages, navigate to the console and use the function install.packages(&quot;pkg-name&quot;). library(ggalt) ## Registered S3 methods overwritten by &#39;ggalt&#39;: ## method from ## grid.draw.absoluteGrob ggplot2 ## grobHeight.absoluteGrob ggplot2 ## grobWidth.absoluteGrob ggplot2 ## grobX.absoluteGrob ggplot2 ## grobY.absoluteGrob ggplot2 acs_counties &lt;- count(acs, county) ggplot(acs_counties, aes(county, n)) + geom_lollipop() + coord_flip() ridgelines (1 continuous 1 categorical) line chart (1 continuous 1 time), this is a unique case 9.3 Expanding bivariate visualizations to trivariate &amp; other tri-variate we can visualize other vcariables by setting further aesthetics. can set the color or fill, size, and shape we alreay did this previously when we set the color, let’s do that here. lets see how commuting by walking changes with the family house and under 18 pop set the color argument of the aes() function as color = by_walk it’s important you do this within the aesthetics function ggplot(acs, aes(fam_house_per, age_u18, color = by_auto)) + geom_point() we can add size to this as well by setting the size aesthetic lets see if the more female headed house holds there are affects commuting by car as minors increases ggplot(acs, aes(fam_house_per, age_u18, color = by_auto, size = fem_head_per)) + geom_point(alpha = .2) from this chart we can see quite a few things: as fam_house_per increases so does the under 18 pop, as both age_u18 and fam_house_per increase so does the rate of communiting by car as both age_u18 and fam_house_per so does female headed houses, but to a lesser degree this gives us a good idea of some relationships that we can test with our data at a later point minors_lm &lt;- lm(age_u18 ~ fam_house_per + by_auto + fem_head_per, data = acs) huxtable::huxreg(minors_lm) ## Registered S3 methods overwritten by &#39;broom.mixed&#39;: ## method from ## augment.lme broom ## augment.merMod broom ## glance.lme broom ## glance.merMod broom ## glance.stanreg broom ## tidy.brmsfit broom ## tidy.gamlss broom ## tidy.lme broom ## tidy.merMod broom ## tidy.rjags broom ## tidy.stanfit broom ## tidy.stanreg broom (1) (Intercept) 0.000&nbsp;&nbsp;&nbsp;&nbsp; (0.006)&nbsp;&nbsp;&nbsp; fam_house_per 0.245 *** (0.009)&nbsp;&nbsp;&nbsp; by_auto 0.016 *&nbsp;&nbsp; (0.007)&nbsp;&nbsp;&nbsp; fem_head_per 0.257 *** (0.012)&nbsp;&nbsp;&nbsp; N 1311&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R2 0.564&nbsp;&nbsp;&nbsp;&nbsp; logLik 2444.648&nbsp;&nbsp;&nbsp;&nbsp; AIC -4879.296&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Trivariate: grouped / stacked bar charts heatmaps 9.4 The Grammar of Layered Graphics recommended reading: A Layered Grammar of Graphics in R we will use a package called ggplot2 to do the visualizaiton of our data the gg in ggplot stands for “grammar of graphics”. once we can internalize the grammar, creating plots becomes rather easy we specify our aesthetics we add layers (hence the plus sign). these take values from the specified aesthetics can add multiple layers add aesthetics other than x and y. helps us visualize more dimensions of the data. we can use shape, color, and size 9.4.1 revisiting the cartesian plane x and y coordinates generally two numeric values on the x and y. think of the standards scatterplot (below) we also can place groups on one axis i.e. barchart (below) the y is usually the variable of interest as we move along the x axis (to the right) we can see how the y changes in response "],
["data-structures.html", "Chapter 10 Data Structures 10.1 Atomic Vectors 10.2 Data Frames 10.3 Lists", " Chapter 10 Data Structures There is a topic I have been skirting around for some time now and I think it is time that we have to have a rather important conversation. It’s one that is almost never fun but is quite necessary because without it, there may be many painful lessons learned in the future. We’re going to spend this next chapter talking about data structures—but not all of them! We’ll only cover the three most common and, by the end of this, it is my hope you will have a much stronger idea of what you are working with and why it behaves the way it does. We will cover vectors, data frames rather briefly, and lists. We’ll talk about some of their defining characteristics and how we can interact with them. Often the theory behind these object types are ommitted, but I am of the mind that learning this early on will pay off in dividends. Take a deep breath before we dive in and remind yourself that it ain’t nothin’ but a thing. This section is undoubtedly the most theoretically dense from a software perspective of this entire book. These concepts may be a little bit difficult to grasp at the frist go around particularly if you do not have a programming background. But do not be discouraged! This is tough and there is no way to around it. If you can grasp this chapter programming in R will become so much easier. You will develop an intuition of why certain things happen to your data and how to interact with other data structures. 10.1 Atomic Vectors I like to think of the atomic vector as the building block of any R object. You’ve actually been working with atomic vectors this entire time. But we haven’t been very explicit about this yet. Up until this point we have been working mainly with tibbles. Each column of a tibble is actually an atomic vector. What makes a vector a atomic is that it can only be a single data type and that they are one-dimensional—opposed to tibbles which are two-dimensional (https://swcarpentry.github.io/r-novice-inflammation/13-supp-data-structures/). You may have noticed that every value of a column is of the same data type. This means that they are rather strict to work with and for good reason. Imagine you wanted to multiple a column by 10, what would happen if a few of the values in the column were actually written out as text? Let’s try exploring this idea. The most common way to create a vector in R is to use the c() function. This stands for combine. We can combine as many elements as we want into a single vector using c(). Each element of the vector is it’s own argument (separated by a comma). For example if we wanted to create a vector of Boston’s unemployment rate rate for each month in 2019 that we have data for (until October as of this writing on 2019-12-18) we could write the below. We will save it in a a vector called unemp. unemp &lt;- c(3.2, 2.8, 2.8, 2.4, 2.8, 2.9, 2.7, 2.6, 2.7, 2.3) unemp ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 What is really great about vectors is that we can perform any number of operations on them—i.e. find the sum of all the values, the average, add a value to each element, etc. If we wanted to find the average unemployment rate for Boston for Jan - Oct. 2019, we can supply the vector to the function mean(). mean(unemp) ## [1] 2.72 However, you may be thinking “there are 12 months in a year not 10 and that should be represented” and if you are, I totally agree with you. Since the data for November and December are missing, we should denote that and update unemp accordingly. R uses NA to represent missing data. To represent this we can append two NAs to the vector we have. There are two ways we can do this. We can either combine unemp with two NAs, or rewrite the above vector. # combining existing with 2 NAs c(unemp, NA, NA) ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA This works, but since we will be saving this to unemp again it is not best practices to use the variable you are changing in that objects assignment. # for example unemp &lt;- c(unemp, NA, NA) The above is rather unclear and might confuse someone that will have to read your code at a later time—that person may even be you. For this reason we will redefine it. unemp &lt;- c(3.2, 2.8, 2.8, 2.4, 2.8, 2.9, 2.7, 2.6, 2.7, 2.3, NA, NA) unemp ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA We know that there are 12 elements in this vector, but sometimes it is quite nice to sanity check oneself. We can always find out how long (or how many elements are in) a vector is by supplying the vector to the length() function. # how many observations are in `unemp`? length(unemp) ## [1] 12 There are a total of six types of vectors. Fortunately, only four of these really matter to us. These are integer, double, character, logical. Integers represent whole numbers. To specify an integer we append an L after the number such as 20L. Doubles are any number that requires any precision aka decimal places. You can specify doubles in a number of formats such as scientific notation. Generally the easiest way to do this, though, is using a decimal. Together integers and doubles are lumped into the category of numeric. This is because, well, they are numbers. As you learned previously, character vectors are created with the use of quotation marks; either &quot; or '. We’ve already created a vector of type double, unemp. You can check what type of vector unemp is with typeof() Note: typeof() is used only for internal R object such as lists and vectors. In most cases you will want to use class() to return the class of an object. typeof(unemp) ## [1] &quot;double&quot; Say we create another vector called month with the numbers 1 through 12. month &lt;- c(1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12) typeof(month) ## [1] &quot;double&quot; Notice that since we didn’t specify the L after the numbers R defaulted to treating month as a double. When possible it is good to make the distinction between integer and numeric. month &lt;- c(1L, 2L, 3L, 4L, 5L, 6L, 7L, 8L, 9L, 10L, 11L, 12L) typeof(month) ## [1] &quot;integer&quot; R has a number of vectors that are built in these being the letters of the alphabet (letters and LETTERS respectively), as well as month.abb, month.name, and pi. month.name is already available to us so let’s not recreate it. month.name ## [1] &quot;January&quot; &quot;February&quot; &quot;March&quot; &quot;April&quot; &quot;May&quot; ## [6] &quot;June&quot; &quot;July&quot; &quot;August&quot; &quot;September&quot; &quot;October&quot; ## [11] &quot;November&quot; &quot;December&quot; typeof(month.name) ## [1] &quot;character&quot; Notice the quotes around each vector element. This is how we identify character vectors. Logical vectors are the last kind of vector we need to go over. Logical vectors are represented as the values TRUE and FALSE. Simple enough. Onward! Recall that vectors are atomic meaning that there can only be one type per vector and we cannot mix and match. When a character is in the presence of another element of a different type, that value is coerced into a character. Coersion is the process of implicitly or contextually changing an object from one type to another. For example: x &lt;- c(&quot;a&quot;, 1) typeof(x) ## [1] &quot;character&quot; Something similar happens when a logical value is in the presence of a numeric value c(TRUE, 1, FALSE) ## [1] 1 1 0 In the presence of a numeric value TRUE becomes equal to 1L and FALSE equal to 0L.This behavior exists whenever a logical value is presented where a numeric is expected such as the function call below. sum(TRUE, FALSE, FALSE) ## [1] 1 While coersion occurs from other processes like combining values in a vector, casting is the process of intentionally changing an object’s class. There are a number of casting functions whice generaly take the shape of as.class() or as_class(). Each of the vector types covered have their own casting functions. as.integer(TRUE) ## [1] 1 as.character(123) ## [1] &quot;123&quot; as.double(&quot;2.331&quot;) ## [1] 2.331 as.logical(0) ## [1] FALSE As you progress in your R journey you will find scenarios in which you need to cast objects from one class to another and these functions are the trick. You now have a strong understanding of the underbellies of R vectors. One thing that is missing is an understanding of how we can select subsets from vectors. To extract a value from vectors we append square brackets at the end of the vector vec[]. We supply an index value to the square brackets to receive the value at that position To select the month of January from the unemp vector, the first element, we provide the value of 1 to the brackets. unemp[1] ## [1] 3.2 To extract more than one value, we provide a vector of the row indexes we desire. unemp[c(1, 3)] ## [1] 3.2 2.8 There is yet another way to extract values from these vectors. We can provide a logical vector to our square brackets. For example, we can identify every value of unemp that is above the average rate. # find average removing missing values avg_unemp &lt;- mean(unemp, na.rm = TRUE) # identify which values are above average index &lt;- unemp &gt; avg_unemp index ## [1] TRUE TRUE TRUE FALSE TRUE TRUE FALSE FALSE FALSE FALSE NA ## [12] NA Notice that the NAs stayed NA? They can be pesky. Hadley writes in Advanced R “missing values tend to be infectious: most computations involving a missing value will return another missing value.” (https://adv-r.hadley.nz/vectors-chap.html) unemp[index] ## [1] 3.2 2.8 2.8 2.8 2.9 NA NA How annoying those NAs can be! To prevent these NAs from showing upwe can add another condition to our index line to remove NAs. Like there are as.*() functions for casting, there are also is.*() functions for testing. is.*() returns a logical vector of the same length as the provided vector. Note: the * is called a wildcard. The wildcard character comes from SQL and when present means that any string can follow. is.*() is intended to indicate any possible testing function such as is.numeric(), is_tibble(), etc. is.na(unemp) ## [1] FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE FALSE TRUE ## [12] TRUE As we learned, we can negate logical vectors with an !. We can negate the test results and include an an &amp; condition to only identify unemployment values above average and aren’t missing. index &lt;- unemp &gt; avg_unemp &amp; !is.na(unemp) unemp[index] ## [1] 3.2 2.8 2.8 2.8 2.9 There is one last thing to keep in mind and with subsetting vectors using a logical vector that is of a different length. When you use a logical vector to subset and they are of differing length, the logical vector will be recycled for the remaining values of the vector being subset. As always, an example will be the best. Say we have an object called x which are the values from 0 to 10 and an index to subset with. If we subset it with index and index is a logical vector of length two with the values of TRUE and FALSE, every other observation will be returned. This is because come the third value in x, R has ran out of values in index to use so it goes back to the beginning x &lt;- 0:10 x ## [1] 0 1 2 3 4 5 6 7 8 9 10 index &lt;- c(TRUE, FALSE) x[index] ## [1] 0 2 4 6 8 10 And what happens when the only value is a single logical value? x[TRUE] ## [1] 0 1 2 3 4 5 6 7 8 9 10 x[FALSE] ## integer(0) In this latter case see how the output says integer(0). This is informing you that the vector contains 0 elements. 10.2 Data Frames The entirety of the work in this book so far has been with tibbles. Tibbles are actually a special type of data frame. Data frames are R’s native way for storing rectangular data. Rectangles are two-dimensional, so are data frames. Data frames are secretly just a bunch of vectors squished together. The important thing is that all vectors are of the same length. This ensures that each observation (row) has one value from each vector. Because of the nature of a data frame, each column must adhere to the rules of vectors. Let’s create a tibble using the unemp vector and the tibble() function. tibble() works in a somewhat similar manner as mutate() where the arguments we provide are name value pairs. In the case of tibble, the argument take the form of col_name = vector. We create a tibble with the unemployment rate below. library(dplyr) tibble( unemp_rate = unemp ) ## # A tibble: 12 x 1 ## unemp_rate ## &lt;dbl&gt; ## 1 3.2 ## 2 2.8 ## 3 2.8 ## 4 2.4 ## 5 2.8 ## 6 2.9 ## 7 2.7 ## 8 2.6 ## 9 2.7 ## 10 2.3 ## 11 NA ## 12 NA We can add the month name and create a new column to indicate if that month has a higher than average unemployment rate. unemp_tbl &lt;- tibble( unemp_rate = unemp, month = month.name ) %&gt;% mutate(above_avg = unemp_rate &gt; avg_unemp) unemp_tbl ## # A tibble: 12 x 3 ## unemp_rate month above_avg ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 3.2 January TRUE ## 2 2.8 February TRUE ## 3 2.8 March TRUE ## 4 2.4 April FALSE ## 5 2.8 May TRUE ## 6 2.9 June TRUE ## 7 2.7 July FALSE ## 8 2.6 August FALSE ## 9 2.7 September FALSE ## 10 2.3 October FALSE ## 11 NA November NA ## 12 NA December NA To interact with the underlying vector of a data frame we can use the dollar sign $ operator. This takes the form of tbl$col_name. For example, extracting the unemp_rate column looks like: unemp_tbl$unemp_rate ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA Note that the difference between select(tbl, col) and tbl$col. select(unemp_tbl, unemp_rate) ## # A tibble: 12 x 1 ## unemp_rate ## &lt;dbl&gt; ## 1 3.2 ## 2 2.8 ## 3 2.8 ## 4 2.4 ## 5 2.8 ## 6 2.9 ## 7 2.7 ## 8 2.6 ## 9 2.7 ## 10 2.3 ## 11 NA ## 12 NA The difference is that $ returns the underlying vector whereas select() will always return another data frame. You now have the ability to both filter data and grab a subset of a vector. But we have yet to visit how to grab a single value from a data frame. You could try something like unemp_tbl %&gt;% select(1) %&gt;% slice(10) ## # A tibble: 1 x 1 ## unemp_rate ## &lt;dbl&gt; ## 1 2.3 to grab the 10th value of the first column. But again, you still have a tibble and you are not able to use that directly like a standalone number. We can again use brackets to subset the our R object. But data frames are two dimensional, so we need to specify the indexes in two dimensions. If you have made a hand drawn graph used a cartesian plane, which I assume you all have, this will is the same idea. With a cartesian plane we can identify any point with a combination of two values: x and y. x refers to the horizontal axis and y the vertical axis. When we put the cartesian plane in the same frame of reference as the rectangular data frame we envision our rows as the x and our columns as the y. In specifying our index, we are able to select all rows or all columns by leaving the x or y spot empty respectively. unemp_tbl[,1] ## # A tibble: 12 x 1 ## unemp_rate ## &lt;dbl&gt; ## 1 3.2 ## 2 2.8 ## 3 2.8 ## 4 2.4 ## 5 2.8 ## 6 2.9 ## 7 2.7 ## 8 2.6 ## 9 2.7 ## 10 2.3 ## 11 NA ## 12 NA unemp_tbl[10,] ## # A tibble: 1 x 3 ## unemp_rate month above_avg ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 2.3 October FALSE To replicate the above tidyverse example we would provide the indexes 10 and 1 respectively. unemp_tbl[10,1] ## # A tibble: 1 x 1 ## unemp_rate ## &lt;dbl&gt; ## 1 2.3 This is great, we’ve rewritten our tidyverse code in base R. But, just like the tidyverse code, we maintain the tibble data structure. This is because when we use a single bracket, it maintains the data structure of the object we are selecting from. If we wrap our brackets in another set of bracket, we are returned the an object of the same class as the underlying object. unemp_tbl[[10,1]] ## [1] 2.3 What that code is doing is narrowing the tibble down to a single column with a single row index and then extracting the underlying vector (the second bracket). To extract the underlying vector using the tidyverse, we can use the function dplyr::pull(). unemp_tbl %&gt;% select(1) %&gt;% slice(10) %&gt;% pull() ## [1] 2.3 Now this brings us to the second-most fundamental structure in R: the list. Yes, second-most fundamental. I’ve been keeping a secret from you. Data frames are actually just lists in disguise. To prove it, I will remove the class from unemp_tbl and return the class of that unclassed object. unclass(unemp_tbl) %&gt;% class() ## [1] &quot;list&quot; That is right, data frames are actually just lists disguised as rectangles. 10.3 Lists There is a good chance that you will not have to interact with them too often That doesn’t mean you shouldn’t know how to when that time comes. Lists are generally the most flexible object type in R. Unlike vectors and data frames lists do not impose any structure on the storage of our data. The most simple lists may resemble something like a vector. list(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;) ## [[1]] ## [1] &quot;Jan&quot; ## ## [[2]] ## [1] &quot;Feb&quot; ## ## [[3]] ## [1] &quot;Mar&quot; Notice how this prints differently than c(&quot;Jan&quot;, &quot;Feb&quot;, &quot;Mar&quot;) ## [1] &quot;Jan&quot; &quot;Feb&quot; &quot;Mar&quot; Each element of a list is self-contained. I think of lists somewhat like shipping containers where each element is its own container and all components of each element are together. We can include any type of R object in a list. For example, we can include the unemp_tbl and associated vectors. l &lt;- list(unemp_tbl, unemp, month.name) We can view the structure of the list to get an idea of what is actually contained by that list. str(l) ## List of 3 ## $ :Classes &#39;tbl_df&#39;, &#39;tbl&#39; and &#39;data.frame&#39;: 12 obs. of 3 variables: ## ..$ unemp_rate: num [1:12] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 ... ## ..$ month : chr [1:12] &quot;January&quot; &quot;February&quot; &quot;March&quot; &quot;April&quot; ... ## ..$ above_avg : logi [1:12] TRUE TRUE TRUE FALSE TRUE TRUE ... ## $ : num [1:12] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 ... ## $ : chr [1:12] &quot;January&quot; &quot;February&quot; &quot;March&quot; &quot;April&quot; ... The structure of l shows us that the first element is a tibble (has class tbl_df), and the other elements are numeric and character vectors respectively. Because of this flexibility there are not predetermined dimensions that we can specify to our brackets. Like extracting the underlying vector value from a data frame we have to use [[ for indexing. I like to think of [ as walking up to the storage container and [[ as actually opening it up and going inside. To get a sense of the difference lets look at the unemp vector. l[2] ## [[1]] ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA class(l[2]) ## [1] &quot;list&quot; When using the single bracket we are just selecting the first element of the list which is why we are returned another list. l[[2]] ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA class(l[[2]]) ## [1] &quot;numeric&quot; When we use the double bracket we are going inside of the container and actually plucking that element out of the list. Once you have plucked out that element, we can again use another set of brackets to subset that item. To grab the tenth row and first column of the unemp_tbl inside of l we can write. l[[1]][[10,1]] ## [1] 2.3 Now that we know that data frames are lists we can actually extract the underlying vectors using [[ as well as $. We can get the tenth row and first column a number of ways. # subsetting the data frame l[[1]][[10,1]] ## [1] 2.3 # grabbing the first vector then position l[[1]][[1]][10] ## [1] 2.3 # grabbing the vector by name then position l[[1]]$unemp_rate[10] ## [1] 2.3 Frankly all of these brackets can get a little messy. The tidyverse package purrr has a super handy function called pluck() which handles all of these brackets for us. purrr::pluck() is meant for flexible indexing into data structures (documentation). pluck() works by first providing the object that you’d like to index—again, notice the data first emphasis—and then providing the position of the element you would like to pluck out of the object. Generally, I will use pluck() when possible. By doing so the code becomes more readable and adheres to a single style more thoroughly. purrr::pluck(l, 1, 1, 10) ## [1] 2.3 Congratulations! You made it to the end of this exceptionally dense chapter. You may feel a little overwhlemed and that is to be expected. Nonetheless you should be proud! I have a few more asks of you before you move on. 10.3.1 Exercises Drink some water Move around a bit and shake it out Create a list with the vectors unemp, month.name, and avg_unemp. Recreate the unemp_tbl but referencing the list elements library(purrr) unemp_l &lt;- list(unemp, month.name, avg_unemp) tibble( unemp_rate = pluck(unemp_l, 1), month = pluck(unemp_l, 2) ) %&gt;% mutate(above_avg = unemp_rate &gt; pluck(unemp_l, 3)) ## # A tibble: 12 x 3 ## unemp_rate month above_avg ## &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; ## 1 3.2 January TRUE ## 2 2.8 February TRUE ## 3 2.8 March TRUE ## 4 2.4 April FALSE ## 5 2.8 May TRUE ## 6 2.9 June TRUE ## 7 2.7 July FALSE ## 8 2.6 August FALSE ## 9 2.7 September FALSE ## 10 2.3 October FALSE ## 11 NA November NA ## 12 NA December NA "],
["summary-statistics.html", "Chapter 11 Summary statistics", " Chapter 11 Summary statistics what is a summary stat? measures of central tendency measures of spread basic functions used mean, median, &amp; sd "],
["the-pipe.html", "Chapter 12 The pipe %&gt;% 12.1 Chaining functions", " Chapter 12 The pipe %&gt;% previously we have been working with one function at a time. it will become necessary to perform one function after another one way we could go about this is to write over the eisting object multiple times with an assignment &lt;- this is repetitive and hard to read we can utilize the forward pipe operator %&gt;% to chain functions together. from https://magrittr.tidyverse.org/: Basic piping x %&gt;% f is equivalent to f(x) x %&gt;% f(y) is equivalent to f(x, y) x %&gt;% f %&gt;% g %&gt;% h is equivalent to h(g(f(x))) from tidy soc sci 12.1 Chaining functions The true power of the tidyverse comes from it’s ability to chain functions after eachother. This is all enabled by the forward pipe operator %&gt;%. What it does: The pipe operator takes the output of it’s left hand side lhs and provides that output as the first argument in the function of the right hand side. Additionally, it exposes the lhs as a temporary variable .. Remember how I pointed out that the first argument for almost every function is the data? This is where that comes in handy. This allows us to use the pipe to chain together functions and “makes it more intuitive to both read and write” (magrittr vignette). You’ve seen how the first argument for every function here has been the data this is done purposefully to enable the use of the pipe. As always, the most helpful way to wrap your head around this is to see it in action. Let’s take one of the lines of code we used above and adapt it to use a pipe. We will select the name column of our data again. Previously we wrote select(data_frame, col_name). "],
["summarizing-the-tidy-way.html", "Chapter 13 summarizing the tidy way", " Chapter 13 summarizing the tidy way summarising data sets group_by() summarize() "],
["i-have-two-datasets-what-now.html", "Chapter 14 I have two datasets, what now?", " Chapter 14 I have two datasets, what now? multiple data sets what is a join? the need for a common identifier join types: left inner right full (rare) anti-join (for removing data) "],
["spatial-analysis.html", "Chapter 15 Spatial Analysis", " Chapter 15 Spatial Analysis What is spatial data? events happen at a location and are associate with place data become spatial when there is a geographic element present the most obvious is when lat and long data are present data which lists county, for example, is also spatial often there isn’t spatial data explicitly associated Types of spatial data Vector and Raster in gneral you will encounter vector data more frequently. vector data are coordinates raster data is used to “represent spatially continuous phenomenon” (https://rspatial.org/raster/spatial/Spatialdata.pdf) vector data is less complex to analyse and is the easiest to pick up raster data is extremely interesting but requires more devoted attention raster data analysis can be done to look at changing elevation, analyse vegatation, the amount of reflective surfaces, etc Vector data "],
["creating-ecometrics.html", "Chapter 16 Creating Ecometrics", " Chapter 16 Creating Ecometrics Note: a case study should be recreating these ecometrics Explore what the data description from BARI looks like library(tidyverse) ## ── Attaching packages ───────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ──────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ecometrics &lt;- readr::read_csv(&quot;data/911/911-ecometrics-2014-19.csv&quot;) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## TYPE = col_character(), ## tycod = col_character(), ## typ_eng = col_character(), ## sub_tycod = col_character(), ## sub_eng = col_character(), ## SocDis = col_double(), ## PrivateConflict = col_double(), ## Violence = col_double(), ## Guns = col_double(), ## Frequency_2015 = col_double(), ## Frequency_2016 = col_double(), ## Frequency_2017 = col_double(), ## Frequency_2018 = col_double(), ## yr.intro = col_double(), ## last.yr = col_double() ## ) glimpse(ecometrics) ## Observations: 302 ## Variables: 15 ## $ type &lt;chr&gt; &quot;AB===&gt;&gt;&gt;&quot;, &quot;ABAN===&gt;&gt;&gt;&quot;, &quot;ABANBU&quot;, &quot;ABANCELL&quot;,… ## $ tycod &lt;chr&gt; &quot;AB&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;… ## $ typ_eng &lt;chr&gt; &quot;ASSAULT AND BATTERY&quot;, &quot;ABANDONED CALL&quot;, &quot;ABAND… ## $ sub_tycod &lt;chr&gt; &quot;===&gt;&gt;&gt;&quot;, &quot;===&gt;&gt;&gt;&quot;, &quot;BU&quot;, &quot;CELL&quot;, &quot;INCCAL&quot;, &quot;PH… ## $ sub_eng &lt;chr&gt; &quot;PICK A SUB-TYPE&quot;, &quot;PICK A SUB-TYPE&quot;, &quot;FROM A B… ## $ soc_dis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ private_conflict &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ violence &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,… ## $ guns &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ frequency_2015 &lt;dbl&gt; 27, 40, 12768, 72081, 686, 1734, 3143, 194, 122… ## $ frequency_2016 &lt;dbl&gt; 29, 26, 13287, 56074, 1188, 1205, 2687, 397, 19… ## $ frequency_2017 &lt;dbl&gt; 29, 17, 12599, 39323, 124, 724, 1944, 253, 162,… ## $ frequency_2018 &lt;dbl&gt; 14, 10, 6422, 19947, 70, 506, 840, 262, 169, 78… ## $ yr_intro &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,… ## $ last_yr &lt;dbl&gt; 2019, 2018, 2019, 2019, 2019, 2019, 2019, 2019,… Check out the offense descriptions raw_911 &lt;- read_csv(&quot;data/911/911-raw.csv&quot;) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## INCIDENT_NUMBER = col_character(), ## OFFENSE_CODE = col_character(), ## OFFENSE_CODE_GROUP = col_character(), ## OFFENSE_DESCRIPTION = col_character(), ## DISTRICT = col_character(), ## REPORTING_AREA = col_double(), ## SHOOTING = col_character(), ## OCCURRED_ON_DATE = col_datetime(format = &quot;&quot;), ## YEAR = col_double(), ## MONTH = col_double(), ## DAY_OF_WEEK = col_character(), ## HOUR = col_double(), ## UCR_PART = col_character(), ## STREET = col_character(), ## Lat = col_double(), ## Long = col_double(), ## Location = col_character() ## ) raw_911 %&gt;% count(offense_code_group) ## # A tibble: 68 x 2 ## offense_code_group n ## &lt;chr&gt; &lt;int&gt; ## 1 Aggravated Assault 10687 ## 2 Aircraft 60 ## 3 Arson 109 ## 4 Assembly or Gathering Violations 1143 ## 5 Auto Theft 6211 ## 6 Auto Theft Recovery 1385 ## 7 Ballistics 1338 ## 8 Biological Threat 3 ## 9 Bomb Hoax 109 ## 10 Burglary - No Property Taken 5 ## # … with 58 more rows There are fewer types of code groups. They are rather informative. 33k missing observations though is the same amount of missingness present in the descriptions? raw_911 %&gt;% count(offense_description) ## # A tibble: 279 x 2 ## offense_description n ## &lt;chr&gt; &lt;int&gt; ## 1 A&amp;B HANDS, FEET, ETC. - MED. ATTENTION REQ. 1 ## 2 A&amp;B ON POLICE OFFICER 7 ## 3 ABDUCTION - INTICING 12 ## 4 AFFRAY 318 ## 5 AIRCRAFT INCIDENTS 70 ## 6 ANIMAL ABUSE 89 ## 7 ANIMAL CONTROL - DOG BITES - ETC. 493 ## 8 ANIMAL INCIDENTS 388 ## 9 ANIMAL INCIDENTS (DOG BITES, LOST DOG, ETC) 34 ## 10 ANNOYING AND ACCOSTIN 3 ## # … with 269 more rows no missingness more types what do we do? choose one of these? we can actually use both filter data set to exclude NAs from offense_code_group. anti_join() that dataset from raw now use the raw_911 remainders and hand code that. reminder: we cannot automate everything. It requires long hours, dedication, and freakin’ grit. string manipulation using SPSS data using the tidycensus package for pulling in census data using rtweet for twitter data "],
["case-study-twitter-data.html", "Chapter 17 Case Study: Twitter Data", " Chapter 17 Case Study: Twitter Data appendix - style "]
]
