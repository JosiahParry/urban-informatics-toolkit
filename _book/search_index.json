[
["index.html", "Urban Informatics Toolkit Chapter 1 Welcome 1.1 What is Urban Informatics 1.2 What to expect 1.3 Reminders 1.4 Setting the Context: Big Data 1.5 outline / notes 1.6 Other notes", " Urban Informatics Toolkit Josiah Parry 2020-02-10 Chapter 1 Welcome Welcome to the Urban Informatics Toolkit! This book will provide you with an introduction to some of the history of Urban Informatics as a field, important concepts in the field, and provide you with the skills required to jumpstart your own exploration of the urban commons. 1.1 What is Urban Informatics 1.2 What to expect This book is partitioned into chapters where each goes over a small subset of skills that is important to know and a few case studies. the first half of the book will introduce you to the basic tools that you will need to know to become self-sufficient. From there on, we will work with case studies that also introduce new topics and further more advances skills In Part II each chapter, with a few exceptions, will introduce you to a social phenomenon. We will ask a question, and use a relevant dataset to answer this question. While we work to approach each question, we will pause to learn about the tools that we are using, understand what they are doing and why. There will always be a TL;DR for each chapter recapping what we covered, and links to further resources. technical chapters will be interspersed with non-techinical chapters. 1.3 Reminders Learning to program can be exceptionally difficult at times. It can be a roller coaster of emotions. It is expected that you will not understand everything the first go around. Do not get down on yourself. I encourage you to take breaks and not push yourself too hard. I understand you have deadlines, but sometimes it is better if you take a break, eat a healthy snack, go exercise, sleep, be social, etc and then come back. You will be much happier and your work will be even better and that I promise you! Perhaps I am showing my cards a bit too much, but I have always found that with tough assignments it is better to go to bed early and finish them in the morning than it is to stay up exceptionally late. Moral of that story is, get some sleep and take care of yourself! I’ll be sure to remind you from time to time. 1.4 Setting the Context: Big Data The study of Urban Informatics is inherently intertwined with big data. Working with big data requires a set of skills that extends beyond quantitative and qualitative analysis expertise. Students need to know how to access, manipulate, analyze, communicate and share data artifacts. The intention of this practicum is to develop educational content to cultivate an expertise in data. An online text and accompanying exercises will be created to teach students data science in a manner that is tailored toward Urban Informatics studies. Writing this content will provide an opportunity to synthesize the learnings and skills taught in the Urban Informatics program. The manner in which it will be done will also further access to the field and assist in the cultivation of future students. I have always struggled with overly academic writing and overly technical and obtuse descriptions of concepts, tasks, and phenomena that can be written in easy to understand ways. my goal for this text is to provide you with a non-technical instruction to a technical topic. I want this to be friendly and not adversarial nor patronizing. I will do my best to try and explain this language and topics the way that I would to my friends over a beer. 1.5 outline / notes intro is what is ui What is administrative data Current examples of the data driven city Why we need to learn data with theory The data workflow Question then data Data then questions new approach due to massive data There are many insights yet to be discovered R The hard part is getting started. I want to make this as easy as possible with social science and UI examples bear with me for a bit. * Basics as a calculator * Visualization / Visualizing relationships / Visuializing relationships * the type of the data that we have will dictate how we visualize it * Univariate * Bivariate * Data Manipulation * filtering data Tidy and untidy data * tidy data: * this is just to introduce you to the idea and get an idea of what it looks like. No need to understand the code or anything that will show up afterwards. * the principles * demonstrate what it looks like The boring part data structures Administrative data data cleaning manopulatoon Aggregation Introduce interacting with public data via webportals and APIs Geospatial analysis: data are often grounded in space its important to understand where things happen 3 Common data that are associated with space 311 - point (vector data) census tracts - polygon (vector) sattelite imagery - raster (we wont cover this) polygon is the most simple to work with visualization of aggregate data aggregating by polygon (building permits) points are a bit trickier often we want to join this Text analysis unstructured text as data what is the scope and limitations of text data? working with text data: stop words tokenization 1.6 Other notes I will say these data and this data interchangably. I apologize for that. "],
["the-basics.html", "Chapter 2 The basics 2.1 What is R and why do I care? 2.2 The RStudio IDE 2.3 Installing R &amp; RStudio 2.4 R Projects", " Chapter 2 The basics 2.1 What is R and why do I care? What is R? R is the 18th letter of the alphabet, the fourth letter in QWERTY—like the keyboard—and, most importantly, R is a software package for statistical computing. R is developed and maintained by a group known as the R Core Team they are part of the R Foundation The R-project describes R as: R is an integrated suite of software facilities for data manipulation, calculation and graphical display. https://www.r-project.org/about.html it’s like a very fancy calculator that does many things in addition to all sorts of math R is produced by the R-project independent non-profit organization funded by the R foundation R is more than just a language for statistics it is designed to be extended for further capabilities, and indeed it has been, this is done through software packages will review this very shortly you may have been exposed to stata and spss previously. R is different. R is unique for a number of reasons it is both free and open source while it is free monetarily, free refers to “liberty, not price.” (https://gnu.org) while you continue in your learning be aware of the four essential freedoms and how it relates to the importance of scientific inquiry: The freedom to run the program as you wish, for any purpose (freedom 0). The freedom to study how the program works, and change it so it does your computing as you wish (freedom 1). Access to the source code is a precondition for this. The freedom to redistribute copies so you can help others (freedom 2). The freedom to distribute copies of your modified versions to others (freedom 3). By doing this you can give the whole community a chance to benefit from your changes. Access to the source code is a precondition for this. this is important to know as you begin to be a part of the R community these freedoms are part of the success of R as a language many of the statistical techniques that are being discovered and implemented are being done in R academics from around the globe are contributing to the language and the tools we need to begin to think of tooling as part of the scientific process, not apart from it while you will likely use R to engage in scientific inquiry, many people have developed these tools specifically for that purpose and are no less scientific than what you will engage in this means you should cite R when you get that chance you should think about how the science you engage in will contribute to the open science movement are you following the four essential freedoms? https://cran.r-project.org/ 2.2 The RStudio IDE R when downloaded is a programming language that is interacted with through the terminal which, while some people love it, can feel like programming in the matrix _ For this reason, we will download RStudio RStudio is an IDE (integrated development environment) i think of R and RStudio is typesetting a printing press and using a word processor like microsoft word Chester Ismay and Albert Kim’s Modern Dive provide another excellent analogy of R and RStudio R is the engine RStudio is the car dashboard: https://moderndive.com/1-getting-started.html rstudio gives you a place to write R, make and visualize graphics, and analyse data. R and RStudio can be used to build web applications (shiny gallery), websites and blogs, and even art (generative thomas). there are (almost) no limits as to what you can do with R and RStudio Let’s get familiar with RStudio. You need to know where you are when working within RStudio. There are 4 quadrants that we work with (called panes). This lovely graphic was created by my colleage thomas mock. Source Let’s focus on the edit, console, and output areas 2.2.1 The Editor The editor is where you will actually write your R code. R code is written in text files that have the .R extension you can also write in what is known as an RMarkdown Document which has an extension of .Rmd SIDE BAR fun fact, this book is written in RMarkdown R markdown lets you integrate prose and R code together. RMarkdown can be rendered into many formats including pdf, html, ppt we will go over this at the end of the book we will work within R scripts primarily to keep things simple i donkt know if youre like me at all, but when I write, I tend to have two documents up, or at least two sections of a document. One for where the finalized content and another one as scratch paper think of the editor as the place where you put the writing you want to keep executing code (either with command + enter on the highlighted code or the line where your cursor is) or the run button above i recommend learning the key strokes. it will be immensely helpful this brings us to the console 2.2.2 The console the console is where your r code is actually executed so when you run a line of code from the editor i also treat the console as my scratch paper you can write r code in here and it will not be saved into your R script (your word document) when you produce R code that creates a visualization, the code will be executed in the console and the visualization will show up in the viewer pane (the “Output” section) 2.2.3 Output This quadrant is a very versatile section. It will be used primarily for looking at things. this is where you will see your visualizations help documentation where you can navigate your files and open things 2.3 Installing R &amp; RStudio It’s easiest if you install R and then RStudio you will download R from the Central R Archival Network aka CRAN this is the “offical” location for R and it’s software extensions (called packages) install R: https://cran.r-project.org/ install RStudio: https://rstudio.com/products/rstudio/download/#download when you’re ready to get started, open RStudio not R (look for the circular logo) if you get lost, check out the RStudio IDE cheatsheet 2.4 R Projects once you open up RStudio you will be able to get rocking and rolling though, we want to instill some best practices from the get go one of which is the use of R projects. This will help contain code and files in one location. By being diligent in creating R projects you will save yourself SO MUCH hair pulling. now what makes an R project and R project? a folder with a special .Rproj file the Rproj file lets R know that the directory (folder) you are working from is a project and will manage file paths for you. This may not be making a ton of sense, but it will once we get further through this book. how do you make a new R project? file new project name the thing something informative do NOT use spaces or periods. use dashes or underscores when should you make a new project? if the code is related to eachother i think of every general analysis as its own project refer to rstats.wtf "],
["r-as-a-calculator.html", "Chapter 3 R as a calculator 3.1 Arithmetic Operators 3.2 Variable assignment 3.3 Functions 3.4 Extensions to R 3.5 Loading Packages", " Chapter 3 R as a calculator Before we get going, let’s find our footing. R is a statistical programming language. That means that R does math and pretty well too. In this chapter you’ll learn the basics of using R including: arithmetic operators creating and assigning variables using functions 3.1 Arithmetic Operators Do you remember PEMDAS? If not, a quick refresher that PEMDAS specifies the order of operations for a math equation. Do the math inside of the parentheses, then the exponents, and then the multiplication or the division before addition or subtraction. We can’t write the math out, so we need to type it out. Below are the basic arithmetic operators ^ : exponentiation (exponents) [E] * : multiplication [M] / : division [D] + : addition [A] - : subtraction [S] These can be used together in parentheses [P] ( ) to determine the order of operations (PEMDAS) Try out some mathematic expressions in the console. 3.2 Variable assignment I’m sure you recall some basic algebra questions like \\(y = 3x - 24\\). In this equation, x and y are variables that represents some value. We will often need to create variables to represent some value or set of values. In R, we refer to variables as objects. Objects can be a single number, a set of words, matrixes, and so many other things. To create an object we need to assign it to some value. Object assignment is done with the assignment operator which looks like &lt;-. You can automagically insert the assignment operator with opt + -. Let’s work with the above example. We will solve for y when x is equal to 5. First, we need to assign 5 to the variable x. x &lt;- 5 If you want to see the contents of an object, you can print it. To print an object you can type the name of it. x ## [1] 5 We can reference the value that x stores in other mathematic expressions. Now what does y equal? Now solve for y in the above equation! y &lt;- 3 * x - 24 y ## [1] -9 3.3 Functions Functions are a special kind of R object. Very simply, a function is an object that performs some action and (usually) produces an output. Functions exist to simplify a task. You can identify a function by the parentheses that are appended to the function name. A function looks like function_name(). R has many functions that come built in. The collection of functions that come out of the box with R are called *Base R**. An example of a simple base R function is sum(). sum() takes any number of inputs and calculates the sum of those inputs. We can run sum() without providing any inputs. sum() ## [1] 0 We can provide more inputs (formally called function arguments) to sum(). For example to find the sum of 10 we write sum(10) ## [1] 10 The sum of a single number is the number itself. We can provide more arguments to sum(). Additional arguments are specified by separating them with commas—e.g. function(argument_1, argument_2). To find the sum of 10, 3, and 2 we write sum(10, 3, 2). sum(10, 3, 2) ## [1] 15 Much of the analysis we will do is done with functions. You will become much more comfortable with them rather quickly. If you ever need to know how a function works, you can look at its help page by typing ?function_name() in your console. That will bring up the documentation page in the bottom right pane. 3.4 Extensions to R While R was created as a statistical programming language, it was designed with the intention of being extended to include even more functionality. Extensions to R are called packages. R packages often provide a set of functions to accomplish a specific kind of task. To analyse, manipulate, and visualize our data, we will use a number of different packages to do so. Throughout this book we will become familiar with a set of packages that together are known as the Tidyverse. R packages do not come installed out of the box. We will need to install them our selves. Base R includes a function called install.packages(). install.packages() will download a specified package from CRAN and install it for us. To download packages, we must tell install.packages() which package to download. We will provide the name of the package as the only argument to install.packages(). The name of the package needs to be put into quotations such as install.packages(&quot;package-name&quot;). Note: By putting text into quotations we are creating what is called a character string. Reminder: we create objects with the assignment operator &lt;-. When we don’t use quotes (create a character string), R thinks we are referring to an object we have created. 3.4.1 Exercise Use your new knowlege of functions and installing packages to install the tidyverse. install.packages(&quot;tidyverse&quot;) 3.5 Loading Packages Now that you have installed the tidyverse, you are going to need to know how to make it available to you for use. To load a package, we use the function library(). Oddly, though, when specifying which package to load, we do not put that name in quotations. Note: It is best practice to load all of your packages at the top of your R script. library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() Notice the message above. When we load the tidyverse, we are actually loading eight packages at once! Thesee are the packages listed under “Attaching packages.” You have now successfully installed and loaded the tidyverse. Next, we will begin to learn how to visually analyze data! 3.5.1 Resources: https://techterms.com/definition/string "],
["visual-analysis.html", "Chapter 4 Visual Analysis 4.1 Data Exploration 4.2 The American Community Survey", " Chapter 4 Visual Analysis 4.1 Data Exploration introduce you to the data analysis workflow and exploratory data analysis in R for Data Science Garret Grolemund and Hadley Wickham define data exploration as: Data exploration is the art of looking at your data, rapidly generating hypotheses, quickly testing them, then repeating again and again and again. exploratory data analysis is done generally when your data were not collected as part of an experiment EDA is used for inductive scientific inquiry we do not necessarily know what we are looking for at the outset or we may have an idea of what is available to us generally when we are performing exploratory data analysis, we did not partake too greatly in the data collection process however, even if your data are the result of an experiment, you may we want to dig into it further. You never know what you may find. You may find the seedling that sprouts into your next experiment below is an image from R for Data Science which accurately describes the data exploration process there are 6 steps in this illustration, 3 of which happen in a cyclical process this chapter focuses on visualizing our data visualization is key for this process. An older National Institute of Standards and Technology (NIST) statistical manual contains this gem The reason for the heavy reliance on graphics is that by its very nature the main role of EDA is to open-mindedly explore, and graphics gives the analysts unparalleled power to do so, enticing the data to reveal its structural secrets, and being always ready to gain some new, often unsuspected, insight into the data. In combination with the natural pattern-recognition capabilities that we all possess, graphics provides, of course, unparalleled power to carry this out. source 4.2 The American Community Survey we’ll work with data from the american community survey. this is data you will get really familiar with what is the acs and why do we love it? https://www.vox.com/explainers/2015/12/3/9845152/acs-survey-defunded random sample of individuals across the us random samples are used as they representative and statistically non-biased the information is used to determine funding tells us about age, ethnicity, country of origin, occupation, education, voting behavior, etc. this information is available in the decennial census (in the constitution) ACS tells us about rates rather than the actual number of people in a thing one of the major problems is that some populations are under-represented https://prospect.org/economy/insidious-way-underrepresent-minorities/ What is the relationship between education and income? we have a data frame loaded this is very similar to a table in excel each column is a variable acs_edu ## # A tibble: 1,456 x 7 ## med_house_income less_than_hs hs_grad some_coll bach white black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 0.0252 0.196 0.221 0.325 0.897 0.0122 ## 2 69625 0.0577 0.253 0.316 0.262 0.885 0.0171 ## 3 70679 0.0936 0.173 0.273 0.267 0.733 0.0795 ## 4 74528 0.0843 0.253 0.353 0.231 0.824 0.0306 ## 5 52885 0.145 0.310 0.283 0.168 0.737 0.0605 ## 6 64100 0.0946 0.294 0.317 0.192 0.966 0.00256 ## 7 37093 0.253 0.394 0.235 0.101 0.711 0.0770 ## 8 87750 0.0768 0.187 0.185 0.272 0.759 0.0310 ## 9 97417 0.0625 0.254 0.227 0.284 0.969 0.00710 ## 10 43384 0.207 0.362 0.262 0.124 0.460 0.105 ## # … with 1,446 more rows we can visualize the relationship to get a better understanding. we can look at income and college grad rates start building a visualization with ggplot() function with the acs_edu object. Functions are characterised by the parentheses at the end of them. Functions do things. Objects hold information. ggplot(acs_edu) we add to this plot determine what we want to plot with the aesthetics aes() function inside of the ggplot(). want to specify x and y. These are called arguments. To set argument we use the = sign. set x to bach and y to med_house_income ggplot(acs_edu, aes(x = bach, y = med_house_income)) notice how the plot is being filled a bit more? now we need to specify what type of plot we will be creating. add geometry, or geoms. We use the plus sign + to signify that we are adding on top of the basic graph there are many different kinds of charts we can use we will get into these a bit more later. a common way of visualizing a relationship between two variables is with a scatterplot a scatter plot graphs points for each x and y pair. you’ve likely made a few of these in your primary education to add points to the graph we use geom_point(), remember, we are adding a layer so we use the plus sign for legibility we add each new layer on a new line. R will indent for you. Good style is important. We’ll get into this later ggplot(acs_edu, aes(x = bach, y = med_house_income)) + geom_point() notice that there is a positive linear trend. lets break that down: when the point point up to the right, that is positive, down to the left is negative. for each unit we go up on the x (bach) we tend to go up on the y (hh income) linear means it resembles a line what are we looking for in a scatter plot? we’re looking for low variation and a consistent pattern or line we want the points to be very close imagine we drew a line going through the middle of the points, we’d want each point to be either on that line or extremely close. if the line is further away, that means there is a lot more variation to finish this up we can add some informative labels we will add a labels layer with the function labs() give them more legible labels. we will give each axis a better name and give the plot a title the arguments we will set to the labs function are x, y and title. Rather intuitive, huh? x = “% of population with a Bachelor’s Degree” y = “Median Household Income” title = “Relationship between Education and Income” note that for each argument I have a new line. again, this helps with legibility ggplot(acs_edu, aes(x = bach, y = med_house_income)) + geom_point() + labs(x = &quot;% of population with a Bachelor&#39;s Degree&quot;, y = &quot;Median Household Income&quot;, title = &quot;Relationship between Education and Income&quot;) what can we determine from this graph? in the sociology literature there is a lot about the education gap between white and black folks can we see this in a graph? we can modify our existing plot to illustrate this too. we can map the % white to the color of the chart we add to this within the aesthetics. The aes()thetics is where we will determine things like size, group, shape, etc. set the color argument to the white column while we’re at it, we can add a subtitle to inform that we’re also coloring by % white ggplot(acs_edu, aes(x = bach, y = med_house_income, color = white)) + geom_point() + labs(x = &quot;% of population with a Bachelor&#39;s Degree&quot;, y = &quot;Median Household Income&quot;, title = &quot;Relationship between Education and Income&quot;, subtitle = &quot;Colored by whiteness&quot;) what can we conclude now? 4.2.0.1 Misc Notes: this chapter is to introduce the concept of exploratory data analysis this should be done at a later point NIST EDA: https://www.itl.nist.gov/div898/handbook/eda/section1/eda11.htm https://r4ds.had.co.nz/explore-intro.html iteration this chapter focuses on visualization the process of exploratory anlaysis is naturally inductive "],
["schools-of-urban-informatics.html", "Chapter 5 Schools of Urban Informatics", " Chapter 5 Schools of Urban Informatics Sociological origins the chicago school Park and Burgess the key findings from The City Discuss how these findings were generated inductive approaches to theory, ONLY the sante fe school this is the complete other end of the example sante fe is a group of physical scientists looking to use hyper-modern techniques and apply theory to observed social data this group has a general set of theories that they use to evaluate urban life where does the Boston School fit in? let’s step back to understand the two general approaches to scientific inquiry Inductive - finding something from the data (think chicago) Deductive - having a theory and setting out to explicitly test that (think psychology experiment) Overview of inductive Overview of deductive the Boston School is a hybrid approach Next Chapter: Let us use the example of broken windows theory and the development of ecometrics some questions to ponder: what is a strnegth of the chicago school? what is the strength of the santa fe institute? https://www.santafe.edu/research/projects/cities-scaling-sustainability what do you think is the ideal approach? is it one or the other? a blend? something entirely new? "],
["broken-windows-and-the-origin-of-ecometrics.html", "Chapter 6 Broken Windows and the origin of Ecometrics", " Chapter 6 Broken Windows and the origin of Ecometrics To Cover: sampson &amp; raudenbush systematic social observation: https://www.journals.uchicago.edu/doi/abs/10.1086/210356 ecometrics: where was this first coined for the social sciences???? 1980 “Broken Windows” in the Atlantic Posited that the presence of physical disorder led to crime this influenced policing policies like stop and frisk led to the infamous implementation of CompStat the simplicity of the theory was appealing to public officials. media lapped it up social scientists were quick to shoot down the theory, seemed to not matter come 1999 Sampson and Raudenbush seek to test this find no conclusion much more work comes from it most noteably is the use of ecometrics in assessing this. I say most noteably because of its approach it combines an inductive approach with a deductive one. this is the quintessence of the Boston approach, of course it comes from Dan O’Brien. why?: big data (actually big-ish which is rare in social science) The development and use of ecometrics (induction) deliberately testing an existing theory (deduction) Absolutely crushing it. what is an ecometric? why is it inductive? taking naturally occuring data, and extracting latent (already existing variables) Note: a case study should be recreating these ecometrics Explore what the data description from BARI looks like library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ecometrics &lt;- readr::read_csv(&quot;data/911/911-ecometrics-2014-19.csv&quot;) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## TYPE = col_character(), ## tycod = col_character(), ## typ_eng = col_character(), ## sub_tycod = col_character(), ## sub_eng = col_character(), ## SocDis = col_double(), ## PrivateConflict = col_double(), ## Violence = col_double(), ## Guns = col_double(), ## Frequency_2015 = col_double(), ## Frequency_2016 = col_double(), ## Frequency_2017 = col_double(), ## Frequency_2018 = col_double(), ## yr.intro = col_double(), ## last.yr = col_double() ## ) glimpse(ecometrics) ## Observations: 302 ## Variables: 15 ## $ type &lt;chr&gt; &quot;AB===&gt;&gt;&gt;&quot;, &quot;ABAN===&gt;&gt;&gt;&quot;, &quot;ABANBU&quot;, &quot;ABANCELL&quot;,… ## $ tycod &lt;chr&gt; &quot;AB&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;… ## $ typ_eng &lt;chr&gt; &quot;ASSAULT AND BATTERY&quot;, &quot;ABANDONED CALL&quot;, &quot;ABAND… ## $ sub_tycod &lt;chr&gt; &quot;===&gt;&gt;&gt;&quot;, &quot;===&gt;&gt;&gt;&quot;, &quot;BU&quot;, &quot;CELL&quot;, &quot;INCCAL&quot;, &quot;PH… ## $ sub_eng &lt;chr&gt; &quot;PICK A SUB-TYPE&quot;, &quot;PICK A SUB-TYPE&quot;, &quot;FROM A B… ## $ soc_dis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ private_conflict &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ violence &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,… ## $ guns &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ frequency_2015 &lt;dbl&gt; 27, 40, 12768, 72081, 686, 1734, 3143, 194, 122… ## $ frequency_2016 &lt;dbl&gt; 29, 26, 13287, 56074, 1188, 1205, 2687, 397, 19… ## $ frequency_2017 &lt;dbl&gt; 29, 17, 12599, 39323, 124, 724, 1944, 253, 162,… ## $ frequency_2018 &lt;dbl&gt; 14, 10, 6422, 19947, 70, 506, 840, 262, 169, 78… ## $ yr_intro &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,… ## $ last_yr &lt;dbl&gt; 2019, 2018, 2019, 2019, 2019, 2019, 2019, 2019,… Check out raw_911 &lt;- read_csv(&quot;data/911/911-raw.csv&quot;, n_max = 5000) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## INCIDENT_NUMBER = col_character(), ## OFFENSE_CODE = col_character(), ## OFFENSE_CODE_GROUP = col_character(), ## OFFENSE_DESCRIPTION = col_character(), ## DISTRICT = col_character(), ## REPORTING_AREA = col_double(), ## SHOOTING = col_character(), ## OCCURRED_ON_DATE = col_datetime(format = &quot;&quot;), ## YEAR = col_double(), ## MONTH = col_double(), ## DAY_OF_WEEK = col_character(), ## HOUR = col_double(), ## UCR_PART = col_character(), ## STREET = col_character(), ## Lat = col_double(), ## Long = col_double(), ## Location = col_character() ## ) raw_911 %&gt;% filter(str_detect(offense_description, coll(&quot;battery&quot;, T))) ## # A tibble: 342 x 17 ## incident_number offense_code offense_code_gr… offense_descrip… district ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 I192078613 00802 Simple Assault ASSAULT SIMPLE … A7 ## 2 I192078603 00802 Simple Assault ASSAULT SIMPLE … A7 ## 3 I192078600 00413 Aggravated Assa… ASSAULT - AGGRA… C11 ## 4 I192078573 00802 Simple Assault ASSAULT SIMPLE … B2 ## 5 I192078563 00413 Aggravated Assa… ASSAULT - AGGRA… B3 ## 6 I192078563 00802 Simple Assault ASSAULT SIMPLE … B3 ## 7 I192078538 00802 Simple Assault ASSAULT SIMPLE … C6 ## 8 I192078531 00802 Simple Assault ASSAULT SIMPLE … C11 ## 9 I192078530 00413 Aggravated Assa… ASSAULT - AGGRA… C6 ## 10 I192078529 00413 Aggravated Assa… ASSAULT - AGGRA… E5 ## # … with 332 more rows, and 12 more variables: reporting_area &lt;dbl&gt;, ## # shooting &lt;chr&gt;, occurred_on_date &lt;dttm&gt;, year &lt;dbl&gt;, month &lt;dbl&gt;, ## # day_of_week &lt;chr&gt;, hour &lt;dbl&gt;, ucr_part &lt;chr&gt;, street &lt;chr&gt;, ## # lat &lt;dbl&gt;, long &lt;dbl&gt;, location &lt;chr&gt; 6.0.1 Resources: https://www.annualreviews.org/doi/abs/10.1146/annurev-criminol-011518-024638?journalCode=criminol Large-scale data use, ecometrics to assess disorder: https://journals.sagepub.com/doi/abs/10.1177/0022427815577835 "],
["reading-data.html", "Chapter 7 Reading data 7.1 Background 7.2 Actually Reading Data 7.3 Other common data formats", " Chapter 7 Reading data We spent the last chapter performing our first exploratory visual analysis. From our visualizations we were able to inductively conlcude that as both median household income and the proportion of the population with a bachelors degree increases, so does the share of the population is white. This is a finding we will work to address empirically at a later point. While we were able to make wonderful visualizations, we did skip a number of steps in the exploratory analysis process! Arguably most importantly we skipped the importing of our datasets. The ACS dataset was already loaded into R for you. This almost always will not be the case. As such, we’re going to spend this chapter learning about some of the common data formats that you will often work with. { image of the visualization step } 7.1 Background There are three general sources where we as social scientists will recieve or access data: 1) text files, 2) databases, and 3) application programming interfaces (APIs). Frankly, though this is the age of “big data,” we are not always able to interface directly with these sources. But through partnership efforts between the public and private we able to share data. For example, BARIs work with the Boston Police Department provides them with annual access to crime data. But BARIs access is limited. They do not have credentials to log in to the database and perform their own queries. What they are usually presented with is a flat text file(s) that contains the data requisite for analysis. And this is what we will focus in this chapter. Flat text files will be sufficient for 85% of all of your data needs Now, what do I mean by flat text file? A flat text file is a file that stores data in plain text—I know, this seems somewhat confusing. In otherwords, you can open up a text file and actually read the data with your own eyes or a screenreader. For a long while tech pundits believed—and some still do—that text data will be a thing of the past. Perhaps this may be true in the future, but plain text still persists and there are some good reasons for that. Since plain text is extremely simple it is lightweight and usually does not take up that much memory. Also, because there is no fancy embelishing of the data in a plain text file, they can be easily shared from machine to machine without concern of dependent tools and software. Not to mention that we humans can actually be rather hands on and inspect the source of the data ourselves. 7.2 Actually Reading Data Within the tidyverse there is a package called readr which we use for reading in rectangular data from text files. Aside: I am still unsure if it is pronounced read-arr or read-er. So just do your best. I just threw the phrase rectangular data at you. It is only fair to actually describe what that means. If you were to look at rectangular data in something like excel it would resemble a rectangle. In fancy speak, rectangular data is a two-dimensional data structure with rows and columns. We will learn more about the “proper” way to shape rectangular data in the “tidying data” chapter. For now, all you need to know is that there are rows and columns in rectangular data. To get started, let us load the tidyverse. This will load readr for us. library(tidyverse) You most likely have seen and encountered flat text files in the wild inthe form of a csv. It is important to know what csv stands for because it will help you understand what it actually is. it stands for comma separated values. _csv_s are a flat text data file where the data is rectangular! Each new line of the file indicates that there is a new row. Within each row, each comma indicates a new column. If you opened one up in a text editor like text edit or notepad a csv would look something like below. column_a, column_b, column_c, 10, &quot;these are words&quot;, .432, 1, &quot;and more words&quot;, 1.11 To read a csv we use the readr::read_csv() function. read_csv() will read in the csv file and create a tibble. A tibble is type of a data structure that we will be interacting with the most throughout this book. A tibble is a rectangular data structure with rows and columns. Since a csv contains rectangular data, it is natural for it to be stored in a tibble. Note: the syntax above is used for referencing a function from a namespace (package name). The syntax is pkgname::function(). This means the read_csv() function from the package readr. This is something you will see frequently on websites like StackOverflow. Have a look at the arguments of read_csv() by entering ?read_csv() into the console. You will notice that there are many arguments that you can set. These are there to give you a lot of control over how R will read your data. For now, and most of the time, we do not need to be concerned about these extra arguments. All we need to do is tell R where our data file lives. If you haven’t deduced from the help page yet, we will supply only the first argument file. This argument is either a path to a file, a connection, or literal data (either a single string or a raw vector). Note: When you see the word string, that means values inside of quotations—i.e. “this is a string”. We will read in the dataset that was already loaded in the first chapter. These data are stored in the file named acs_edu.csv. We can try reading this as the file path. read_csv(&quot;acs_edu.csv&quot;) ## Error: &#39;acs_edu.csv&#39; does not exist in current working directory ## (&#39;/Users/Josiah/GitHub/urban-commons-toolkit&#39;). Oops. We’ve got red text and that is never fun. Except, this is a very important error message that, frankly, you will get a lot. Again it says: Error: ‘acs_edu.csv’ does not exist in current working directory I’ve bolded two portions of this error message. Take a moment to think through what this error is telling you. For those of you who weren’t able to figure it out or just too impatient (like myself): this error is telling us that R looked for the file we provided acs_edu.csv but it could not find it. This usually means to me that I’ve either misspelled the file name, or I have not told R to look in the appropriate folder (a.k.a. directory). acs_edu.csv actually lives in a directory called data. To tell R—or any computer system, really—where that file is we write data/acs_edu.csv. This tells R to first enter the data directory and then look for the acs_edu.csv file. Now, read the acs_edu.csv file! read_csv(file = &quot;data/acs_edu.csv&quot;) ## Parsed with column specification: ## cols( ## med_house_income = col_double(), ## less_than_hs = col_double(), ## hs_grad = col_double(), ## some_coll = col_double(), ## bach = col_double(), ## white = col_double(), ## black = col_double() ## ) ## # A tibble: 1,456 x 7 ## med_house_income less_than_hs hs_grad some_coll bach white black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 0.0252 0.196 0.221 0.325 0.897 0.0122 ## 2 69625 0.0577 0.253 0.316 0.262 0.885 0.0171 ## 3 70679 0.0936 0.173 0.273 0.267 0.733 0.0795 ## 4 74528 0.0843 0.253 0.353 0.231 0.824 0.0306 ## 5 52885 0.145 0.310 0.283 0.168 0.737 0.0605 ## 6 64100 0.0946 0.294 0.317 0.192 0.966 0.00256 ## 7 37093 0.253 0.394 0.235 0.101 0.711 0.0770 ## 8 87750 0.0768 0.187 0.185 0.272 0.759 0.0310 ## 9 97417 0.0625 0.254 0.227 0.284 0.969 0.00710 ## 10 43384 0.207 0.362 0.262 0.124 0.460 0.105 ## # … with 1,446 more rows This is really good! Except, all that happened was that the function was ran. The data it imported was not saved anywhere which means we will not be able to interact with it. What we saw was the output of the data. In order to interact with the data we need to assign it to an object. Reminder: we assign object with the assignment operator &lt;-—i.e. new_obj &lt;- read_csv(&quot;file-path.csv&quot;). Objects are things that we interact with such as a tibble. Functions such as read_csv() usually, but not always, modify or create objects. In order to interact with the data, let us store the output into a tibble object called acs. acs &lt;- read_csv(file = &quot;data/acs_edu.csv&quot;) Notice how now there was no data printed in the console. This is a good sign! It means that R read the data and stored it properly into the acs object. When we don’t store the function results, the results are (usually) printed out. To print an object, we can just type it’s name into the console. acs ## # A tibble: 1,456 x 7 ## med_house_income less_than_hs hs_grad some_coll bach white black ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 0.0252 0.196 0.221 0.325 0.897 0.0122 ## 2 69625 0.0577 0.253 0.316 0.262 0.885 0.0171 ## 3 70679 0.0936 0.173 0.273 0.267 0.733 0.0795 ## 4 74528 0.0843 0.253 0.353 0.231 0.824 0.0306 ## 5 52885 0.145 0.310 0.283 0.168 0.737 0.0605 ## 6 64100 0.0946 0.294 0.317 0.192 0.966 0.00256 ## 7 37093 0.253 0.394 0.235 0.101 0.711 0.0770 ## 8 87750 0.0768 0.187 0.185 0.272 0.759 0.0310 ## 9 97417 0.0625 0.254 0.227 0.284 0.969 0.00710 ## 10 43384 0.207 0.362 0.262 0.124 0.460 0.105 ## # … with 1,446 more rows This is sometimes a little overwhelming of a view. For previewing data, the function dplyr::glimpse() (there is the namespace notation again) is a great option. Try using the function glimpse() with the first argument being the acs object. glimpse(acs) ## Observations: 1,456 ## Variables: 7 ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, 3709… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426318,… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298192,… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.2… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124279,… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.7… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.030640… 7.2.1 Exercise Recreate a plot from the previous chapter using our newly imported acs tibble object. ggplot(acs, aes(x = bach, y = med_house_income)) + geom_point(alpha = 0.25) + labs(x = &quot;% of population with a Bachelor&#39;s Degree&quot;, y = &quot;Median Household Income&quot;, title = &quot;Relationship between Education and Income&quot;) 7.3 Other common data formats While csv files are going to be the most ubiquitous, you will invariably run into other data formats. The workflow is almost always the same. If you want to read excel files, you can use the function readxl::read_excel() from the readxl package. acs_xl &lt;- readxl::read_excel(&quot;data/acs_edu.xlsx&quot;) glimpse(acs_xl) ## Observations: 1,456 ## Variables: 7 ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, 3709… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426318,… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298192,… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052, 0.2… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124279,… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779, 0.7… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.030640… Another common format is a tsv which stands for tab separated format. readr::read_tsv() will be able to assist you here. If for some reason there are special delimiters like |, the readr::read_delim() function will work best. For example readr::read_delim(&quot;file-path&quot;, delim = &quot;|&quot;) would do the trick! Additionally, another extremely common data type is json which is short for javascript object notation. json is a data type that you will usually not read directly from a text file but interact with from an API. If you do happen to encounter a json flat text file, use the jsonlite package. jsonlite::read_json(). In the next chapter we will begin to work with dplyr and start getting hands on with our data. "],
["general-data-manipulation.html", "Chapter 8 General data manipulation 8.1 Scenario: 8.2 Getting physical with the data 8.3 Selecting Rows 8.4 filter 8.5 mutate 8.6 asking good questions 8.7 writing data lol", " Chapter 8 General data manipulation Now that you have the ability to read in data, it is important that you get comfortable handling it. Some people call the process of rearranging, cleaning, and reshaping data massaging, plumbing, engineering, and myriad other names. Here, we will refer to this as data manipulation. This is a preferrable catch-all term that does not ilicit images of toilets or Phoebe Buffay (she was a masseuse!). You may have heard of the 80/20 rule, or at least one of the many 80/20 rules. The 80/20 rule I’m referring to, is the idea that data scientists will spend 80% or more of their time cleaning and manipulating their data. The other 20% is the analysis part—creating statistics and models. I mention this because working with data is mostly data manipulation and only some statistics. Be prepared to get your hands dirty with data—euphamisms like this is probably why the phrase data plumbing ever came about. In this chapter we will learn how to preview data, select columns, arrange rows, and filter rows. This is where we get hands on with our data. This is just about as physical as we will be able to get unless you want to open up the raw csv and edit the data there! 8.0.0.1 CHECK THIS LATER Note: I do not recommend editing any csvs directly. That statement was in jest. We will cover the concept of reproducibility. This is all to say that you will find yourself with messy, unsanitized, gross, not fun to look at data most of the time. Because of this, it is really important that we have the skills to clean our data. Right now we’re going to go over the foundational skills we will learn how to select columns and rows, filter data, and create new columns, and arrange our data. To do this, we will be using the dplyr package from the tidyverse. The data we read in in the last chapter was only a select few variables from the annual census data release that the team at the Boston Area Research Initiative (BARI) provides. These census indicators are used to provide a picture into the changing landscape of Boston and Massachussettes more gnerally. In this chapter we will work through a rather real life scenario that you may very well encounter using the BARI data. 8.1 Scenario: A local non-profit is interested in the commuting behavior of Greater Boston residents. Your advisor suggested that you assist the non-profit in their work. You’ve just had coffee with the project manager to learn more about what their specific research question is. It seems that the extension of the Green Line is of great interest to them. They spoke at length about the history of the Big Dig and its impact on commuters working in the city. They look at their watch and realize they’re about to miss the commuter train home. You shake their hand, thank them for their time (and the free coffee because you’re a grad student), and affirm that you will email them in a week or so with some data for them to work with. https://www.mass.gov/green-line-extension-project-glx https://en.wikipedia.org/wiki/Big_Dig You’re back at your apartment, french press next your laptop, though not too close, notes open, and ready to begin. You review your notes and realize, while you now have a rather good understanding of what the Green Line Extensions is and the impact that the Big Dig had, you really have no idea what about commuting behavior in Greater Boston they are interested in. You realize you did not even confirm what constitutes the Greater Boston area. You push down the coffee grinds and pour your first cup of coffee. This will take at least two cups of coffee. The above scenario sounds like something out of a stress dream. This is scenario that I have found myself in many times and I am sure that you will find yourself in at one point as well. The more comfortable you get with data analysis and asking good questions, the more guided and directed you can make these seemingly vague objectives. At the end of this chapter, we will expound upon ways to prevent this in the future. 8.2 Getting physical with the data The data we used in both chapters one and two were curated from the annual census indicator release from BARI. This is the dataset from which acs_edu was created. We will use these data to provide relevant data relating to commuting in the Greater Boston Area. To do so, we will be using the tidyverse to read in and manipulate our data (as we did last chapter). Recall that we will load the tidyverse using library(tidyverse). Refresher: the tidyverse is a collection of packages used for data analysis. When you load the tidyverse it loads readr, ggplot2, and dplyr for us, among other packages. For now, though, these are the only relevant packages. Try it: Load the tidyverse Read in the file ACS_1317_TRACT.csv located in the data directory, store it in an object called acs_raw library(tidyverse) acs_raw &lt;- read_csv(&quot;data/ACS_1317_TRACT.csv&quot;) Wonderful! You’ve got the hang of reading data in which is truly no small feat. Once we have the data accessible from R, it is important to get familiar with what the data are. This means we need to know which variables are available to us and get a feel for what the values in those variables represent. Try printing out the acs_raw object in your console. acs_raw Oof, yikes. It’s a bit messy! Not to mention that R did not even print out all of the columns. That’s because it ran out of room. When we’re working with wide data (many columns), it’s generally best to view only a preview of the data. The function dplyr::glimpse() can help us do just that. Provide acs_raw as the only argument to glimpse(). glimpse(acs_raw) ## Observations: 1,478 ## Variables: 59 ## $ ct_id_10 &lt;dbl&gt; 25027728100, 25027729200, 25027730700, 2502… ## $ name &lt;chr&gt; &quot;Census Tract 7281, Worcester County, Massa… ## $ total_pop &lt;dbl&gt; 4585, 2165, 6917, 7278, 5059, 6632, 3259, 2… ## $ pop_den &lt;dbl&gt; 332.5741, 1069.6977, 2112.9526, 1345.5905, … ## $ sex_ratio &lt;dbl&gt; 1.1315667, 1.3179872, 1.1329016, 1.1156977,… ## $ age_u18 &lt;dbl&gt; 0.2340240, 0.1810624, 0.1705942, 0.2033526,… ## $ age1834 &lt;dbl&gt; 0.2023991, 0.1510393, 0.2143993, 0.2272602,… ## $ age3564 &lt;dbl&gt; 0.3980371, 0.4609700, 0.4371838, 0.4359714,… ## $ age_o65 &lt;dbl&gt; 0.1655398, 0.2069284, 0.1778228, 0.1334158,… ## $ for_born &lt;dbl&gt; 0.04383860, 0.08729792, 0.20586960, 0.15526… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779,… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.03… ## $ asian &lt;dbl&gt; 0.006324973, 0.021709007, 0.019950846, 0.03… ## $ hispanic &lt;dbl&gt; 0.070229008, 0.047575058, 0.136330779, 0.07… ## $ two_or_more &lt;dbl&gt; 0.012431843, 0.027713626, 0.017348562, 0.01… ## $ eth_het &lt;dbl&gt; -17032656, -3685242, -26905553, -36365806, … ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, … ## $ pub_assist &lt;dbl&gt; 0.020782396, 0.004667445, 0.021067926, 0.03… ## $ gini &lt;dbl&gt; 0.4084, 0.3886, 0.4693, 0.4052, 0.5327, 0.4… ## $ fam_pov_per &lt;dbl&gt; 0.04754601, 0.06521739, 0.05843440, 0.02486… ## $ unemp_rate &lt;dbl&gt; 0.03361345, 0.03127572, 0.06124498, 0.03983… ## $ total_house_h &lt;dbl&gt; 1636, 857, 2753, 2878, 2326, 2635, 1245, 80… ## $ fam_house_per &lt;dbl&gt; 0.7970660, 0.6977830, 0.6589175, 0.6567060,… ## $ fem_head_per &lt;dbl&gt; 0.08985330, 0.12018670, 0.11442063, 0.12126… ## $ same_sex_coup_per &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000… ## $ grand_head_per &lt;dbl&gt; 0.000000000, 0.005834306, 0.000000000, 0.00… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052,… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124… ## $ master &lt;dbl&gt; 0.15942502, 0.09705160, 0.12059807, 0.06714… ## $ prof &lt;dbl&gt; 0.0405096374, 0.0098280098, 0.0397403108, 0… ## $ doc &lt;dbl&gt; 0.033322444, 0.004914005, 0.033248082, 0.00… ## $ commute_less10 &lt;dbl&gt; 0.07874016, 0.25109361, 0.05369894, 0.14119… ## $ commute1030 &lt;dbl&gt; 0.5131234, 0.4636920, 0.6620965, 0.3941685,… ## $ commute3060 &lt;dbl&gt; 0.33114611, 0.18110236, 0.17709226, 0.31263… ## $ commute6090 &lt;dbl&gt; 0.055555556, 0.082239720, 0.086832334, 0.13… ## $ commute_over90 &lt;dbl&gt; 0.021434821, 0.021872266, 0.020279920, 0.01… ## $ by_auto &lt;dbl&gt; 0.9267791, 0.9704861, 0.9248285, 0.9037018,… ## $ by_pub_trans &lt;dbl&gt; 0.000000000, 0.004340278, 0.020301783, 0.01… ## $ by_bike &lt;dbl&gt; 0.002879473, 0.000000000, 0.000000000, 0.00… ## $ by_walk &lt;dbl&gt; 0.002468120, 0.012152778, 0.004938272, 0.02… ## $ total_house_units &lt;dbl&gt; 1701, 886, 2928, 3181, 2511, 2713, 1581, 85… ## $ vacant_unit_per &lt;dbl&gt; 0.03821282, 0.03273138, 0.05976776, 0.09525… ## $ renters_per &lt;dbl&gt; 0.1088020, 0.2100350, 0.2804214, 0.3425990,… ## $ home_own_per &lt;dbl&gt; 0.8911980, 0.7899650, 0.7195786, 0.6574010,… ## $ med_gross_rent &lt;dbl&gt; 1640, 894, 1454, 954, 1018, 867, 910, 1088,… ## $ med_home_val &lt;dbl&gt; 349000, 230200, 207200, 268400, 223200, 232… ## $ med_yr_built_raw &lt;dbl&gt; 1988, 1955, 1959, 1973, 1964, 1966, 1939, 1… ## $ med_yr_built &lt;chr&gt; &quot;1980 to 1989&quot;, &quot;1950 to 1959&quot;, &quot;1950 to 19… ## $ med_yr_moved_inraw &lt;dbl&gt; 2004, 2003, 2007, 2006, 2006, 2000, 2011, 2… ## $ med_yr_rent_moved_in &lt;dbl&gt; 2012, 2010, 2012, 2011, 2011, 2009, 2012, 2… ## $ area_acres &lt;dbl&gt; 9407.9167, 1294.2828, 2123.4927, 3581.9026,… ## $ town_id &lt;dbl&gt; 134, 321, 348, 185, 153, 151, 316, 348, 28,… ## $ town &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… ## $ fips_stco &lt;dbl&gt; 25027, 25027, 25027, 25027, 25027, 25027, 2… ## $ county &lt;chr&gt; &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WOR… ## $ area_acr_1 &lt;dbl&gt; 23241.514, 8867.508, 24609.965, 9615.644, 1… ## $ m_atown &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… Much better, right? It is frankly still a lot of text, but the way it is presented is rather useful. Each variable is written followed by its data type, i.e. &lt;dbl&gt;, and then a preview of values in that column. If the &lt;dbl&gt; does not make sense yet, do not worry. We will go over data types in depth later. Data types are not the most fun and I think it is important we have fun! acs_raw is the dataset from which acs_edu was created. As you can see, there are many, many, many different variables that the ACS data provide us with. These are only the tip of the iceberg. Pause Now have a think. Looking at the preview of these data, which columns do you think will be most useful to the non-profit for understanding commuter behavior? Note: “all of them” is not always the best answer. By providing too much data one may moved to inaction because they now determine what variables are the most useful and how to use them. HAVE SOURCES ON PARADOX OF CHOICE AND INFORMATION OVERLOAD. If you spotted the columns commute_less10, commute1030, commute3060, commute6090, and commute_over90, your eyes and intuition have served you well! These variables tell us about what proportion of the sampled population in a given census tract have commute times that fall within the indicated duration range, i.e. 30-60. PERSONAL NOTE: USE COMMUTE VARIABLES FOR PIVOTING AND THEN SEPARATING INTO NEW VARS 8.2.1 select()ing So now we have an intuition of the most important variables, but the next problem soon arises: how do we isolate just these variables? Whenever you find yourself needing to select or deselect columns from a tibble dplyr::select() will be the main function that you will go to. What does it do?: select() selects variables from a tibble and always returns another tibble. Before we work through how to use select(), refer to the help documentation and see if you can get somewhat of an intuition by typing ?select() into the console. Once you press enter the documentation page should pop up in RStudio. There are a few reasons why I am directing you towards the function documentation. To get you comfortable with navigating the RStudio IDE Expose you to the R vocabulary Soon you’ll be too advanced for this book and will have to figure out the way functions work on your own! Perhaps the help documentation was a little overwhelming and absolutely confusing. That’s okay. It’s just an exposure! With each exposure things will make more sense. Let’s tackle these arguments one by one. .data: A tbl. All main verbs are S3 generics and provide methods for tbl_df(), dtplyr::tbl_dt() and dbplyr::tbl_dbi(). What I want you to take away from this argument definition is a tbl. Whenever you read tbl think to your self “oh, that is just a tibble.” If you recall, when we read rectangular data with readr::read_csv() or any other readr::read_*() function we will end up with a tibble. To verify that this is the case, we can double check our objects using the function is.tbl(). This function takes an object and returns a logical value (TRUE or FALSE) if the statement is true. Let’s double check that acs_raw is in fact a tbl. is.tbl(acs_raw) ## [1] TRUE Aside: Each object type usually has a function that let’s you test if objects are that type that follow the structure is.obj_type() or the occasional is_obj_type(). We will go over object types more later. We can read the above as if we are asking R the question “is this object a tbl?” The resultant output of is.tbl(acs_raw) is TRUE. Now we can be doubly confident that this object can be used with select(). The second argument to select() is a little bit more difficult to grasp, so don’t feel discouraged if this isn’t clicking right away. There is a lot written in this argument definition and I feel that not all of it is necessary to understand from the get go. ...: One or more unquoted expressions separated by commas. You can treat variable names like they are positions, so you can use expressions like x:y to select ranges of variables. ..., referred to as “dots” means that we can pass any number of arguments to the function. Translating “one or more unquoted expressions separated by commas” into regular person speak reiterates that there can be multiple other arguments passed into select(). “Unquoted expressions” means that if we want to select a column we do not put that column name in quotes. “You can treat variable names like they are positions” translates to “if you want the first column you can write the number 1 etc.” and because of this, if you want the first through tenth variable you can pass 1:10 as an argument to .... The most important thing about ... is that we do not assign ... as an argument, for example . ... = column_a is not the correct notation. We provide column_a alone. As always, this makes more sense once we see it in practice. We will now go over the many ways in which we can select columns using select(). Once we have gotten the hang of selecting columns we will return back to assisting our non-profit. We will go over: selecting by name selecting by position select helpers 8.2.2 select()ing exercises select() enables us to choose columns from a tibble based on their names. But remember that these will be unquoted column names. Try it: select the column name from acs_raw select(acs_raw, name) ## # A tibble: 1,478 x 1 ## name ## &lt;chr&gt; ## 1 Census Tract 7281, Worcester County, Massachusetts ## 2 Census Tract 7292, Worcester County, Massachusetts ## 3 Census Tract 7307, Worcester County, Massachusetts ## 4 Census Tract 7442, Worcester County, Massachusetts ## 5 Census Tract 7097.01, Worcester County, Massachusetts ## 6 Census Tract 7351, Worcester County, Massachusetts ## 7 Census Tract 7543, Worcester County, Massachusetts ## 8 Census Tract 7308.02, Worcester County, Massachusetts ## 9 Census Tract 7171, Worcester County, Massachusetts ## 10 Census Tract 7326, Worcester County, Massachusetts ## # … with 1,468 more rows The column name was passed to .... Recall that dots allows us to pass “one ore more unquoted expressions separated by commas.” To test this statement out, select town in addition to name from acs_raw Try it: select name and town from acs_raw select(acs_raw, name, town) ## # A tibble: 1,478 x 2 ## name town ## &lt;chr&gt; &lt;chr&gt; ## 1 Census Tract 7281, Worcester County, Massachusetts HOLDEN ## 2 Census Tract 7292, Worcester County, Massachusetts WEST BOYLSTON ## 3 Census Tract 7307, Worcester County, Massachusetts WORCESTER ## 4 Census Tract 7442, Worcester County, Massachusetts MILFORD ## 5 Census Tract 7097.01, Worcester County, Massachusetts LEOMINSTER ## 6 Census Tract 7351, Worcester County, Massachusetts LEICESTER ## 7 Census Tract 7543, Worcester County, Massachusetts WEBSTER ## 8 Census Tract 7308.02, Worcester County, Massachusetts WORCESTER ## 9 Census Tract 7171, Worcester County, Massachusetts BERLIN ## 10 Census Tract 7326, Worcester County, Massachusetts WORCESTER ## # … with 1,468 more rows Great, you’re getting the hang of it. Now, in addition to to selecting columns solely based on their names, we can also select a range of columns using the format col_from:col_to. In writing this select() will register that you want every column from and including col_from up until and including col_to. Let’s refresh ourselves with what our data look like: glimpse(acs_raw) ## Observations: 1,478 ## Variables: 59 ## $ ct_id_10 &lt;dbl&gt; 25027728100, 25027729200, 25027730700, 2502… ## $ name &lt;chr&gt; &quot;Census Tract 7281, Worcester County, Massa… ## $ total_pop &lt;dbl&gt; 4585, 2165, 6917, 7278, 5059, 6632, 3259, 2… ## $ pop_den &lt;dbl&gt; 332.5741, 1069.6977, 2112.9526, 1345.5905, … ## $ sex_ratio &lt;dbl&gt; 1.1315667, 1.3179872, 1.1329016, 1.1156977,… ## $ age_u18 &lt;dbl&gt; 0.2340240, 0.1810624, 0.1705942, 0.2033526,… ## $ age1834 &lt;dbl&gt; 0.2023991, 0.1510393, 0.2143993, 0.2272602,… ## $ age3564 &lt;dbl&gt; 0.3980371, 0.4609700, 0.4371838, 0.4359714,… ## $ age_o65 &lt;dbl&gt; 0.1655398, 0.2069284, 0.1778228, 0.1334158,… ## $ for_born &lt;dbl&gt; 0.04383860, 0.08729792, 0.20586960, 0.15526… ## $ white &lt;dbl&gt; 0.8972737, 0.8849885, 0.7328322, 0.8235779,… ## $ black &lt;dbl&gt; 0.012213740, 0.017090069, 0.079514240, 0.03… ## $ asian &lt;dbl&gt; 0.006324973, 0.021709007, 0.019950846, 0.03… ## $ hispanic &lt;dbl&gt; 0.070229008, 0.047575058, 0.136330779, 0.07… ## $ two_or_more &lt;dbl&gt; 0.012431843, 0.027713626, 0.017348562, 0.01… ## $ eth_het &lt;dbl&gt; -17032656, -3685242, -26905553, -36365806, … ## $ med_house_income &lt;dbl&gt; 105735, 69625, 70679, 74528, 52885, 64100, … ## $ pub_assist &lt;dbl&gt; 0.020782396, 0.004667445, 0.021067926, 0.03… ## $ gini &lt;dbl&gt; 0.4084, 0.3886, 0.4693, 0.4052, 0.5327, 0.4… ## $ fam_pov_per &lt;dbl&gt; 0.04754601, 0.06521739, 0.05843440, 0.02486… ## $ unemp_rate &lt;dbl&gt; 0.03361345, 0.03127572, 0.06124498, 0.03983… ## $ total_house_h &lt;dbl&gt; 1636, 857, 2753, 2878, 2326, 2635, 1245, 80… ## $ fam_house_per &lt;dbl&gt; 0.7970660, 0.6977830, 0.6589175, 0.6567060,… ## $ fem_head_per &lt;dbl&gt; 0.08985330, 0.12018670, 0.11442063, 0.12126… ## $ same_sex_coup_per &lt;dbl&gt; 0.00000000, 0.00000000, 0.00000000, 0.00000… ## $ grand_head_per &lt;dbl&gt; 0.000000000, 0.005834306, 0.000000000, 0.00… ## $ less_than_hs &lt;dbl&gt; 0.02515518, 0.05773956, 0.09364548, 0.08426… ## $ hs_grad &lt;dbl&gt; 0.19568768, 0.25307125, 0.17332284, 0.25298… ## $ some_coll &lt;dbl&gt; 0.2211696, 0.3157248, 0.2726736, 0.3534052,… ## $ bach &lt;dbl&gt; 0.32473048, 0.26167076, 0.26677159, 0.23124… ## $ master &lt;dbl&gt; 0.15942502, 0.09705160, 0.12059807, 0.06714… ## $ prof &lt;dbl&gt; 0.0405096374, 0.0098280098, 0.0397403108, 0… ## $ doc &lt;dbl&gt; 0.033322444, 0.004914005, 0.033248082, 0.00… ## $ commute_less10 &lt;dbl&gt; 0.07874016, 0.25109361, 0.05369894, 0.14119… ## $ commute1030 &lt;dbl&gt; 0.5131234, 0.4636920, 0.6620965, 0.3941685,… ## $ commute3060 &lt;dbl&gt; 0.33114611, 0.18110236, 0.17709226, 0.31263… ## $ commute6090 &lt;dbl&gt; 0.055555556, 0.082239720, 0.086832334, 0.13… ## $ commute_over90 &lt;dbl&gt; 0.021434821, 0.021872266, 0.020279920, 0.01… ## $ by_auto &lt;dbl&gt; 0.9267791, 0.9704861, 0.9248285, 0.9037018,… ## $ by_pub_trans &lt;dbl&gt; 0.000000000, 0.004340278, 0.020301783, 0.01… ## $ by_bike &lt;dbl&gt; 0.002879473, 0.000000000, 0.000000000, 0.00… ## $ by_walk &lt;dbl&gt; 0.002468120, 0.012152778, 0.004938272, 0.02… ## $ total_house_units &lt;dbl&gt; 1701, 886, 2928, 3181, 2511, 2713, 1581, 85… ## $ vacant_unit_per &lt;dbl&gt; 0.03821282, 0.03273138, 0.05976776, 0.09525… ## $ renters_per &lt;dbl&gt; 0.1088020, 0.2100350, 0.2804214, 0.3425990,… ## $ home_own_per &lt;dbl&gt; 0.8911980, 0.7899650, 0.7195786, 0.6574010,… ## $ med_gross_rent &lt;dbl&gt; 1640, 894, 1454, 954, 1018, 867, 910, 1088,… ## $ med_home_val &lt;dbl&gt; 349000, 230200, 207200, 268400, 223200, 232… ## $ med_yr_built_raw &lt;dbl&gt; 1988, 1955, 1959, 1973, 1964, 1966, 1939, 1… ## $ med_yr_built &lt;chr&gt; &quot;1980 to 1989&quot;, &quot;1950 to 1959&quot;, &quot;1950 to 19… ## $ med_yr_moved_inraw &lt;dbl&gt; 2004, 2003, 2007, 2006, 2006, 2000, 2011, 2… ## $ med_yr_rent_moved_in &lt;dbl&gt; 2012, 2010, 2012, 2011, 2011, 2009, 2012, 2… ## $ area_acres &lt;dbl&gt; 9407.9167, 1294.2828, 2123.4927, 3581.9026,… ## $ town_id &lt;dbl&gt; 134, 321, 348, 185, 153, 151, 316, 348, 28,… ## $ town &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… ## $ fips_stco &lt;dbl&gt; 25027, 25027, 25027, 25027, 25027, 25027, 2… ## $ county &lt;chr&gt; &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WORCESTER&quot;, &quot;WOR… ## $ area_acr_1 &lt;dbl&gt; 23241.514, 8867.508, 24609.965, 9615.644, 1… ## $ m_atown &lt;chr&gt; &quot;HOLDEN&quot;, &quot;WEST BOYLSTON&quot;, &quot;WORCESTER&quot;, &quot;MI… Try it: select the columns age_u18 through age_o65. select(acs_raw, age_u18:age_o65) ## # A tibble: 1,478 x 4 ## age_u18 age1834 age3564 age_o65 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.202 0.398 0.166 ## 2 0.181 0.151 0.461 0.207 ## 3 0.171 0.214 0.437 0.178 ## 4 0.203 0.227 0.436 0.133 ## 5 0.177 0.203 0.430 0.190 ## 6 0.163 0.237 0.439 0.162 ## 7 0.191 0.326 0.380 0.102 ## 8 0.202 0.183 0.466 0.148 ## 9 0.188 0.150 0.462 0.200 ## 10 0.244 0.286 0.342 0.128 ## # … with 1,468 more rows Now to really throw you off! You can even reverse the order of these ranges. Try it: select columns from age_o65 to age_u18. select(acs_raw, age_o65:age_u18) ## # A tibble: 1,478 x 4 ## age_o65 age3564 age1834 age_u18 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.166 0.398 0.202 0.234 ## 2 0.207 0.461 0.151 0.181 ## 3 0.178 0.437 0.214 0.171 ## 4 0.133 0.436 0.227 0.203 ## 5 0.190 0.430 0.203 0.177 ## 6 0.162 0.439 0.237 0.163 ## 7 0.102 0.380 0.326 0.191 ## 8 0.148 0.466 0.183 0.202 ## 9 0.200 0.462 0.150 0.188 ## 10 0.128 0.342 0.286 0.244 ## # … with 1,468 more rows NOTE: This is an awful scenario. SOMEONE HALP Alright, so now we have gotten the hang of selecting columns based on their names. But equally important is the ability to select columns based on their position. Consider the situation in which you regularly receive georeferenced data from a research partner and the structure of the dataset is rather consistent except that they frequently change the name of the coordinate columns. Sometimes the columns are x and y. Sometimes they are capitalized X and Y, lon and lat, or even long and lat. It eats you up inside! But you know that while the names may change, their positiions never do. You decide to program a solution rather than having a conversation with your research partner—though, I recommend you both level set on reproducibility standards. “…You can treat variable names like they are positions…” The above was taken from the argument definition of dots .... Like providing the name of the column, we can also provide their positions (also referred to as an index). In our previous example, we selected the name column. We can select this column by it’s position. name is the second column in our tibble. We select it like so: select(acs_raw, 2) ## # A tibble: 1,478 x 1 ## name ## &lt;chr&gt; ## 1 Census Tract 7281, Worcester County, Massachusetts ## 2 Census Tract 7292, Worcester County, Massachusetts ## 3 Census Tract 7307, Worcester County, Massachusetts ## 4 Census Tract 7442, Worcester County, Massachusetts ## 5 Census Tract 7097.01, Worcester County, Massachusetts ## 6 Census Tract 7351, Worcester County, Massachusetts ## 7 Census Tract 7543, Worcester County, Massachusetts ## 8 Census Tract 7308.02, Worcester County, Massachusetts ## 9 Census Tract 7171, Worcester County, Massachusetts ## 10 Census Tract 7326, Worcester County, Massachusetts ## # … with 1,468 more rows Try it: select age_u18 and age_o65 by their position select(acs_raw, 6, 9) ## # A tibble: 1,478 x 2 ## age_u18 age_o65 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.166 ## 2 0.181 0.207 ## 3 0.171 0.178 ## 4 0.203 0.133 ## 5 0.177 0.190 ## 6 0.163 0.162 ## 7 0.191 0.102 ## 8 0.202 0.148 ## 9 0.188 0.200 ## 10 0.244 0.128 ## # … with 1,468 more rows You may see where I am going with this. Just like column names, we can select a range of columns using the same method index_from:index_to. Try it: select the columns from age_u18 to age_o65 using : and the column position select the columns in reverse order by their indexes select(acs_raw, 6:9) ## # A tibble: 1,478 x 4 ## age_u18 age1834 age3564 age_o65 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.234 0.202 0.398 0.166 ## 2 0.181 0.151 0.461 0.207 ## 3 0.171 0.214 0.437 0.178 ## 4 0.203 0.227 0.436 0.133 ## 5 0.177 0.203 0.430 0.190 ## 6 0.163 0.237 0.439 0.162 ## 7 0.191 0.326 0.380 0.102 ## 8 0.202 0.183 0.466 0.148 ## 9 0.188 0.150 0.462 0.200 ## 10 0.244 0.286 0.342 0.128 ## # … with 1,468 more rows select(acs_raw, 9:6) ## # A tibble: 1,478 x 4 ## age_o65 age3564 age1834 age_u18 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.166 0.398 0.202 0.234 ## 2 0.207 0.461 0.151 0.181 ## 3 0.178 0.437 0.214 0.171 ## 4 0.133 0.436 0.227 0.203 ## 5 0.190 0.430 0.203 0.177 ## 6 0.162 0.439 0.237 0.163 ## 7 0.102 0.380 0.326 0.191 ## 8 0.148 0.466 0.183 0.202 ## 9 0.200 0.462 0.150 0.188 ## 10 0.128 0.342 0.286 0.244 ## # … with 1,468 more rows Base R Side Bar: To help build your intuition, I want to point out some base R functionality. Using the colon : with integers (whole numbers) is actually not a select() specific functionality. This is something that is rather handy and built directly into R. Using the colon operator, we can create ranges of numbers in the same exact way as we did above. If we want create the range of numbers from 1 to 10, we write 1:10. to do so we use the function select(). Look at the help documentation by running ?select() inside of the console. What are the arguments? - .data and .... We’ll tackle this one by one. - .data: “A tbl.” - read this as “A tibble, or data frame”. - for those of you who are curious, tbl is the formal object class of a tibble. a tibble is still a data frame - this means that the first argument will be passed the acs_raw object - ...: One or more unquoted expressions separated by commas. You can treat variable names like they are positions, so you can use expressions like x:y to select ranges of variables. this one is a little bit more complex. ... referred to as “dots”. This really means we can pass any number of other arguments to select() here the ... will be ways of referring to the columns we want. and there are a number of ways we can referrer to which columns we want we can: write the name (unquoted) of the columns—i.e. i want the column col_x would be select(.data, col_x) write the position of the column—i.e. i want the first column would be select(.data, 1) There are also a number of select helpers of which we will go over a few everything() deselecting one behavior that you should notice is that select will always return a tibble, even if we don’t sepcify any columns to select 8.2.3 select() exercises try using the select function on acs_raw without passing any column specifications to ... select(acs_raw) select the median house hold income column (med_house_income) select(acs_raw, med_house_income) ## # A tibble: 1,478 x 1 ## med_house_income ## &lt;dbl&gt; ## 1 105735 ## 2 69625 ## 3 70679 ## 4 74528 ## 5 52885 ## 6 64100 ## 7 37093 ## 8 87750 ## 9 97417 ## 10 43384 ## # … with 1,468 more rows as the documentation notes we can select a range of columns using col_x:col_y select the columns relating to education from less than high school to doctoral select(acs_raw, less_than_hs:doc) ## # A tibble: 1,478 x 7 ## less_than_hs hs_grad some_coll bach master prof doc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0252 0.196 0.221 0.325 0.159 0.0405 0.0333 ## 2 0.0577 0.253 0.316 0.262 0.0971 0.00983 0.00491 ## 3 0.0936 0.173 0.273 0.267 0.121 0.0397 0.0332 ## 4 0.0843 0.253 0.353 0.231 0.0671 0.00616 0.00481 ## 5 0.145 0.310 0.283 0.168 0.0879 0.00343 0.00158 ## 6 0.0946 0.294 0.317 0.192 0.0858 0.0161 0 ## 7 0.253 0.394 0.235 0.101 0.0155 0.000484 0 ## 8 0.0768 0.187 0.185 0.272 0.145 0.0569 0.0782 ## 9 0.0625 0.254 0.227 0.284 0.127 0.0223 0.0236 ## 10 0.207 0.362 0.262 0.124 0.0353 0 0.00972 ## # … with 1,468 more rows though you will likely not use it too often, it’s still important to be comfortable with column index (position) selecting select the 1st, 5th, and 10th columns select(acs_raw, 1, 5, 10) ## # A tibble: 1,478 x 3 ## ct_id_10 sex_ratio for_born ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 25027728100 1.13 0.0438 ## 2 25027729200 1.32 0.0873 ## 3 25027730700 1.13 0.206 ## 4 25027744200 1.12 0.155 ## 5 25027709701 1.30 0.106 ## 6 25027735100 1.11 0.0280 ## 7 25027754300 1.25 0.188 ## 8 25027730802 0.908 0.198 ## 9 25027717100 0.990 0.0797 ## 10 25027732600 1.19 0.210 ## # … with 1,468 more rows 8.2.3.1 tidyselect helpers we use the tidyselect helpers in the ... argument starts_with(): a string to search that columns start with find all columns that start with &quot;med&quot; select(acs_raw, starts_with(&quot;med&quot;)) ## # A tibble: 1,478 x 7 ## med_house_income med_gross_rent med_home_val med_yr_built_raw ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 105735 1640 349000 1988 ## 2 69625 894 230200 1955 ## 3 70679 1454 207200 1959 ## 4 74528 954 268400 1973 ## 5 52885 1018 223200 1964 ## 6 64100 867 232700 1966 ## 7 37093 910 170900 1939 ## 8 87750 1088 270100 1939 ## 9 97417 1037 379600 1981 ## 10 43384 1017 156500 1939 ## # … with 1,468 more rows, and 3 more variables: med_yr_built &lt;chr&gt;, ## # med_yr_moved_inraw &lt;dbl&gt;, med_yr_rent_moved_in &lt;dbl&gt; ends_with(): a string to search that columns end with select columns that end with &quot;per&quot; select(acs_raw, ends_with(&quot;per&quot;)) ## # A tibble: 1,478 x 8 ## fam_pov_per fam_house_per fem_head_per same_sex_coup_p… grand_head_per ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.0475 0.797 0.0899 0 0 ## 2 0.0652 0.698 0.120 0 0.00583 ## 3 0.0584 0.659 0.114 0 0 ## 4 0.0249 0.657 0.121 0 0 ## 5 0.198 0.531 0.158 0 0.00946 ## 6 0.0428 0.665 0.0603 0 0.0353 ## 7 0.0762 0.632 0.227 0 0.00643 ## 8 0.101 0.636 0.0582 0.297 0.0260 ## 9 0.0149 0.758 0.0721 0 0.00434 ## 10 0.0954 0.460 0.225 0 0.0279 ## # … with 1,468 more rows, and 3 more variables: vacant_unit_per &lt;dbl&gt;, ## # renters_per &lt;dbl&gt;, home_own_per &lt;dbl&gt; contains(): a string to search for in the column headers the string that we are searching for can be at any position find any column that contains the string &quot;yr&quot; select(acs_raw, contains(&quot;yr&quot;)) ## # A tibble: 1,478 x 4 ## med_yr_built_raw med_yr_built med_yr_moved_inraw med_yr_rent_moved_in ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1988 1980 to 1989 2004 2012 ## 2 1955 1950 to 1959 2003 2010 ## 3 1959 1950 to 1959 2007 2012 ## 4 1973 1970 to 1979 2006 2011 ## 5 1964 1960 to 1969 2006 2011 ## 6 1966 1960 to 1969 2000 2009 ## 7 1939 Prior to 1940 2011 2012 ## 8 1939 Prior to 1940 2006 2012 ## 9 1981 1980 to 1989 2004 2012 ## 10 1939 Prior to 1940 2011 NA ## # … with 1,468 more rows everything(): helpful when you want to move some columns to the front and dont care about the order of others takes no arguments select the town, county, then everything else select(acs_raw, town, county, everything()) ## # A tibble: 1,478 x 59 ## town county ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 HOLD… WORCE… 2.50e10 Cens… 4585 333. 1.13 0.234 0.202 ## 2 WEST… WORCE… 2.50e10 Cens… 2165 1070. 1.32 0.181 0.151 ## 3 WORC… WORCE… 2.50e10 Cens… 6917 2113. 1.13 0.171 0.214 ## 4 MILF… WORCE… 2.50e10 Cens… 7278 1346. 1.12 0.203 0.227 ## 5 LEOM… WORCE… 2.50e10 Cens… 5059 2894. 1.30 0.177 0.203 ## 6 LEIC… WORCE… 2.50e10 Cens… 6632 472. 1.11 0.163 0.237 ## 7 WEBS… WORCE… 2.50e10 Cens… 3259 8022. 1.25 0.191 0.326 ## 8 WORC… WORCE… 2.50e10 Cens… 2097 5191. 0.908 0.202 0.183 ## 9 BERL… WORCE… 2.50e10 Cens… 3098 239. 0.990 0.188 0.150 ## 10 WORC… WORCE… 2.50e10 Cens… 3982 17065. 1.19 0.244 0.286 ## # … with 1,468 more rows, and 50 more variables: age3564 &lt;dbl&gt;, ## # age_o65 &lt;dbl&gt;, for_born &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;, asian &lt;dbl&gt;, ## # hispanic &lt;dbl&gt;, two_or_more &lt;dbl&gt;, eth_het &lt;dbl&gt;, ## # med_house_income &lt;dbl&gt;, pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, ## # fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, total_house_h &lt;dbl&gt;, ## # fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, same_sex_coup_per &lt;dbl&gt;, ## # grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, hs_grad &lt;dbl&gt;, ## # some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, doc &lt;dbl&gt;, ## # commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # fips_stco &lt;dbl&gt;, area_acr_1 &lt;dbl&gt;, m_atown &lt;chr&gt; 8.3 Selecting Rows like we can select columns we can also select rows. however rows do not have names. we select the rows based on position unlike select, if we do not specify any arguments to ... it will return the entire data frame slice(acs_raw) ## # A tibble: 1,478 x 59 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 4585 333. 1.13 0.234 0.202 0.398 ## 2 2.50e10 Cens… 2165 1070. 1.32 0.181 0.151 0.461 ## 3 2.50e10 Cens… 6917 2113. 1.13 0.171 0.214 0.437 ## 4 2.50e10 Cens… 7278 1346. 1.12 0.203 0.227 0.436 ## 5 2.50e10 Cens… 5059 2894. 1.30 0.177 0.203 0.430 ## 6 2.50e10 Cens… 6632 472. 1.11 0.163 0.237 0.439 ## 7 2.50e10 Cens… 3259 8022. 1.25 0.191 0.326 0.380 ## 8 2.50e10 Cens… 2097 5191. 0.908 0.202 0.183 0.466 ## 9 2.50e10 Cens… 3098 239. 0.990 0.188 0.150 0.462 ## 10 2.50e10 Cens… 3982 17065. 1.19 0.244 0.286 0.342 ## # … with 1,468 more rows, and 51 more variables: age_o65 &lt;dbl&gt;, ## # for_born &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, ## # two_or_more &lt;dbl&gt;, eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, ## # pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, ## # total_house_h &lt;dbl&gt;, fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, ## # same_sex_coup_per &lt;dbl&gt;, grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, ## # hs_grad &lt;dbl&gt;, some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, ## # doc &lt;dbl&gt;, commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt; positive integers slice(acs_raw) ## # A tibble: 1,478 x 59 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 4585 333. 1.13 0.234 0.202 0.398 ## 2 2.50e10 Cens… 2165 1070. 1.32 0.181 0.151 0.461 ## 3 2.50e10 Cens… 6917 2113. 1.13 0.171 0.214 0.437 ## 4 2.50e10 Cens… 7278 1346. 1.12 0.203 0.227 0.436 ## 5 2.50e10 Cens… 5059 2894. 1.30 0.177 0.203 0.430 ## 6 2.50e10 Cens… 6632 472. 1.11 0.163 0.237 0.439 ## 7 2.50e10 Cens… 3259 8022. 1.25 0.191 0.326 0.380 ## 8 2.50e10 Cens… 2097 5191. 0.908 0.202 0.183 0.466 ## 9 2.50e10 Cens… 3098 239. 0.990 0.188 0.150 0.462 ## 10 2.50e10 Cens… 3982 17065. 1.19 0.244 0.286 0.342 ## # … with 1,468 more rows, and 51 more variables: age_o65 &lt;dbl&gt;, ## # for_born &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, ## # two_or_more &lt;dbl&gt;, eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, ## # pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, ## # total_house_h &lt;dbl&gt;, fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, ## # same_sex_coup_per &lt;dbl&gt;, grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, ## # hs_grad &lt;dbl&gt;, some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, ## # doc &lt;dbl&gt;, commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt; the n() helper negative integers 8.4 filter logical operators 8.4.1 scenario we have now selected the columns we need to provide 8.5 mutate 8.5.1 scenario The non-profit has emailed you back and indicated that they want to report on the income quintiles and requested that you do this for them. You’re a rockstar and a kickass programmer so you’re like, hell yah. they also ask for: tract a combined measure of bach masters and doctoral acs_raw %&gt;% mutate(hh_inc_quin = ntile(med_house_income, 5)) ## # A tibble: 1,478 x 60 ## ct_id_10 name total_pop pop_den sex_ratio age_u18 age1834 age3564 ## &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 2.50e10 Cens… 4585 333. 1.13 0.234 0.202 0.398 ## 2 2.50e10 Cens… 2165 1070. 1.32 0.181 0.151 0.461 ## 3 2.50e10 Cens… 6917 2113. 1.13 0.171 0.214 0.437 ## 4 2.50e10 Cens… 7278 1346. 1.12 0.203 0.227 0.436 ## 5 2.50e10 Cens… 5059 2894. 1.30 0.177 0.203 0.430 ## 6 2.50e10 Cens… 6632 472. 1.11 0.163 0.237 0.439 ## 7 2.50e10 Cens… 3259 8022. 1.25 0.191 0.326 0.380 ## 8 2.50e10 Cens… 2097 5191. 0.908 0.202 0.183 0.466 ## 9 2.50e10 Cens… 3098 239. 0.990 0.188 0.150 0.462 ## 10 2.50e10 Cens… 3982 17065. 1.19 0.244 0.286 0.342 ## # … with 1,468 more rows, and 52 more variables: age_o65 &lt;dbl&gt;, ## # for_born &lt;dbl&gt;, white &lt;dbl&gt;, black &lt;dbl&gt;, asian &lt;dbl&gt;, hispanic &lt;dbl&gt;, ## # two_or_more &lt;dbl&gt;, eth_het &lt;dbl&gt;, med_house_income &lt;dbl&gt;, ## # pub_assist &lt;dbl&gt;, gini &lt;dbl&gt;, fam_pov_per &lt;dbl&gt;, unemp_rate &lt;dbl&gt;, ## # total_house_h &lt;dbl&gt;, fam_house_per &lt;dbl&gt;, fem_head_per &lt;dbl&gt;, ## # same_sex_coup_per &lt;dbl&gt;, grand_head_per &lt;dbl&gt;, less_than_hs &lt;dbl&gt;, ## # hs_grad &lt;dbl&gt;, some_coll &lt;dbl&gt;, bach &lt;dbl&gt;, master &lt;dbl&gt;, prof &lt;dbl&gt;, ## # doc &lt;dbl&gt;, commute_less10 &lt;dbl&gt;, commute1030 &lt;dbl&gt;, commute3060 &lt;dbl&gt;, ## # commute6090 &lt;dbl&gt;, commute_over90 &lt;dbl&gt;, by_auto &lt;dbl&gt;, ## # by_pub_trans &lt;dbl&gt;, by_bike &lt;dbl&gt;, by_walk &lt;dbl&gt;, ## # total_house_units &lt;dbl&gt;, vacant_unit_per &lt;dbl&gt;, renters_per &lt;dbl&gt;, ## # home_own_per &lt;dbl&gt;, med_gross_rent &lt;dbl&gt;, med_home_val &lt;dbl&gt;, ## # med_yr_built_raw &lt;dbl&gt;, med_yr_built &lt;chr&gt;, med_yr_moved_inraw &lt;dbl&gt;, ## # med_yr_rent_moved_in &lt;dbl&gt;, area_acres &lt;dbl&gt;, town_id &lt;dbl&gt;, ## # town &lt;chr&gt;, fips_stco &lt;dbl&gt;, county &lt;chr&gt;, area_acr_1 &lt;dbl&gt;, ## # m_atown &lt;chr&gt;, hh_inc_quin &lt;int&gt; i need to introduce measures of central tendency and summar stats in between this groups summarizing using the name column I can consider introducing string methods - this would probably be better done with text reviews or something 8.6 asking good questions 8.7 writing data lol 8.7.0.0.1 old notes They define the greater boston area as Suffolk, Middlesex, and Norfolk counties. before we go ahead and start cleaning this data, we need to learn the tools to do so. Please bear with me! recall how to load the tidyverse we’ll read in the ACS_1317_TRACT.csv file located in the data directory putting this together the file path is data/ACS_1317_TRACT.csv. store it in the variable acs_raw To learn these tools we will work with a role-play / workthrough. a local non-profit is interested in learning about the demographic characteristics of the greater boston area. They are specifically interested to learn more about the relationship between the age, race, and economic status. They’ve come to you to provide them with the relevant data. you have acces to the annual BARI census data and you will curate the data for them. "],
["visualizing-trends-and-relationships.html", "Chapter 9 Visualizing Trends and Relationships 9.1 Univariate visualizations 9.2 Bivariate visualizations 9.3 Expanding bivariate visualizations to trivariate &amp; other tri-variate 9.4 The Grammar of Layered Graphics", " Chapter 9 Visualizing Trends and Relationships library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() acs_messy &lt;- read_csv(&quot;data/ACS_1317_TRACT.csv&quot;) ## Parsed with column specification: ## cols( ## .default = col_double(), ## name = col_character(), ## med_yr_built = col_character(), ## town = col_character(), ## county = col_character(), ## m_atown = col_character() ## ) ## See spec(...) for full column specifications. acs &lt;- acs_messy %&gt;% separate(name, sep = &quot;, &quot;, into = c(&quot;tract&quot;, &quot;county&quot;, &quot;state&quot;)) %&gt;% mutate(tract = str_remove(tract, &quot;Census Tract &quot;)) %&gt;% na.omit() most data analyses start with a visualization. the data we have will dictate the type of visualizations we create there are many many different ways in which data can be represented generally these can be bucketed into a few major categories numeric integer double character think groups, factors, nominal, anything that doesn’t have a numeric value that makes sense to count, aggregate, etc. time / order 9.1 Univariate visualizations what are we looking for in univariate visualizations? the shape of the distribution measures of central tendency where do the data cluster? is there a center? more than one? how much variation is in the data? is the distribution flatter or steeper? 9.1.1 histogram puts data into n groups or bins or buckets. ggplot calls them bins you can identify how many obs fit into a bucket ggplot(acs, aes(age_u18)) + geom_histogram(bins = 15) 9.1.2 density plot A density plot is a representation of the distribution of a numeric variable visualizes the distribution of data over a continuous interval it’s called a density plot because it uses a kernel density. you do not need to know what this is, just that it shows a continuous representation of the distribution unlike histograms you cannot determine how many obs fall in a bucket as there are no buckets. ggplot(acs, aes(age_u18)) + geom_density() 9.1.3 box plot box plots are another way to show the distribution unlike histograms and density plots which show the shape of the distribution box plots are concerned with illustrating any potential outliers aside: an outlier is a value that differs substantially from other obs based on 5 summary values: the first quartile median (the middle value) the third quartile the minimum and the maximum: these aren’t actually the max and min, these are we have to set this aesthetic to y refs: https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51 ggplot(acs, aes(y = age_u18)) + geom_boxplot() its easier to evaluate a box plot when it’s horizontal. we can flip any ggplot with a coord_flip() layer ggplot(acs, aes(y = age_u18)) + geom_boxplot() + coord_flip() 9.1.3.1 understanding the box plot 9.2 Bivariate visualizations with bi-variate relationships we’re looking to answer, in general, if one variable affects the other. we usually will be comparing two numeric variables or one numeric and one categorical variable in the former situtation we’re looking to see if there is a related trend, i.e. when one goes up does the other go down or vice versa in the latter scenario, we want to know if the distribution of the data changes for different groups 9.2.1 scatter plot (2 continuous) we made one previously this is two continuous on each axis. the variable of interest is on the y example: we can hypothesize that where there are more under 18 there are more families we can ask how does the prop of population under 18 vary with prop of family households? when there are many points (which is often the case w/ big data) we can change the transparency (often called opacity) so we can see where there is the most overlap. Inside of the geom_point() we set the alpha argument to a value between 0 and 1 where 1 is not transparent and 0 is invisible. this is artistic preference and there is no one true answer ggplot(acs, aes(fam_house_per, age_u18)) + geom_point(alpha = 0.25) 9.2.2 boxplot (1 continuous 1 categorical) we can use boxplots to compare groups for this, we set the categorical variable to the x aesthetic ggplot(acs, aes(county, age_u18)) + geom_boxplot() + coord_flip() 9.2.3 barplot (1 categorical 1 continuous / discrete) the geom_bar() will count the number observations of the specified categorical variables ggplot(acs, aes(county)) + geom_bar() + coord_flip() 9.2.4 lollipop chart barplot’s more fun cousin, the lollipop chart the package ggalt makes a geom for us so we don’t have to create it manually we do, however, have to count the observations ourself. we use the function count() to do this. the arguments are x, or the tibble, and ... these are the columns we want to count important note: in the tidyverse, the first argument is almost always the tibble we are working with this will become very useful at a later point remember, to install packages, navigate to the console and use the function install.packages(&quot;pkg-name&quot;). library(ggalt) ## Registered S3 methods overwritten by &#39;ggalt&#39;: ## method from ## grid.draw.absoluteGrob ggplot2 ## grobHeight.absoluteGrob ggplot2 ## grobWidth.absoluteGrob ggplot2 ## grobX.absoluteGrob ggplot2 ## grobY.absoluteGrob ggplot2 acs_counties &lt;- count(acs, county) ggplot(acs_counties, aes(county, n)) + geom_lollipop() + coord_flip() ridgelines (1 continuous 1 categorical) line chart (1 continuous 1 time), this is a unique case 9.3 Expanding bivariate visualizations to trivariate &amp; other tri-variate we can visualize other vcariables by setting further aesthetics. can set the color or fill, size, and shape we alreay did this previously when we set the color, let’s do that here. lets see how commuting by walking changes with the family house and under 18 pop set the color argument of the aes() function as color = by_walk it’s important you do this within the aesthetics function ggplot(acs, aes(fam_house_per, age_u18, color = by_auto)) + geom_point() we can add size to this as well by setting the size aesthetic lets see if the more female headed house holds there are affects commuting by car as minors increases ggplot(acs, aes(fam_house_per, age_u18, color = by_auto, size = fem_head_per)) + geom_point(alpha = .2) from this chart we can see quite a few things: as fam_house_per increases so does the under 18 pop, as both age_u18 and fam_house_per increase so does the rate of communiting by car as both age_u18 and fam_house_per so does female headed houses, but to a lesser degree this gives us a good idea of some relationships that we can test with our data at a later point minors_lm &lt;- lm(age_u18 ~ fam_house_per + by_auto + fem_head_per, data = acs) huxtable::huxreg(minors_lm) ## Registered S3 methods overwritten by &#39;broom.mixed&#39;: ## method from ## augment.lme broom ## augment.merMod broom ## glance.lme broom ## glance.merMod broom ## glance.stanreg broom ## tidy.brmsfit broom ## tidy.gamlss broom ## tidy.lme broom ## tidy.merMod broom ## tidy.rjags broom ## tidy.stanfit broom ## tidy.stanreg broom (1) (Intercept) 0.000&nbsp;&nbsp;&nbsp;&nbsp; (0.006)&nbsp;&nbsp;&nbsp; fam_house_per 0.245 *** (0.009)&nbsp;&nbsp;&nbsp; by_auto 0.016 *&nbsp;&nbsp; (0.007)&nbsp;&nbsp;&nbsp; fem_head_per 0.257 *** (0.012)&nbsp;&nbsp;&nbsp; N 1311&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; R2 0.564&nbsp;&nbsp;&nbsp;&nbsp; logLik 2444.648&nbsp;&nbsp;&nbsp;&nbsp; AIC -4879.296&nbsp;&nbsp;&nbsp;&nbsp; *** p &lt; 0.001; ** p &lt; 0.01; * p &lt; 0.05. Trivariate: grouped / stacked bar charts heatmaps 9.4 The Grammar of Layered Graphics recommended reading: A Layered Grammar of Graphics in R we will use a package called ggplot2 to do the visualizaiton of our data the gg in ggplot stands for “grammar of graphics”. once we can internalize the grammar, creating plots becomes rather easy we specify our aesthetics we add layers (hence the plus sign). these take values from the specified aesthetics can add multiple layers add aesthetics other than x and y. helps us visualize more dimensions of the data. we can use shape, color, and size 9.4.1 revisiting the cartesian plane x and y coordinates generally two numeric values on the x and y. think of the standards scatterplot (below) we also can place groups on one axis i.e. barchart (below) the y is usually the variable of interest as we move along the x axis (to the right) we can see how the y changes in response "],
["data-structures.html", "Chapter 10 Data Structures 10.1 Vectors 10.2 Data Frames 10.3 Lists", " Chapter 10 Data Structures There is a topic I have been skirting around for some time now and I think it is time that we have to have a rather important conversation. It’s one that is almost never fun but is quite necessary because without it, there may be many painful lessons learned in the future. We’re going to spend this next chapter talking about data structures—but not all of them! We’ll only cover the three most common and, by the end of this, it is my hope you will have a much stronger idea of what you are working with and why it behaves the way it does. We will cover vectors, data frames rather briefly, and lists. We’ll talk about some of their defining characteristics and how we can interact with them. Often the theory behind these object types are ommitted, but I am of the mind that learning this early on will pay off in dividends. Take a deep breath before we dive in and remind yourself that it ain’t nothin’ but a thing. 10.1 Vectors I like to think of the vector as the building block of any R object. You’ve actually been working with vectors this entire time. But we haven’t been very explicit about this yet. Up until this point we have been working mainly with tibbles. Each column of a tibble is actually just a vector. What makes a vector a vector is that it can only be a single data type and that they are one-dimensional—opposed to tibbles which are two-dimensional. You may have noticed that every value of a column is of the same data type. This means that they are rather strict to work with and for good reason. Imagine you wanted to multiple a column by 10, what would happen if a few of the values in the column were actually written out as text? Let’s try exploring this idea. The most common way to create a vector in R is to use the c() function. This stands for combine. We can combine as many elements as we want into a single vector using c(). Each element of the vector is it’s own argument (separated by a comma). For example if we wanted to create a vector of Boston’s unemployment rate rate for each month in 2019 that we have data for (until October as of this writing on 2019-12-18) we could write the below. We will save it in a a vector called unemp. unemp &lt;- c(3.2, 2.8, 2.8, 2.4, 2.8, 2.9, 2.7, 2.6, 2.7, 2.3) unemp ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 What is really great about vectors is that we can perform any number of operations on them—i.e. find the sum of all the values, the average, add a value to each element, etc. If we wanted to find the average unemployment rate for Boston for Jan - Oct. 2019, we can supply the vector to the function mean(). mean(unemp) ## [1] 2.72 However, you may be thinking “there are 12 months in a year not 10 and that should be represented” and if you are, I totally agree with you. Since the data for November and December are missing, we should denote that and update unemp accordingly. R uses NA to represent missing data. To represent this we can append two NAs to the vector we have. There are two ways we can do this. We can either combine unemp with two NAs, or rewrite the above vector. # combining existing with 2 NAs c(unemp, NA, NA) ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA This works, but since we will be saving this to unemp again it is not best practices to use the variable you are changing in that objects assignment. # for example unemp &lt;- c(unemp, NA, NA) The above is rather unclear and might confuse someone that will have to read your code at a later time—that person may even be you. For this reason we will redefine it. unemp &lt;- c(3.2, 2.8, 2.8, 2.4, 2.8, 2.9, 2.7, 2.6, 2.7, 2.3, NA, NA) unemp ## [1] 3.2 2.8 2.8 2.4 2.8 2.9 2.7 2.6 2.7 2.3 NA NA We know that there are 12 elements in this vector, but sometimes it is quite nice to sanity check oneself. We can always find out how long (or how many elements are in) a vector is by supplying the vector to the length() function. # how many observations are in `unemp`? length(unemp) ## [1] 12 10.1.1 Types of vectors There are a total of six types of vectors. Fortunately, only four of these really matter to us. These are integer, double, logical and character. 10.2 Data Frames accessing the underlying vectors of a data frame $ , [, [[, pull() 10.3 Lists lists are the most flexible object in R data frames are actually just lists disguised as rectangles unclass(slice(acs_edu, 1:6)) "],
["summary-statistics.html", "Chapter 11 Summary statistics", " Chapter 11 Summary statistics what is a summary stat? measures of central tendency measures of spread basic functions used mean, median, &amp; sd "],
["the-pipe.html", "Chapter 12 The pipe %&gt;% 12.1 Chaining functions", " Chapter 12 The pipe %&gt;% previously we have been working with one function at a time. it will become necessary to perform one function after another one way we could go about this is to write over the eisting object multiple times with an assignment &lt;- this is repetitive and hard to read we can utilize the forward pipe operator %&gt;% to chain functions together. from https://magrittr.tidyverse.org/: Basic piping x %&gt;% f is equivalent to f(x) x %&gt;% f(y) is equivalent to f(x, y) x %&gt;% f %&gt;% g %&gt;% h is equivalent to h(g(f(x))) from tidy soc sci 12.1 Chaining functions The true power of the tidyverse comes from it’s ability to chain functions after eachother. This is all enabled by the forward pipe operator %&gt;%. What it does: The pipe operator takes the output of it’s left hand side lhs and provides that output as the first argument in the function of the right hand side. Additionally, it exposes the lhs as a temporary variable .. Remember how I pointed out that the first argument for almost every function is the data? This is where that comes in handy. This allows us to use the pipe to chain together functions and “makes it more intuitive to both read and write” (magrittr vignette). You’ve seen how the first argument for every function here has been the data this is done purposefully to enable the use of the pipe. As always, the most helpful way to wrap your head around this is to see it in action. Let’s take one of the lines of code we used above and adapt it to use a pipe. We will select the name column of our data again. Previously we wrote select(data_frame, col_name). "],
["summarizing-the-tidy-way.html", "Chapter 13 summarizing the tidy way", " Chapter 13 summarizing the tidy way summarising data sets group_by() summarize() "],
["i-have-two-datasets-what-now.html", "Chapter 14 I have two datasets, what now?", " Chapter 14 I have two datasets, what now? multiple data sets what is a join? the need for a common identifier join types: left inner right full (rare) anti-join (for removing data) "],
["creating-ecometrics.html", "Chapter 15 Creating Ecometrics", " Chapter 15 Creating Ecometrics Note: a case study should be recreating these ecometrics Explore what the data description from BARI looks like library(tidyverse) ## ── Attaching packages ──────────────────────────────────────────── tidyverse 1.2.1 ── ## ✓ tibble 2.1.3 ✓ purrr 0.3.3 ## ✓ tidyr 1.0.2 ✓ dplyr 0.8.3 ## ✓ readr 1.3.1 ✓ stringr 1.4.0 ## ✓ tibble 2.1.3 ✓ forcats 0.4.0 ## ── Conflicts ─────────────────────────────────────────────── tidyverse_conflicts() ── ## x dplyr::filter() masks stats::filter() ## x dplyr::lag() masks stats::lag() ecometrics &lt;- readr::read_csv(&quot;data/911/911-ecometrics-2014-19.csv&quot;) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## TYPE = col_character(), ## tycod = col_character(), ## typ_eng = col_character(), ## sub_tycod = col_character(), ## sub_eng = col_character(), ## SocDis = col_double(), ## PrivateConflict = col_double(), ## Violence = col_double(), ## Guns = col_double(), ## Frequency_2015 = col_double(), ## Frequency_2016 = col_double(), ## Frequency_2017 = col_double(), ## Frequency_2018 = col_double(), ## yr.intro = col_double(), ## last.yr = col_double() ## ) glimpse(ecometrics) ## Observations: 302 ## Variables: 15 ## $ type &lt;chr&gt; &quot;AB===&gt;&gt;&gt;&quot;, &quot;ABAN===&gt;&gt;&gt;&quot;, &quot;ABANBU&quot;, &quot;ABANCELL&quot;,… ## $ tycod &lt;chr&gt; &quot;AB&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;ABAN&quot;, &quot;… ## $ typ_eng &lt;chr&gt; &quot;ASSAULT AND BATTERY&quot;, &quot;ABANDONED CALL&quot;, &quot;ABAND… ## $ sub_tycod &lt;chr&gt; &quot;===&gt;&gt;&gt;&quot;, &quot;===&gt;&gt;&gt;&quot;, &quot;BU&quot;, &quot;CELL&quot;, &quot;INCCAL&quot;, &quot;PH… ## $ sub_eng &lt;chr&gt; &quot;PICK A SUB-TYPE&quot;, &quot;PICK A SUB-TYPE&quot;, &quot;FROM A B… ## $ soc_dis &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ private_conflict &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,… ## $ violence &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,… ## $ guns &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,… ## $ frequency_2015 &lt;dbl&gt; 27, 40, 12768, 72081, 686, 1734, 3143, 194, 122… ## $ frequency_2016 &lt;dbl&gt; 29, 26, 13287, 56074, 1188, 1205, 2687, 397, 19… ## $ frequency_2017 &lt;dbl&gt; 29, 17, 12599, 39323, 124, 724, 1944, 253, 162,… ## $ frequency_2018 &lt;dbl&gt; 14, 10, 6422, 19947, 70, 506, 840, 262, 169, 78… ## $ yr_intro &lt;dbl&gt; 2014, 2014, 2014, 2014, 2014, 2014, 2014, 2014,… ## $ last_yr &lt;dbl&gt; 2019, 2018, 2019, 2019, 2019, 2019, 2019, 2019,… Check out the offense descriptions raw_911 &lt;- read_csv(&quot;data/911/911-raw.csv&quot;) %&gt;% janitor::clean_names() ## Parsed with column specification: ## cols( ## INCIDENT_NUMBER = col_character(), ## OFFENSE_CODE = col_character(), ## OFFENSE_CODE_GROUP = col_character(), ## OFFENSE_DESCRIPTION = col_character(), ## DISTRICT = col_character(), ## REPORTING_AREA = col_double(), ## SHOOTING = col_character(), ## OCCURRED_ON_DATE = col_datetime(format = &quot;&quot;), ## YEAR = col_double(), ## MONTH = col_double(), ## DAY_OF_WEEK = col_character(), ## HOUR = col_double(), ## UCR_PART = col_character(), ## STREET = col_character(), ## Lat = col_double(), ## Long = col_double(), ## Location = col_character() ## ) raw_911 %&gt;% count(offense_code_group) ## # A tibble: 68 x 2 ## offense_code_group n ## &lt;chr&gt; &lt;int&gt; ## 1 Aggravated Assault 10687 ## 2 Aircraft 60 ## 3 Arson 109 ## 4 Assembly or Gathering Violations 1143 ## 5 Auto Theft 6211 ## 6 Auto Theft Recovery 1385 ## 7 Ballistics 1338 ## 8 Biological Threat 3 ## 9 Bomb Hoax 109 ## 10 Burglary - No Property Taken 5 ## # … with 58 more rows There are fewer types of code groups. They are rather informative. 33k missing observations though is the same amount of missingness present in the descriptions? raw_911 %&gt;% count(offense_description) ## # A tibble: 279 x 2 ## offense_description n ## &lt;chr&gt; &lt;int&gt; ## 1 A&amp;B HANDS, FEET, ETC. - MED. ATTENTION REQ. 1 ## 2 A&amp;B ON POLICE OFFICER 7 ## 3 ABDUCTION - INTICING 12 ## 4 AFFRAY 318 ## 5 AIRCRAFT INCIDENTS 70 ## 6 ANIMAL ABUSE 89 ## 7 ANIMAL CONTROL - DOG BITES - ETC. 493 ## 8 ANIMAL INCIDENTS 388 ## 9 ANIMAL INCIDENTS (DOG BITES, LOST DOG, ETC) 34 ## 10 ANNOYING AND ACCOSTIN 3 ## # … with 269 more rows no missingness more types what do we do? choose one of these? we can actually use both filter data set to exclude NAs from offense_code_group. anti_join() that dataset from raw now use the raw_911 remainders and hand code that. reminder: we cannot automate everything. It requires long hours, dedication, and freakin’ grit. string manipulation using SPSS data using the tidycensus package for pulling in census data using rtweet for twitter data "],
["case-study-twitter-data.html", "Chapter 16 Case Study: Twitter Data", " Chapter 16 Case Study: Twitter Data appendix - style "]
]
