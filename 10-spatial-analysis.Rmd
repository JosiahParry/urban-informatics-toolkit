# Spatial Analysis

In the very beginning of the book we discussed some of the benefits of administrative data. One of the benefits mentioned is that administrative data are inherently **spatial**. Most data tend to be spatial in nature. This is because most events happen at a physical place. In the context of administrative data, we know that all data recorded at that level must fall within the municipal boundaries. Often we know the location within the municipality to a much finer scale—i.e. the block group, the voting ward, or even the exact point location. 

Identifying whether or not your data is spatial is easiest when there are geographic components present such as latitude and longitude. However, your data can also be spatial even if it isn't explicitly stated. Some things you can keep an eye out for are things like neighborhood, towns, county, etc. as these are location specific. It is likely that you will be performing analyses rooted in space and accounting for things such as counties but perhaps without using any geospatial techniques. 

In this section we will go over the very basics of geospatial analysis. 


## Types of spatial data

Within the field of Geographic Information Systems (**GIS**) there are two general umbrellas in which data fall under. These are vector and raster data.

**Vector** data is what you will find yourself working with most frequently. Most simply put they are _"points, lines, and polygons."_ The basis of vector data is the coordinate point. Just like the scatter plots we have built, each coordinate point is a combination of an x and y value (longitude and latitude respectively). This combination of x and y will tell us where something is. By combining two or more points we can trace along a path—think of the connect the dots diagrams you would do at restaurants as a kid—and create line segments. If, however, at any point these lines close, you now have a polygon.

The other umbrella of data is known as **raster** data. Raster data are to deal with more complex data that cannot easily be captured by a single point. Rasters are used to "represent spatially continuous phenomenon" [source](https://rspatial.org/raster/spatial/Spatialdata.pdf). Raster analysis is done to evaluate things like changing vegetation, elevation and slope modeling, analysing reflective surfaces, among much more. Raster analysis typically relies on satellite imagery or LiDAR laser point cloud data. Raster analysis is an extremely complex topic and requires devoted attention. As such, we will not cover it in this book. But, know that it exists and is out there! 


![](http://michaelminn.net/tutorials/gis-data/2019-points-lines-polygons.png)

## Working with spatial data 

Working with spatial data is made rather straightforward by the [`sf`](https://github.com/r-spatial/sf) package. 

sf is shorthand for _simple features_. sf let's us represent physical objects or phenomena in that occur the real world through data. It is built upon an international standard that "describes how such objects can be stored in and retrieved from databases, and which geometrical operations should be defined for them." (sf vinette, 1).

All simple features are representation of vector data. That is that they are composed of points. These points are usually represented two-dimensionally with longitude and latitude (x and y). We can associate a third dimension of altitude if so desired to extend to three dimensions: longitude, latitude, and altitude (x, y, & z). In our cases, however, this will not be used. 

In this section we will working with the `locations` Airbnb dataset. `locations` contains the longitude and latitude of Airbnb listings in Boston. These are an example of point data (which contain two-dimension). 


## Creating simple features from a tibble

Begin by installing the `sf` (simple-features) package and loading the `locations` Airbnb dataset.

```{r}
# install.packages("sf")
library(sf)
library(tidyverse)

locations <- read_csv("data/airbnb/locations.csv")

head(locations)
```

So far everything is the same. We have read in our dataset and created a tibble. The next step is to make this tibble a simple feature. Fortunately, sf keeps this process rather simple for us by representing spatial data in native R data formats—namely, the data frame. To make simple features from an existing tibble, we need to cast the object as an `sf` object. And we do this with `st_as_sf()`. 

Generally when we cast objects we use functions like `as.integer()` or `as_tibble()`. Here, there is a prefixed `st_`. This stands for *spatial transformation*. All transformations are prefixed as such—this is in an effort to keep continuity between GIS tools. Most functions cast objects to other classes with no function arguments. `st_as_sf()` unfortunately cannot read your mind and is not aware of what the geometry is in the tibble. As such, we need to use the `coords` argument in `st_as_sf()`. `coords` gives us the ability to tell sf what the columns are that contain our coordinate points. For point data we need to provide a character vector of length two with the x and y dimensions aka longitude and latitude.

> Note that we are likely used to saying _lat, long_, but that actually maps to _y, x_. This is something that trips everyone up! Just make sure you put longitude in the x spot and latitude in the y spot. 

To convert the `locations` data frame to a simple feature we will use `st_as_sf()` and set the `coords` argument to `c("longitude", "latitude")`. 


```{r}
st_as_sf(locations,
         coords = c("longitude", "latitude"))

```

Now that we have successfully created a simple feature we can see that we no longer have the columns `longitude` and `latitude` but a `geometry` column instead. Notice that when printed, the object tells us what type of geometry we are working with, it's dimensions, and the bounding box for these points.

> A bounding box is the furthest extent that our data reaches in both latiude and longitude.

The printed object informs us that there are actually two missing pieces of information the epsg and proj4string. This is because we failed to specify a **coordinate reference system** (CRS). While this book is not intended as an introduction to GIS, this is still worth briefly expanding upon. We use a CRS because we are trying to place points in two-dimensions when the Earth is round! Try peeling and orange and laying the peel flat. It's impossible. There is now way to visualize a circle as a rectangle without introducing _some_ error. This is what a CRS accounts for. There are many CRS for each type of map projection and each type of unit. Most, if not all, of the frustration you may encounter when working with spatial data will be due to mismatching CRS.

Fortunately we will _most likely_ be working with data that is collected using the **WGS84** reference system. This is a CRS that is used to define a global reference system that is used consistently throughout government agencies, and typically in online data recording. The Airbnb data uses this references system. 

Most other online data sources use this reference system as well. For example Google and Twitter provide their data using this CRS. The times when you are most likely to encounter a CRS isn't WGS84 is when working with data from local agencies that need highly accurate and tailored spatial data. This would be agencies like water departments, and forestry groups, etc. 

To ensure that our data are properly represented in space, we need to provide the CRS in the creation of our simple features. We do this by specifying the `crs` argument. `crs` will accept a number that indicates what projection you are using. There are too many CRS identifiers to commit to memory. This information is usually recorded in the original data source. Be sure to confirm the spatial dimensions! For WGS84, the CRS identifier is `4326`. This is probably worth committing to memory. 

https://confluence.qps.nl/qinsy/latest/en/world-geodetic-system-1984-wgs84-182618391.html#id-.WorldGeodeticSystem1984(WGS84)v9.1-WGS84definitions


We will now create an object called `loc_sf` using `st_as_sf()` and providing both the `coords` and the `crs`. 

```{r}
loc_sf <- st_as_sf(locations,
         coords = c("longitude", "latitude"),
         crs = 4326)

loc_sf
```

Since an sf object is also a data frame we are able to perform all of the operations that we may with a normal tibble such as selecting columns, joining, mutating etc.


```{r}
count(loc_sf)
```

Notice that it keeps the geometry even when counting. When our data is spatial, we have to incorporate the geometry into our computations. This often times leads to slower processing times. So if you do not immediately need the geometry, my recommendation is that you join it back on as late as possible. You can cast an sf object to a tibble with `as_tibble()`

```{r}
as_tibble(loc_sf) %>% 
  count()
```

Notice that we now lose the geometry column. This is because we have stopped keeping track of the geometry. 


## Plotting sf objects with ggplot

Plotting sf objects is made rather straightforward with ggplot2. Since sf objects contain a ton of spatial information this is inferred from ggplot. As such, we are not required to map the aesthetics for x and y. We simple provide just the data argument to ggpplot and then add a `geom_sf()` layer. Inside of `geom_sf()` we can provide any and all arguments that we may like such as color, size, shape, etc. as this will be passed to the underlying `geom_*`—in the case of points, it will be `geom_point()`.

```{r}
ggplot(loc_sf) +
  geom_sf(shape = ".")
```


- this is great we can already somewhat see the shape of boston and suffolk county
- these points are nice, but we want so supplement them with census data 
- we can read in polygon data which represents the census tracts
- perform spatial joins to associate the points with which tracts they fall in

## Connecting points to polygons

- st_join()
- following the join we can associate it with normal tabular data
- locations has an id column that joins back to listings data.
- create a plot of census tracts colored by the number of listings in that tract


https://geocompr.robinlovelace.net/

https://rud.is/books/30-day-map-challenge/points-01.html