# General data manipulation


```{r, include=FALSE}
source("common.R")
```


Now that you have the ability to read in data, it is important that you get comfortable handling it. Some people call the process of rearranging, cleaning, and reshaping data massaging, plumbing, engineering, and myriad other names. Here, we will refer to this as data manipulation. This is a preferrable catch-all term that does not ilicit images of toilets or Phoebe Buffay (she was a masseuse!). 

You may have heard of the 80/20 rule, or at least one of the many 80/20 rules. The 80/20 rule I'm referring to, is the idea that data scientists will spend 80% or more of their time cleaning and manipulating their data. The other 20% is the analysis part—creating statistics and models. I mention this because working with data is _mostly_ data manipulation and only _some_ statistics. Be prepared to get your hands dirty with data—euphamisms like this is probably why the phrase _data plumbing_ ever came about. 

In this chapter we will learn how to preview data, select columns, arrange rows, and filter rows. This is where we get hands on with our data. This is just about as physical as we will be able to get unless you want to open up the raw csv and edit the data there!

#### CHECK THIS LATER

> Note: I do not recommend editing any csvs directly. That statement was in jest. We will cover the concept of reproducibility.


This is all to say that you will find yourself with messy, unsanitized, gross, not fun to look at data most of the time. Because of this, it is really important that we have the skills to clean our data. Right now we're going to go over the foundational skills we will learn how to select columns and rows, filter data, and create new columns, and arrange our data. To do this, we will be using the [`dplyr`](https://dplyr.tidyverse.org) package from the tidyverse. 

The data we read in in the last chapter was only a select few variables from the annual census data release that the team at the Boston Area Research Initiative (BARI) provides. These census indicators are used to provide a picture into the changing landscape of Boston and Massachussettes more gnerally. In this chapter we will work through a rather real life scenario that you may very well encounter using the BARI data. 

## Scenario:

A local non-profit is interested in the commuting behavior of Greater Boston residents. Your advisor suggested that you assist the non-profit in their work. You've just had coffee with the project manager to learn more about what their specific research question is. It seems that the extension of the Green Line is of great interest to them. They spoke at length about the history of the Big Dig and its impact on commuters working in the city. They look at their watch and realize they're about to miss the commuter train home. You shake their hand, thank them for their time (and the free coffee because you're a grad student), and affirm that you will email them in a week or so with some data for them to work with.

https://www.mass.gov/green-line-extension-project-glx
https://en.wikipedia.org/wiki/Big_Dig


You're back at your apartment, french press next your laptop, though not too close, notes open, and ready to begin. You review your notes and realize, while you now have a rather good understanding of _what_ the Green Line Extensions is and the impact that the Big Dig had, you really have no idea what about commuting behavior in Greater Boston they are interested in. You realize you did not even confirm what constitutes the Greater Boston area. You push down the coffee grinds and pour your first cup of coffee. This will take at least two cups of coffee. 

The above scenario sounds like something out of a stress dream. This is scenario that I have found myself in many times and I am sure that you will find yourself in at one point as well. The more comfortable you get with data analysis and asking good questions, the more guided and directed you can make these seemingly vague objectives. 

> At the end of this chapter, we will expound upon ways to prevent this in the future.

## Getting physical with the data 

The data we used in both chapters one and two were curated from the [annual census indicator release](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/XZXAUP) from BARI. This is the dataset from which `acs_edu` was created. We will use these data to provide relevant data relating to commuting in the Greater Boston Area.

To do so, we will be using the tidyverse to read in and manipulate our data (as we did last chapter). Recall that we will load the tidyverse using `library(tidyverse)`. 

> Refresher: the tidyverse is a collection of packages used for data analysis. When you load the tidyverse it loads `readr`, `ggplot2`, and `dplyr` for us, among other packages. For now, though, these are the only relevant packages. 

**Try it:**

* Load the tidyverse
* Read in the file `ACS_1317_TRACT.csv` located in the `data` directory, store it in an object called `acs_raw`

```{r message=FALSE}
library(tidyverse)
acs_raw <- read_csv("data/ACS_1317_TRACT.csv")
```

Wonderful! You've got the hang of reading data in which is truly no small feat. Once we have the data accessible from R, it is important to get familiar with what the data are. This means we need to know _which_ variables are available to us and get a feel for what the values in those variables represent. 

Try printing out the `acs_raw` object in your console. 

```{r, eval=FALSE}
acs_raw
```


Oof, yikes. It's a bit messy! Not to mention that R did not even print out all of the columns. That's because it ran out of room. When we're working with wide data (many columns), it's generally best to view only a preview of the data. The function `dplyr::glimpse()` can help us do just that. Provide `acs_raw` as the only argument to `glimpse()`.


```{r}
glimpse(acs_raw)
```

Much better, right? It is frankly still a lot of text, but the way it is presented is rather useful. Each variable is written followed by its data type, i.e. `<dbl>`, and then a preview of values in that column. If the `<dbl>` does not make sense yet, do not worry. We will go over data types in depth later. Data types are not the most fun and I think it is important we have fun! 

`acs_raw` is the dataset from which `acs_edu` was created. As you can see, there are many, many, _many_ different variables that the ACS data provide us with. These are only the tip of the iceberg. 

**Pause**

Now have a think. 

Looking at the preview of these data, which columns do you think will be most useful to the non-profit for understanding commuter behavior?

> Note: "all of them" is not always the best answer. By providing too much data one may moved to inaction because they now determine what variables are the most useful and how to use them. HAVE SOURCES ON PARADOX OF CHOICE AND INFORMATION OVERLOAD.

If you spotted the columns `r glue::glue_collapse(names(select(acs_raw, contains("commute"))), sep = ", ", last = ", and ")`, your eyes and intuition have served you well! These variables tell us about what proportion of the sampled population in a given census tract have commute times that fall within the indicated duration range, i.e. 30-60.

> PERSONAL NOTE: USE COMMUTE VARIABLES FOR PIVOTING AND THEN SEPARATING INTO NEW VARS

### `select()`ing

So now we have an intuition of the most important variables, but the next problem soon arises: how do we isolate just these variables? Whenever you find yourself needing to select or deselect columns from a tibble `dplyr::select()` will be the main function that you will go to. 

**What does it do?**: `select()` selects variables from a tibble and returns another tibble. 

Before we work through how to use `select()`, refer to the help documentation and see if you can get somewhat of an intuition by typing `?select()` into the console. Once you press enter the documentation page should pop up in RStudio. 

There are a few reasons why I am directing you towards the function documentation.

1. To get you comfortable with navigating the RStudio IDE
2. Expose you to the R vocabulary
3. Soon you'll be too advanced for this book and will have to figure out the way functions work on your own! 

Perhaps the help documentation was a little overwhelming and absolutely confusing. That's okay. It's just an exposure! With each exposure things will make more sense. Let's tackle these arguments one by one. 

> `.data`: A tbl. All main verbs are S3 generics and provide methods for tbl_df(), dtplyr::tbl_dt() and dbplyr::tbl_dbi().

What I want you to take away from this argument definition is `a tbl`. Whenever you read `tbl` think to your self "oh, that is just a `tibble`." If you recall, when we read rectangular data with `readr::read_csv()` or any other `readr::read_*()` function we will end up with a tibble. To verify that this is the case, we can double check our objects using the function `is.tbl()`. This function takes an object and returns a logical value (`TRUE` or `FALSE`) if the statement is true. Let's double check that `acs_raw` is in fact a tbl.

```{r}
is.tbl(acs_raw)
```

> Aside: Each object type usually has a function that let's you test if objects are that type that follow the structure `is.obj_type()` or the occasional `is_obj_type()`. We will go over object types more later. 

We can read the above as if we are asking R the question "is this object a `tbl`?" The resultant output of `is.tbl(acs_raw)` is `TRUE`. Now we can be doubly confident that this object can be used with `select()`. 

The second argument to `select()` is a little bit more difficult to grasp, so don't feel discouraged if this isn't clicking right away. There is _a lot_ written in this argument definition and I feel that not all of it is necessary to understand from the get go. 

> `...`: 	One or more unquoted expressions separated by commas. You can treat variable names like they are positions, so you can use expressions like x:y to select ranges of variables.

`...`, referred to as "dots" means that we can pass any number of arguments to the function. Translating "one or more unquoted expressions separated by commas" into regular person speak reiterates that there can be multiple other arguments passed into `select()`. "Unquoted expressions" means that if we want to select a column we do not put that column name in quotes. 

"You can treat variable names like they are positions" translates to "if you want the first column you can write the number `1` etc." and because of this, if you want the first through tenth variable you can pass `1:10` as an argument to `...`.

The most important thing about `...` is that we **do not** assign `...` as an argument, for example . `... = column_a` is **not** the correct notation. We provide `column_a` alone. 

As always, this makes more sense once we see it in practice. We will now go over the many ways in which we can select columns using `select()`. Once we have gotten the hang of selecting columns we will return back to assisting our non-profit.

We will go over: 

- selecting by name
- selecting by position
- select helpers


### `select()`ing exercises 

`select()` enables us to choose columns from a tibble based on their names. But remember that these will be unquoted column names. 

Try it:

- select the column `name` from `acs_raw`

```{r max.print = 10}
select(acs_raw, name)
```

The column `name` was passed to `...`. Recall that dots allows us to pass "one ore more unquoted expressions separated by commas." To test this statement out, select `town` in addition to `name` from `acs_raw`

Try it:

- select `name` and `town` from `acs_raw`

```{r max.print=10}
select(acs_raw, name, town)
```

Great, you're getting the hang of it. 

Now, in addition to to selecting columns solely based on their names, we can also select a range of columns using the format `col_from:col_to`. In writing this `select()` will register that you want every column from and including `col_from` up until and including `col_to`. 

Let's refresh ourselves with what our data look like:

```{r}
glimpse(acs_raw)
```


Try it:

- select the columns `age_u18` through `age_o65`.

```{r max.print=10}
select(acs_raw, age_u18:age_o65)
```

Now to really throw you off! You can even reverse the order of these ranges. 

Try it:

- select columns from `age_o65` to `age_u18`.

```{r max.print=10}
select(acs_raw, age_o65:age_u18)
```

> NOTE: This is an awful scenario. SOMEONE HALP

Alright, so now we have gotten the hang of selecting columns based on their names. But equally important is the ability to select columns based on their position. Consider the situation in which you regularly receive georeferenced data from a research partner and the structure of the dataset is rather consistent _except_ that they frequently change the name of the coordinate columns. Sometimes the columns are `x` and `y`. Sometimes they are capitalized `X` and `Y`, `lon` and `lat`, or even `long` and `lat`. It eats you up inside! But you know that while the names may change, their positiions never do—they're always the last two columns. You decide to program a solution rather than having a conversation with your research partner—though, I recommend you both level set on reproducibility standards.

 > "...You can treat variable names like they are positions..."
 
 The above was taken from the argument definition of dots `...`. Like providing the name of the column, we can also provide their positions (also referred to as an index). In our previous example, we selected the `name` column. We can select this column by it's position too. `name` is the second column in our tibble. We select it by position like so:

```{r max.print=10}
select(acs_raw, 2)
```

Try it:

- select `age_u18` and `age_o65` by their position

```{r max.print = 10}
select(acs_raw, 6, 9)
```

You may see where I am going with this. Just like column names, we can select a range of columns using the same method `index_from:index_to`.

Try it:

- select the columns from `age_u18` to `age_o65` using `:` and the column position
- select the columns in reverse order by their indexes 

```{r max.print = 10}
select(acs_raw, 6:9)
```

```{r max.print=10}
select(acs_raw, 9:6)
```

> Base R Side Bar: To help build your intuition, I want to point out some base R functionality. Using the colon `:` with integers (whole numbers) is actually not a `select()` specific functionality. This is something that is rather handy and built directly into R. Using the colon operator, we can create ranges of numbers in the same exact way as we did above. If we want create the range of numbers from 1 to 10, we write `1:10`. `r 1:10`.

In our scenario, we want to select the last two columns. We may not know their names _or_ their position. Luckily, there's a function for that.

![](https://i.imgflip.com/3p2zow.jpg)

`last_col()` is a handy function that enables us to select the last column. There is also an option to get an offset from the last column. An offset would allow us to grab the second to last column by setting the offset to 1. By setting the offset, `last_col()` will from the `offset + 1` from the last column. So if the offset is set to 1, we would be grabbing the second to last column.

Let's give it a shot:

```{r, max.print=5}
select(acs_raw, last_col())
select(acs_raw, last_col(offset = 1))
select(acs_raw, last_col(offset = 1):last_col())
```


`last_col()` comes from another packages called `tidyselect` which is imported with `dplyr`. This package contains a number of helper functions. There are 9 total helpers and you've already learned one of them. We will briefly review four more of these. I'm sure you are able to deduce how the functions work solely based on their names. The functions are:

- `starts_with()`: a string to search that columns start with
- `ends_with()`: a string to search that columns end with
- `contains()`: a string to search for in the column names at any position
- `everything()`: selects the remaining columns

Each of these function take a character string and searches the column headers for them. 

Try it out:

- find all columns that start with `"med"`

```{r max.print=5}
select(acs_raw, starts_with("med"))
```

- select columns that end with `"per"`

```{r, max.print=5}
select(acs_raw, ends_with("per"))
```

- find any column that contains the string `"yr"`

```{r max.print=5}
select(acs_raw, contains("yr"))
```

- select columns that start with `med` then select everything else

```{r, max.print=5}
select(acs_raw, contains("yr"), everything())
```

## Selecting Rows

Though a somewhat infrequent event, it will be handy to know how to select rows. There are two ways in which we can select our rows. The first is by specifying exactly which rows by their position. The other way is to filter down our data based on a condition—i.e. median household income within a range. The functions to do this are `slice()` and `filter()` respectively. The remainder of this chapter will introduce you to `slice()`. We will learn how to filter in the next chapter.

Like `select()` we can also select rows. But rows do not have names, so we must select the rows based on their position. Given your familiarity with selecting by column position this should be a cake walk for you. 

Similar to `last_col()` we have the function `n()`. `n()` is a rather handy little function which tells us how many observations there are in a tibble. This allows to specify the last row of a tibble. 

```{r}
slice(acs_raw, n())
```

Unlike `last_col()`, `n()` provides us with a number. Instead of specifying an offset we can instead subtract directly from the output of `n()`. To grab the last three rows we can write `(n() - 3):n()`. We put `n()-3` inside of parantheses so R knows to process `n() - 3` first.

```{r max.print=10}
slice(acs_raw, (n() - 3):n())
```


Try it:

- select the first row, rows 100-105, and the last row

```{r}
slice(acs_raw, 1, 100:105, n())
```


## Revisiting commmuting 

We've just spent a fair amount of time learning how to work with our data. It's now time to return to the problem at hand. We still haven't addressed what data will be of use to our partner at the non-profit. While urban informatics is largely technical, it is still mostly intellectual. We have to think through problems and be methodical with our data selection and curation. We have to think about what our data tells us and why it is important. 

During these exercises, I hope you were looking at the data and thinking about what may be helpful to the non-profit. Again, the goal is to provide them with what is useful, but not more than they need. 


### Exercise

It is now incumbent upon you to curate the data BARI Census Indicator dataset for the non-profit. Refamiliarize yourself with the data. Select a subset of columns that you believe will provide the best insight into commuting behavior in the Greater Boston Area. 

When making decisions like this, I like to think of a quote from The Master of Disguise:

> "Answer these questions for yourself: who? Why? Where? How?"

Save the resultant tibble to an object named `commute` or something else informative. 

Below is one approach to this question. For this, I have selected all columns pertaining to commute time (columns that start with `commute`), the method by which people commute (begin with `by`), meidan household income, and the name of the census tract. The name of the census tract will be helpful for identifying "where". 

```{r}
commute <- select(acs_raw,
                  name,
                  starts_with("commute"),
                  starts_with("by"),
                  med_house_income)
```



## Asking good questions 

